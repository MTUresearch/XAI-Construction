TY  - CONF
TI  - Towards Intelligent and Trustable Digital Twin Asset Management Platform for Transportation Infrastructure Management Using Knowledge Graph and Explainable Artificial Intelligence (XAI)
T2  - 2023 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)
SP  - 0528
EP  - 0532
AU  - H. Wicaksono
AU  - M. U. Nisa
AU  - A. Vijaya
PY  - 2023
KW  - Explainable AI
KW  - Biological system modeling
KW  - Transportation
KW  - Collaboration
KW  - Data models
KW  - Digital twins
KW  - Asset management
KW  - Digital twins
KW  - explainable artificial intelligence (XAI)
KW  - knowledge graph
KW  - ontologies
KW  - transportation infrastructure management
DO  - 10.1109/IEEM58616.2023.10406401
JO  - 2023 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)
Y1  - 18-21 Dec. 2023
AB  - In the transportation sector, implementing digital twins is part of the digitization measure to improve resource efficiency in infrastructure management. However, the use of digital twins is still limited due to challenges such as a lack of shared understanding of digital twin models, complex model integration, security issues, lack of access to essential data, and high costs due to inefficient business models. This research develops an asset management platform suitable for Small and Medium Enterprises (SMEs) for the cross-company, secure, and intuitive collaborative management of digital twin assets. It can be achieved by developing an ontology-based semantic model of the assets, explainable machine learning (XAI), and a scenario-based intelligent search and discovery mechanism.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence (XAI) Empowered Digital Twin on Soil Carbon Emission Management Using Proximal Sensing
T2  - 2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)
SP  - 1
EP  - 5
AU  - D. An
AU  - Y. Chen
PY  - 2023
KW  - Vegetation mapping
KW  - Carbon dioxide
KW  - Pressing
KW  - Soil
KW  - Predictive models
KW  - Real-time systems
KW  - Digital twins
KW  - Digital Twin
KW  - Explainable Artificial Intelligence (XAI)
KW  - Explainable Artificial Intelligence of the Internet of Things (XAIoT)
KW  - Methane Emission Abatement
KW  - Soil Carbon Emission Management
DO  - 10.1109/DTPI59677.2023.10365455
JO  - 2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)
Y1  - 7-9 Nov. 2023
AB  - Digital Twin can be developed to represent a certain soil carbon emissions ecosystem that takes into account various parameters such as the type of soil, vegetation, climate, human interaction, and many more. With the help of sensors and satellite imagery, real-time data can be collected and fed into the digital model to simulate and predict soil carbon emissions. However, the lack of interpretable prediction results and transparent decision-making results makes Digital Twin unreliable, which could damage the management process. Therefore, we proposed an explainable artificial intelligence (XAI) empowered Digital Twin for better managing soil carbon emissions through AI-enabled proximal sensing. We validated our XAIoT-DT components by analyzing real-world soil carbon content datasets. The preliminary results demonstrate that our framework is a reliable tool for managing soil carbon emissions with relatively high prediction results at a low cost.
ER  - 

TY  - CONF
TI  - Interpretable Deep Learning for Diagnosis of Maize Streak Disease
T2  - 2023 First International Conference on the Advancements of Artificial Intelligence in African Context (AAIAC)
SP  - 1
EP  - 6
AU  - M. F. Kalyango
AU  - K. M. Ntanda
PY  - 2023
KW  - Deep learning
KW  - Productivity
KW  - Explainable AI
KW  - Decision making
KW  - Learning (artificial intelligence)
KW  - Feature extraction
KW  - Fourth Industrial Revolution
KW  - Leaf Diseases
KW  - Maize Streak Disease/Virus
KW  - Interpretable Machine Learning
KW  - Deep Learning
DO  - 10.1109/AAIAC60008.2023.10465315
JO  - 2023 First International Conference on the Advancements of Artificial Intelligence in African Context (AAIAC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 First International Conference on the Advancements of Artificial Intelligence in African Context (AAIAC)
Y1  - 15-16 Nov. 2023
AB  - Maize Streak Disease (MSD) caused by the maize streak virus remains a significant agricultural challenge during the 4th industrial revolutions, as it poses a substantial threat to global maize crops, resulting in severe yield reductions. Effective and timely identification of the disease is pivotal in curbing its propagation and enacting appropriate control strategies. The emergence of the 5th industrial revolutions has witnessed the integration of advanced technologies into various sectors, with deep learning methodologies standing out for their prowess in diverse domains, including disease detection. Nonetheless, the inherent complexity of deep learning models hinders visibility into their transparency, preventing users from comprehending the underlying features and decision-making processes. This opacity often leads to reduced user trust, even when the models demonstrate high accuracy rates.In this research, we introduce an interpretable machine learning framework through explainable AI (XAI) that harnesses the capabilities of deep learning, specifically Convolutional Neural Networks (CNNs), for the identification of the Maize Streak Disease. This framework not only exploits the potential of deep learning for precise disease classification but also integrates techniques for model interpretability. The proposed methodology capitalizes on CNN architectures to extract discerning features from an extensive dataset of MSD-affected leaf images. The trained CNN model is then coupled with interpretable techniques such as SHAP and LIME.Empirical findings highlight that this easy-to-understand deep learning approach is very effective in correctly diagnosing the Maize Streak Disease. The TensorFlow-designed model achieves a commendable accuracy of 96%.In essence, this study presents a novel interpretable machine learning framework that seamlessly integrates deep learning with interpretability methods to tackle the challenge of Maize Streak Disease. Beyond achieving exceptional classification precision, this research provides a transparent window into the decision-making mechanisms of the model. With a pronounced emphasis on interpretability, this study endeavors to bridge the gap between the potent predictive capacities of deep learning and the imperative for intelligible and dependable diagnostic systems within the agricultural disease domain.The implications of this work extend to the 5th industrial revolutions by bolstering agricultural productivity, mitigating yield losses, and enhancing the efficiency and efficacy of Intelligent Agricultural Support Systems.
ER  - 

TY  - CONF
TI  - Utilizing Macroeconomic Factors for Sector Rotation based on Interpretable Machine Learning and Explainable AI
T2  - 2020 IEEE International Conference on Big Data (Big Data)
SP  - 5505
EP  - 5510
AU  - Y. Zhu
AU  - C. Yi
AU  - Y. Chen
PY  - 2020
KW  - Correlation coefficient
KW  - Industries
KW  - Solid modeling
KW  - Data models
KW  - Macroeconomics
KW  - Indexes
KW  - Portfolios
KW  - explainable AI
KW  - random forest
KW  - feature selection
KW  - macroeconomic factors
KW  - crowded market indicator
DO  - 10.1109/BigData50022.2020.9377954
JO  - 2020 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Big Data (Big Data)
Y1  - 10-13 Dec. 2020
AB  - This paper focuses on the application of explainable AI in finance, introducing the use of machine learning models such as multiple linear regression, ridge regression, and random forest. We also compare their effects through empirical analysis on Chinese stock market. In addition, we propose three methods, which are feature selection, discretization of returns, and signal timing strategy, to improve the utility of our model. The empirical results show that our models can effectively select industries that will perform well in the future, further proving the importance and application feasibility of explainable AI in the financial field.
ER  - 

TY  - CONF
TI  - Exploring the Role of XAI in Enhancing Predictive Model Transparency in Healthcare Risk Assessment
T2  - 2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)
SP  - 1
EP  - 5
AU  - R. Mandava
AU  - S. S. Vellela
AU  - N. Malathi
AU  - K. Haritha
AU  - S. Gorintla
AU  - L. Dalavai
PY  - 2025
KW  - Ethics
KW  - Explainable AI
KW  - Computational modeling
KW  - Decision making
KW  - Closed box
KW  - Medical services
KW  - Predictive models
KW  - Safety
KW  - Risk management
KW  - Standards
KW  - Explainable AI (XAI)
KW  - healthcare
KW  - predictive modeling
KW  - risk assessment
KW  - clinician trust
KW  - patient safety
KW  - transparency
KW  - interpretability
KW  - bias detection
KW  - regulatory compliance
KW  - ethical AI
KW  - human-AI collaboration
DO  - 10.1109/ICCRTEE64519.2025.11052988
JO  - 2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)
Y1  - 28-30 May 2025
AB  - The increasing integration of Artificial Intelligence (AI) in healthcare demands systems that not only predict accurately but also explain their decisions transparently. This study explores the role of Explainable Artificial Intelligence (XAI) in enhancing transparency, interpretability, and trust in AI-driven predictive models used for healthcare risk assessment. It evaluates XAI techniques like SHAP and LIME in explaining black-box models such as Random Forest and Neural Networks, focusing on clinician trust, ethical adoption, and patient safety. Utilizing real-world healthcare datasets, the study investigates how interpretable models can bridge the gap between AI outputs and clinical decision-making. Results demonstrate that XAI significantly improves clinician confidence and regulatory compliance without greatly compromising prediction performance. The research underscores the transformative potential of XAI in fostering responsible and effective AI deployment in clinical environments.
ER  - 

TY  - CONF
TI  - Integrating Explainable AI to Enhance Dynamic Risk Assessment in Automated Driving Systems
T2  - 2024 IEEE International Conference on Vehicular Electronics and Safety (ICVES)
SP  - 1
EP  - 6
AU  - A. R. Patel
AU  - T. Göhler
AU  - P. Liggesmeyer
PY  - 2024
KW  - Runtime
KW  - Adaptive systems
KW  - Explainable AI
KW  - ISO Standards
KW  - Safety
KW  - Risk management
KW  - Reliability
KW  - Vehicle dynamics
KW  - Monitoring
KW  - Cruise control
KW  - Dynamic Risk Assessment
KW  - Automated Driving System
KW  - Explainable AI
KW  - Adaptive Cruise Control
KW  - Machine Learning
DO  - 10.1109/ICVES61986.2024.10928046
JO  - 2024 IEEE International Conference on Vehicular Electronics and Safety (ICVES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Vehicular Electronics and Safety (ICVES)
Y1  - 17-19 Dec. 2024
AB  - This paper introduces a Dynamic Risk Assessment (DRA) framework for Automated Driving Systems (ADS) that integrates Explainable Artificial Intelligence (XAI) to enhance the transparency and interpretability of Machine Learning (ML) models. Traditional risk assessment methods, such as those outlined in ISO 26262 and ISO 21448, typically focus on the concept phase and rely on static risk models. However, these approaches are inadequate for addressing the evolving, runtime risks faced by ADS during operation. To overcome these limitations, we propose incorporating runtime risk assessment using the PDTAA (Plan-Do-Train-Adjust-Assess) process, which enables continuous risk monitoring as driving conditions change. Through a case study on Adaptive Cruise Control (ACC) systems in a Highway Lane-Following scenario, we demonstrate how the framework allows for detailed, continuous risk evaluation. The integration of XAI techniques, specifically Shapley values, improves the interpretability and reliability of risk predictions, offering a more robust solution for runtime safety evaluation in ADS. This work contributes to the application of XAI to DRA, offering a valuable tool to enhance the safety and reliability of ADS before deployment in real-world environments.
ER  - 

TY  - CONF
TI  - Explainable AI Using h2o AutoML and Robustness Check in Credit Risk Management
T2  - 2023 Intelligent Computing and Control for Engineering and Business Systems (ICCEBS)
SP  - 1
EP  - 5
AU  - S. S
AU  - A. Vijayakumar
PY  - 2023
KW  - Water
KW  - Explainable AI
KW  - Computational modeling
KW  - Organizations
KW  - Robustness
KW  - Cognition
KW  - Risk management
KW  - eXplainable AI (XAI)
KW  - H20 AutoML
KW  - Robustness Check
KW  - Artificial intelligence
DO  - 10.1109/ICCEBS58601.2023.10449100
JO  - 2023 Intelligent Computing and Control for Engineering and Business Systems (ICCEBS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 Intelligent Computing and Control for Engineering and Business Systems (ICCEBS)
Y1  - 14-15 Dec. 2023
AB  - The paper proposes an eXplainable Artificial Intelligence model that can be utilized in credit risk the board and, specifically, in estimating the dangers that emerge when credit is acquired utilizing shared loaning stages. The model applies connection organizations to Shapley esteems with the goal that Computerized reasoning expectations are gathered by the likeness in the hidden clarifications. The exact examination of 15,000 little and medium organizations requesting credit uncovers that both hazardous and not dangerous borrowers can be gathered by a bunch of comparative monetary qualities, which can be utilized to make sense of their FICO rating and, in this manner, to foresee their future way of behaving.
ER  - 

TY  - CHAP
TI  - Introduction on AI Approaches in Capital Markets
T2  - The AI Book: The Artificial Intelligence Handbook for Investors, Entrepreneurs and FinTech Visionaries
SP  - 150
EP  - 156
AU  - Aric Whitewood
PY  - 2020
KW  - Biological system modeling
KW  - Data models
KW  - Computational modeling
KW  - Business
KW  - Explainable AI
KW  - Data science
KW  - Consumer electronics
KW  - Finance
KW  - Expert systems
KW  - Asset management
DO  - 10.1002/9781119551966.ch41
PB  - Wiley
SN  - 9781119551867
UR  - http://ieeexplore.ieee.org/document/10953748
AB  - Summary <p>This chapter introduces the key considerations, trends and use cases for artificial intelligence (AI) in capital markets. It is meant for non&#x2010;specialists, and, in particular, decision&#x2010;makers needing to understand the benefits, pitfalls, and how to approach AI projects in the financial domain. Of all the subfields of AI, arguably one of the most popular is machine learning (ML), which applies statistical models to data in order to learn from them and make predictions. The professional field most closely linked to AI, and especially ML, is that of the data scientist. One of the most important tasks that a data scientist can perform, in collaboration with stakeholders, is that of translating a business problem to a specification for the AI&#x2010;based system. The key use cases for AI in capital markets can be split into four groups: customer, trading/portfolio management, regulatory, and operations.</p>
ER  - 

TY  - CONF
TI  - A Study on Application of Explainable AI for Credit Risk Management of an Individual
T2  - 2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS)
SP  - 1
EP  - 7
AU  - R. M
AU  - V. K. M U
PY  - 2024
KW  - Logistic regression
KW  - Analytical models
KW  - Explainable AI
KW  - Decision making
KW  - Predictive models
KW  - Risk management
KW  - Reliability
KW  - Decision trees
KW  - Information technology
KW  - Credit Risk Management
KW  - XAI (Explainable AI)
KW  - LIME
KW  - SHAP
DO  - 10.1109/CSITSS64042.2024.10816861
JO  - 2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS)
IS  - 
SN  - 2767-1097
VO  - 
VL  - 
JA  - 2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS)
Y1  - 7-9 Nov. 2024
AB  - This study investigates the potential of Explainable AI (XAI) to enhance credit risk assessment. By employing machine learning models like Logistic Regression and Decision Tree, coupled with XAI techniques LIME and SHAP, the study aim to identify key factors influencing loan default risk. Our analysis, based on both primary and secondary datasets, reveals that XAI can provide valuable insights into model predictions, leading to more transparent and equitable decision-making in the credit lending process. The study's findings highlight the effectiveness of XAI in improving the reliability and interpretability of credit risk assessments
ER  - 

TY  - CONF
TI  - Early PCOS Diagnosis with Minimal Features via Explainable Machine Learning
T2  - 2025 9th International Symposium on Innovative Approaches in Smart Technologies (ISAS)
SP  - 1
EP  - 5
AU  - S. B. Akben
AU  - H. Yumrutaş
PY  - 2025
KW  - Weight measurement
KW  - Obesity
KW  - Accuracy
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Morphology
KW  - Mental health
KW  - Numerical models
KW  - Decision trees
KW  - Risk management
KW  - Polycystic Ovary Syndrome (PCOS)
KW  - Machine Learning (ML)
KW  - Explainable Artificial Intelligence (XAI)
KW  - Clinical Diagnosis
KW  - Decision Tree
KW  - Home-Based Screening
DO  - 10.1109/ISAS66241.2025.11101894
JO  - 2025 9th International Symposium on Innovative Approaches in Smart Technologies (ISAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 9th International Symposium on Innovative Approaches in Smart Technologies (ISAS)
Y1  - 27-28 June 2025
AB  - Polycystic Ovary Syndrome (PCOS) is a common and complex endocrine disorder affecting women. This study aims to develop explainable machine learning models for the diagnosis of PCOS using minimal clinical data. An anonymized dataset comprising 1,000 records obtained from the Kaggle platform was used to compare eight different algorithms. Ensemble and Decision Tree models achieved the highest accuracy (99.9%), with the Decision Tree selected for its superior explainability. The model’s decision process was based on menstrual irregularity, $\mathrm{BMI}\gt25 \mathrm{~kg} / \mathrm{m}^{2}$, testosterone $\geq 41$ $\mathrm{ng} / \mathrm{dL}$, and antral follicle count $\geq 10.5$. The findings align with known risk factors reported in the literature. Results suggest that highly accurate diagnostic support systems can be developed using a small number of accessible variables, supporting the future development of home-based screening methods.
ER  - 

TY  - CONF
TI  - Decision Support by Interpretable Machine Learning in Acoustic Emission Based Cutting Tool Wear Prediction
T2  - 2021 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)
SP  - 629
EP  - 633
AU  - A. Schmetz
AU  - C. Vahl
AU  - Z. Zhen
AU  - D. Reibert
AU  - S. Mayer
AU  - D. Zontar
AU  - J. Garcke
AU  - C. Brecher
PY  - 2021
KW  - Productivity
KW  - Industries
KW  - Engineering management
KW  - Machine learning
KW  - Acoustic emission
KW  - Industrial engineering
KW  - Sensors
KW  - Predictive maintenance
KW  - condition monitoring
KW  - interpretable ML
KW  - explainable AI
KW  - acoustic emission
DO  - 10.1109/IEEM50564.2021.9673044
JO  - 2021 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)
Y1  - 13-16 Dec. 2021
AB  - Predictive maintenance is a prominent and active field for applications of machine learning in industry in recent years. The health and wear of equipment directly influences the productivity and quality of the production process. Especially in ultra-precision manufacturing, tool wear has a major impact on the achievable quality while the wear itself cannot be measured directly in-process. In this paper we present a machine learning-based classification of the tool wear in-process using acoustic emission sensors. To increase the interpretability of the process – to open the black box model – we apply a feature importance analysis and use the obtained feature importances to provide augmented data representations to the users. These representations increase the transparency of the model's decision process and assist the users in validating the model's decisions and gain new insight into the phenomenon of tool wear itself.
ER  - 

TY  - CONF
TI  - Explainable Prediction of Pedestrians' Distress in the Urban Built Environment
T2  - 2022 56th Asilomar Conference on Signals, Systems, and Computers
SP  - 985
EP  - 989
AU  - J. Kim
AU  - D. Yang
AU  - E. H. Nirjhar
AU  - C. R. Ahn
AU  - T. Chaspari
PY  - 2022
KW  - Temperature distribution
KW  - Machine learning algorithms
KW  - Time series analysis
KW  - Predictive models
KW  - Prediction algorithms
KW  - Physiology
KW  - Skin
KW  - Explainable deep learning
KW  - Wearable sensing
KW  - Physiological responses
KW  - Built environment assessment
DO  - 10.1109/IEEECONF56349.2022.10052038
JO  - 2022 56th Asilomar Conference on Signals, Systems, and Computers
IS  - 
SN  - 2576-2303
VO  - 
VL  - 
JA  - 2022 56th Asilomar Conference on Signals, Systems, and Computers
Y1  - 31 Oct.-2 Nov. 2022
AB  - Physiological signals obtained from wearable sensors has the potential to capture pedestrians' distress caused by physical disorders in the built environment, such as litter, abandoned houses, poorly maintained sidewalks, and graffiti. A limited amount of prior work has demonstrated the feasibility of deep learning models using physiological response data for tracking pedestrians' distress. Yet, the explain ability of such models estimating the distress in a time-continuous manner is not widely investigated. In this context, the objective of this paper is to examine an explainable machine learning algorithm for reliably identifying segments of physiological response data that contribute to the detection of physical disorders in the built environment. The data used in this study include a set of time series of electrodermal activity, electrocardiogram, heart rate, and skin temperature, collected from 67 participants walking on a pre-defined route in College Station, Texas, USA. Time continuous self-reports were recorded retrospectively for each participant over the span of the route. A long short-term memory (LSTM) neural network predicts reported stimuli of distress from participants. Next, the local interpretable model-agnostic explanations (LIME) algorithm was applied to the input time series of the LSTM model yielding an explanation of the corresponding distress label. Results from the model of the LIME algorithm are qualitatively evaluated, which indicate the presence of meaningful segments in the time series that have the greatest influence on the final distress prediction. Findings obtained by this research have the potential to advance our understanding about sources of pedestrians' distress, contributing to promoting urban walkability.
ER  - 

TY  - JOUR
TI  - A Review of Trustworthy and Explainable Artificial Intelligence (XAI)
T2  - IEEE Access
SP  - 78994
EP  - 79015
AU  - V. Chamola
AU  - V. Hassija
AU  - A. R. Sulthana
AU  - D. Ghosh
AU  - D. Dhingra
AU  - B. Sikdar
PY  - 2023
KW  - Artificial intelligence
KW  - Art
KW  - Surveys
KW  - Robustness
KW  - Medical services
KW  - General Data Protection Regulation
KW  - Trusted computing
KW  - Autonomous vehicles
KW  - Medical services
KW  - Internet of Things
KW  - Network security
KW  - Computer security
KW  - Risk management
KW  - Artificial intelligence (AI)
KW  - trustworthy AI (TAI)
KW  - eXplainable AI (XAI)
KW  - autonomous vehicles
KW  - healthcare
KW  - IoT
DO  - 10.1109/ACCESS.2023.3294569
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - The advancement of Artificial Intelligence (AI) technology has accelerated the development of several systems that are elicited from it. This boom has made the systems vulnerable to security attacks and allows considerable bias in order to handle errors in the system. This puts humans at risk and leaves machines, robots, and data defenseless. Trustworthy AI (TAI) guarantees human value and the environment. In this paper, we present a comprehensive review of the state-of-the-art on how to build a Trustworthy and eXplainable AI, taking into account that AI is a black box with little insight into its underlying structure. The paper also discusses various TAI components, their corresponding bias, and inclinations that make the system unreliable. The study also discusses the necessity for TAI in many verticals, including banking, healthcare, autonomous system, and IoT. We unite the ways of building trust in all fragmented areas of data protection, pricing, expense, reliability, assurance, and decision-making processes utilizing TAI in several diverse industries and to differing degrees. It also emphasizes the importance of transparent and post hoc explanation models in the construction of an eXplainable AI and lists the potential drawbacks and pitfalls of building eXplainable AI. Finally, the policies for developing TAI in the autonomous vehicle construction sectors are thoroughly examined and eclectic ways of building a reliable, interpretable, eXplainable, and Trustworthy AI systems are explained to guarantee safe autonomous vehicle systems.
ER  - 

TY  - CONF
TI  - Enhancing Financial Risk Management with Federated AI
T2  - 2024 8th SLAAI International Conference on Artificial Intelligence (SLAAI-ICAI)
SP  - 1
EP  - 6
AU  - V. Dhanawat
AU  - V. Shinde
AU  - V. Karande
AU  - K. Singhal
PY  - 2024
KW  - Training
KW  - Data privacy
KW  - Limiting
KW  - Explainable AI
KW  - Information sharing
KW  - Predictive models
KW  - Data models
KW  - Regulation
KW  - Fraud
KW  - Risk management
KW  - Federated Learning
KW  - Explainable AI
KW  - Fraud Detection
KW  - Data Privacy
KW  - Imbalanced Datasets
KW  - Financial Institutions
KW  - Model Transparency
KW  - Collaborative Learning
KW  - Customer Confidentiality
KW  - Risk Management
DO  - 10.1109/SLAAI-ICAI63667.2024.10844982
JO  - 2024 8th SLAAI International Conference on Artificial Intelligence (SLAAI-ICAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 8th SLAAI International Conference on Artificial Intelligence (SLAAI-ICAI)
Y1  - 18-19 Dec. 2024
AB  - Fraudulent transactions are a persistent challenge for financial institutions, demanding robust detection systems to maintain customer trust. Key obstacles include the rarity of fraud cases, leading to imbalanced datasets, and strict privacy regulations limiting data sharing. Additionally, fraud detection must be transparent to preserve user trust. This research addresses these issues by combining Federated Learning (FL) and Explainable AI (XAI), allowing institutions to collaboratively train models without sharing data, thus protecting privacy while ensuring model transparency and interpretability.
ER  - 

TY  - CONF
TI  - Challenges and Opportunities in Deploying Explainable AI for Financial Risk Assessment
T2  - 2025 International Conference on Pervasive Computational Technologies (ICPCT)
SP  - 382
EP  - 386
AU  - A. I. Akkalkot
AU  - N. Kulshrestha
AU  - G. Sharma
AU  - K. Singh Sidhu
AU  - S. S. Palimkar
AU  - N. K
PY  - 2025
KW  - Support vector machines
KW  - Semantic Web
KW  - Accuracy
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Decision making
KW  - Transforms
KW  - Boosting
KW  - Risk management
KW  - Security
KW  - Explainable AI
KW  - Financial risk evaluation
KW  - credit risk
KW  - Support vector machine
KW  - Gradient boosting
DO  - 10.1109/ICPCT64145.2025.10940643
JO  - 2025 International Conference on Pervasive Computational Technologies (ICPCT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Pervasive Computational Technologies (ICPCT)
Y1  - 8-9 Feb. 2025
AB  - Artificial intelligence (AI) has been used more and more in financial decision-making recently, raising questions about the accountability and transparency of these complex systems. The current study investigates the way Explained Artificial Intelligence (XAI) methods might alleviate these concerns and improve the openness of financial decision-making procedures. Nowadays machine learning algorithms are easier to use than ever before, but creating and deploying systems that facilitate real-world banking services has proved challenging. This is mostly due to the fact that algorithms for machine learning are neither transparent or explainable, two attributes that are essential to creating reliable technology. What sets this study unique is the construction of an explainable artificial intelligence (XAI) model that addresses these accessibility concerns while also serving as an instrument for the establishment of credit risk control policies. This work proposes an explainable artificial intelligence model for financing risk control to measure the risks associated with credit financing via peer-to-peer financing networks. The framework uses Shapley parameters to provide AI forecasts according to significant factors that explain. The Support Vector Machine (SVM) and gradient boosting methods had the greatest accuracy scores, 92.4 and 97.6, accordingly. The accuracy of the model was evaluated on a bigger database, and the findings demonstrated that it regularly achieved high levels of accuracy. The SVM and GBM models achieved accuracies of 94.8 and 97.6, respectively.
ER  - 

TY  - CONF
TI  - Optimizing Resilience with Digital Twin in Three-Phase Grid-Connected Systems
T2  - 2024 IEEE Power & Energy Society General Meeting (PESGM)
SP  - 1
EP  - 5
AU  - M. A. Khan
AU  - V. S. B. Kurukuru
AU  - N. Bayati
AU  - T. Ebel
PY  - 2024
KW  - Adaptation models
KW  - Islanding
KW  - Explainable AI
KW  - Power system stability
KW  - Pulse width modulation
KW  - Stability analysis
KW  - Robustness
KW  - Digital twins
KW  - Intelligent control
KW  - Voltage-source converters
KW  - Digital Twin Modeling
KW  - Voltage Source Converter (VSC)
KW  - Islanding Detection
KW  - Fault Ride Through (FRT)
KW  - eXplainable Artificial Intelligence (XAI)
DO  - 10.1109/PESGM51994.2024.10688497
JO  - 2024 IEEE Power & Energy Society General Meeting (PESGM)
IS  - 
SN  - 1944-9933
VO  - 
VL  - 
JA  - 2024 IEEE Power & Energy Society General Meeting (PESGM)
Y1  - 21-25 July 2024
AB  - The integration of renewable energy sources into power grids necessitates advanced fault ride-through (FRT) strategies. This paper proposes a comprehensive approach to address this need by combining Digital Twin (DT) modeling, eXplainable Artificial Intelligence (XAI) for islanding detection, and adaptive FRT methodologies in a grid-connected system with a Voltage Source Converter (VSC). The system operates in both grid-forming and grid-following modes, with a passive damping technique to enhance stability. The DT model encompasses plant modeling, Pulse Width Modulation (PWM) stage, and closed-loop virtual control crucial for islanding detection and dynamic response. XAI is employed for efficient classification using voltage magnitude, frequency, phase angle, and current. The FRT strategy ensures seamless transition from grid-connected to islanded operation, maintaining uninterrupted power supply and stability. Rigorous testing demonstrates the robustness of the DT model, emphasizing its contribution to intelligent control strategies in power systems.
ER  - 

TY  - CONF
TI  - Use of Interpretable Machine Learning Methods to Predict the Fundamental Period of Masonry Infilled Reinforced Concrete Frame Structures
T2  - 2024 Moratuwa Engineering Research Conference (MERCon)
SP  - 127
EP  - 132
AU  - U. J. Ukwaththa
AU  - T. S. D. Liyanarachchi
AU  - W. K. V. J. B. Kulasooriya
AU  - R. S. S. Ranasinghe
PY  - 2024
KW  - Vibrations
KW  - Support vector machines
KW  - Machine learning
KW  - Nearest neighbor methods
KW  - Predictive models
KW  - Concrete
KW  - Mathematical models
KW  - Decision trees
KW  - Random forests
KW  - Periodic structures
KW  - natural Period
KW  - reinforced concrete
KW  - buildings
KW  - machine learning
KW  - explainable artificial intelligence
DO  - 10.1109/MERCon63886.2024.10688600
JO  - 2024 Moratuwa Engineering Research Conference (MERCon)
IS  - 
SN  - 2691-364X
VO  - 
VL  - 
JA  - 2024 Moratuwa Engineering Research Conference (MERCon)
Y1  - 8-10 Aug. 2024
AB  - Machine learning has been used in predicting the natural period of vibration of reinforced concrete structures. However, their lack of interpretability diminishes the end user’s trust in machine learning predictions. Alternatively, the authors employed four classical machine learning methods coupled with eXplainable Artificial Intelligence (XAI) to forecast the fundamental period of vibration of Reinforced Concrete structures with masonry infills. We used SHapley Additive exPlanations (SHAP) to interpret the models and their predictions. Our analysis indicated that Random Forest $\left(R^{2}=0.999\right)$ was the best predictive model. All four machine learning models were better than the existing equations provided in design standards. As the novelty, SHAP explanations revealed the reasoning behind machine learning predictions. Accordingly, the Number of Storeys and Opening Ratio are the most influential parameters that govern the natural period of reinforced concrete structures with masonry infill walls. Therefore, this study suggests that explainable machine learning can be effective in structural engineering applications to help decision-making.
ER  - 

TY  - CONF
TI  - Trustworthy Battery Management: A Digital Twin Approach Leveraging XAI and Blockchain
T2  - 2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)
SP  - 0448
EP  - 0453
AU  - J. N. Njoku
AU  - C. I. Nwakanma
AU  - J. -M. Lee
AU  - D. -S. Kim
PY  - 2025
KW  - Explainable AI
KW  - NASA
KW  - Battery management systems
KW  - Predictive models
KW  - Batteries
KW  - Blockchains
KW  - Digital twins
KW  - Recording
KW  - Security
KW  - Long short term memory
KW  - battery management system
KW  - blockchain
KW  - digital twin
KW  - explainable AI
KW  - battery
DO  - 10.1109/ICAIIC64266.2025.10920782
JO  - 2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)
IS  - 
SN  - 2831-6983
VO  - 
VL  - 
JA  - 2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)
Y1  - 18-21 Feb. 2025
AB  - This study presents a digital twin framework for predicting the state of health (SoH) in battery management systems (BMS). This framework integrates a single particle model with electrolytes (SPME) and a long-short-term memory (LSTM) network to model battery behaviour based on a NASA battery dataset. To ensure the security of battery data, data is recorded on the Ethereum blockchain and queried when needed for secure prediction. To ensure the interpretability of the predictions, an explainable AI (XAI) approach, SHAP, is employed. Experimentation shows the viability of the proposed framework in accurately predicting the SoH of physical batteries.
ER  - 

TY  - JOUR
TI  - Learning to Comprehend and Trust Artificial Intelligence Outcomes: A Conceptual Explainable AI Evaluation Framework
T2  - IEEE Engineering Management Review
SP  - 230
EP  - 247
AU  - P. E. D. Love
AU  - J. Matthews
AU  - W. Fang
AU  - S. Porter
AU  - H. Luo
AU  - L. Ding
PY  - 2024
KW  - Artificial intelligence
KW  - Organizations
KW  - Australia
KW  - Biological system modeling
KW  - Decision making
KW  - Support vector machines
KW  - Stakeholders
KW  - Artificial intelligence (AI)
KW  - benefits
KW  - evaluation
KW  - explainable artificial intelligence (XAI)
KW  - machine learning (ML)
KW  - stakeholders
DO  - 10.1109/EMR.2023.3342200
JO  - IEEE Engineering Management Review
IS  - 1
SN  - 1937-4178
VO  - 52
VL  - 52
JA  - IEEE Engineering Management Review
Y1  - Feb. 2024
AB  - Explainable artificial intelligence (XAI) is a burgeoning concept. It is gaining prominence as an approach to better understand how artificial intelligence solutions' outputs can improve decision making. Evaluation frameworks to enable organizations to understand XAIs what, why, how, and when are yet to be developed. Thus, we aim to fill this void by developing a conceptual content, context, process, and outcome (CCPO) evaluation framework to justify XAIs adoption and effective management using construction organizations as a backdrop for the article's setting. After introducing and describing the proposed novel CCPO framework for operationalizing XAI, we discuss its implications for future research. The contributions of our article are twofold: First, it highlights the need for organizations to embrace and enact XAI so that decision makers and stakeholders can better understand why and how a specific prediction materializes; and second, it provides a frame of reference for organizations to realize the business value and benefits of XAI.
ER  - 

TY  - CONF
TI  - Machine Learning Approaches in Cervical Cancer Research: A Comprehensive Literature Review
T2  - 2025 3rd Cognitive Models and Artificial Intelligence Conference (AICCONF)
SP  - 1
EP  - 4
AU  - M. Yüksel
AU  - T. Ozseven
PY  - 2025
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Feature extraction
KW  - Prediction algorithms
KW  - Distance measurement
KW  - Classification algorithms
KW  - Ensemble learning
KW  - Risk management
KW  - Epidemiology
KW  - Cervical cancer
KW  - cervical cancer
KW  - machine learning
KW  - artificial intelligence
KW  - human papillomavirus (HPV)
KW  - early diagnosis
KW  - Pap smear
KW  - colposcopy
KW  - feature selection
KW  - ensemble learning
KW  - explainable artificial intelligence (XAI)
KW  - medical image processing
KW  - risk prediction
KW  - clinical decision support systems
KW  - epidemiology
KW  - data balancing
DO  - 10.1109/AICCONF64766.2025.11064277
JO  - 2025 3rd Cognitive Models and Artificial Intelligence Conference (AICCONF)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 3rd Cognitive Models and Artificial Intelligence Conference (AICCONF)
Y1  - 13-14 June 2025
AB  - Cervical cancer (CrC) continues to pose a significant global threat to women’s health and accounts for a considerable share of the worldwide cancer burden. Early diagnosis and effective management strategies play a critical role in reducing the morbidity and mortality rates associated with the disease. This review aims to comprehensively evaluate the epidemiology, etiology, traditional screening and diagnostic methods, and the current and potential applications of machine learning (ML) and artificial intelligence (AI) algorithms in cervical cancer. The reviewed studies cover a broad spectrum, ranging from the role of Human Papillomavirus (HPV) infection in disease progression to cancer cell classification, prediction of treatment response, and individual risk assessment. This review highlights the transformative potential of ML and AI techniques in the fight against cervical cancer, demonstrating how advanced methods such as feature selection, ensemble learning, and explainable artificial intelligence (XAI) enhance diagnostic accuracy and clinical applicability.
ER  - 

TY  - CONF
TI  - Reliable electricity distribution using a digital twin based on explainable artificial intelligence
T2  - 2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)
SP  - 84
EP  - 85
AU  - R. L. Olsen
AU  - H. P. Schwefel
AU  - A. L. Madsen
PY  - 2024
KW  - Location awareness
KW  - Computers
KW  - Explainable AI
KW  - Digital twins
KW  - Smart grids
KW  - Bayes methods
KW  - Reliability
DO  - 10.1109/SmartGridComm60555.2024.10738050
JO  - 2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)
IS  - 
SN  - 2474-2902
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)
Y1  - 17-20 Sept. 2024
AB  - In this short paper we present the project Reliable Electricity Distribution utilizing a Digital Twin based on eXplainable Artificial Intelligence (ReDistXAI). Target of the project is to successfully apply methods of explainable Artificial Intelligence (XAI) for the detection of input data anomalies for the digital twin of the electricity distribution grid. As a second use, the detection and localization of grid faults via XAI approaches will be assessed. The paper explains the two use-cases and elaborates the benefits and challenges when applying XAI methods in the form of Bayesian networks to these use-cases.
ER  - 

TY  - CONF
TI  - Smart Farming with Explainable AI for Effective Agricultural Decision Making
T2  - 2025 12th International Conference on Computing for Sustainable Global Development (INDIACom)
SP  - 1
EP  - 6
AU  - Suprit
AU  - A. Chaudhary
PY  - 2025
KW  - Smart agriculture
KW  - Productivity
KW  - Explainable AI
KW  - Weather forecasting
KW  - Predictive models
KW  - Resource management
KW  - Artificial intelligence
KW  - Sustainable development
KW  - Crop yield
KW  - Farming
KW  - Agricultural Decision Making
KW  - Machine Learning
KW  - Deep Learning
KW  - Crop Yield Prediction
KW  - Artificial Intelligence in Agriculture
KW  - Internet of Things
KW  - Resource Management
KW  - Sustainability in Agriculture
KW  - Smart Farming
KW  - Explainable AI (XAI)
DO  - 10.23919/INDIACom66777.2025.11115165
JO  - 2025 12th International Conference on Computing for Sustainable Global Development (INDIACom)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 12th International Conference on Computing for Sustainable Global Development (INDIACom)
Y1  - 2-4 April 2025
AB  - Farming has been one of the most essential activity for the survival of the humans. Smart Farming is performing all those traditional farming activities using the modern technologies and in a more efficient way. Modern Technologies like the IoT, AI are very helpful in making more effective decisions, forecasting different aspects of farming like weather forecasting, predicting the growth of the crops with the help of the AI and the past farming data. Also, the incorporation of the explainable Artificial Intelligence (XAI) into smart farming has made a significant change in agricultural decision making. The XAI is proved to be very beneficial in improving the performance in farming. This paper analyzes the different ways how of the smart farming can be implemented with the use of technologies. Also, how the XAI can play a potential role in maximizing the output of farming, making decisions support systems to facilitate farmers’ practice precision farming. Ultimately, Smart Farming with XAI holds the potential to enhance the productivity and sustainability in agriculture and global food security.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence for Maize Disease Diagnostics
T2  - 2023 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
SP  - 1
EP  - 6
AU  - M. Banadda
AU  - N. K. Aloysius
AU  - S. Nakazzi
AU  - O. B. Ernest
AU  - M. S. Owekitiibwa
AU  - G. Marvin
PY  - 2023
KW  - Training
KW  - Productivity
KW  - Analytical models
KW  - Explainable AI
KW  - Computational modeling
KW  - Transfer learning
KW  - Predictive models
KW  - Explainable AI
KW  - Transfer Learning
KW  - Maize Streak
KW  - Maize Leaf Blight
DO  - 10.1109/CSDE59766.2023.10487660
JO  - 2023 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
Y1  - 4-6 Dec. 2023
AB  - This study focuses on the use of machine learning techniques, specifically Explainable AI and Deep Learning, to diagnose two prevalent diseases in maize crops: maize streak and maize leaf blight. These diseases have been major obstacles for farmers in effectively managing and identifying infections, leading to decreased harvest quantity and quality. By developing and training a customized model using supervised machine learning, combined with Transfer Learning using pretrained models like VGG19, MobileNet, we aim to improve maize disease diagnostics. The accuracy of the generated models was assessed, and the best performing model is selected for further analysis utilizing Explainable AI methods. The final step involves deploying the model on a mobile application, with the web application serving as a platform for visualizing the stored data collected through the mobile app in an external database. The ultimate objective of this research is to equip farmers with the necessary tool to enhance agricultural productivity by enabling precise and efficient diagnosis of maize diseases.
ER  - 

TY  - JOUR
TI  - Explainable Artificial Intelligence: Counterfactual Explanations for Risk-Based Decision-Making in Construction
T2  - IEEE Transactions on Engineering Management
SP  - 10667
EP  - 10685
AU  - J. Zhan
AU  - W. Fang
AU  - P. E. D. Love
AU  - H. Luo
PY  - 2024
KW  - Predictive models
KW  - Decision making
KW  - Artificial intelligence
KW  - Principal component analysis
KW  - Kernel
KW  - Analytical models
KW  - Computational modeling
KW  - Counterfactual explanations (CFE)
KW  - decision-making
KW  - explainable artificial intelligence (XAI)
KW  - risk
KW  - tunneling
DO  - 10.1109/TEM.2023.3325951
JO  - IEEE Transactions on Engineering Management
IS  - 
SN  - 1558-0040
VO  - 71
VL  - 71
JA  - IEEE Transactions on Engineering Management
Y1  - 2024
AB  - Artificial intelligence (AI) approaches, such as deep learning models, are increasingly used to determine risks in construction. However, the black-box nature of AI models makes their inner workings difficult to understand and interpret. Deploying explainable artificial intelligence (XAI) can help explain why and how the output of AI models is generated. This article addresses the following research question: How can we accurately identify the critical factors influencing tunnel-induced ground settlement and provide counterfactual explanations to support risk-based decision-making? We apply an XAI approach using counterfactual explanations to help understand decision-making surrounding risks when considering control ground settlement. Our approach consists of a: 1) construction of Kernel principal components analysis-based deep neural network (DNN) model; 2) generation of counterfactual explanations; 3) analysis of risk prediction and assessment factors' importance, necessity, and sufficiency. We apply our approach to the San-yang road tunnel project in Wuhan, China. The results demonstrate that the KPCA-DNN model better predicted ground settlement based on high-dimensional input features than the baseline model (i.e., AdaBoost and RandomForest). The bubble chamber pressure→ cutter-head speed→ equipment inclination is also identified as the primary risk path. Our findings indicate that using counterfactual explanations enables transparency and trust in AI-based risk models to be acquired. Moreover, our approach can help site managers, engineers, and tunnel-boring machine operators understand how to manage better and mitigate the risk of ground settlement.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence for Prediction of Diabetes using Stacking Classifier
T2  - 2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)
SP  - 1
EP  - 5
AU  - A. D. B
AU  - K. N
PY  - 2024
KW  - Support vector machines
KW  - Explainable AI
KW  - Computational modeling
KW  - Stacking
KW  - Predictive models
KW  - Nearest neighbor methods
KW  - Data models
KW  - diabetes
KW  - missing values
KW  - imbalanced dataset
KW  - anomalies
KW  - ensemble model
KW  - XAI
KW  - LIME
DO  - 10.1109/CONECCT62155.2024.10677165
JO  - 2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)
IS  - 
SN  - 2766-2101
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)
Y1  - 12-14 July 2024
AB  - The escalating incidence of diabetes in globe has prompted the medical sector to explore innovative approaches aimed at enhancing their medical technologies. Integrating machine learning (ML) algorithms into clinical care can play a pivotal role in early diabetes detection, thus helping to mitigate the potential health complications associated with the condition. Moreover, the latest Explainable Artificial Intelligence (XAI) techniques have the potential to facilitate user understanding and trust in AI-driven decisions. This work proposes a method for the precise detection of diabetes through meticulous data preprocessing, the construction of an ensemble ML algorithm and the interpretation of the model's outcomes using XAI. Early detection of diabetes enables timely intervention through medication, dietary adjustments, and lifestyle modifications, leading to improved blood sugar regulation and reduced risk of diabetes-related complications. The proposed work uses preprocessing techniques like K Nearest Neighbors (KNN) imputation, One-Class Support Vector Machine (OCSVM) anomaly detection, Synthetic Minority Over-Sampling Technique and Edited Nearest Neighbour (SMOTE + ENN) data balancing technique, and ensemble model has KNN, Support Vector Machine (SVM), and eXtreme gradient boosting (XGB) as baseline models and Random Forest (RF) as meta classifier. This research underscores the importance of building a reliable model for diabetes prediction and interpreting the results using the Local Interpretable Model-Agnostic Explanation (LIME) technique. This work addresses challenges such as missing data, anomalies, data imbalance, and appropriate model selection, while highlighting the significance of comprehending the model's outcomes. The proposed ensemble model achieved an accuracy of 97%.
ER  - 

TY  - CONF
TI  - A Comprehensive Survey on Enhancing Digital Twin Security Systems with Explainable AI Techniques
T2  - 2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)
SP  - 535
EP  - 542
AU  - B. Lalithadevi
AU  - S. Krishnaveni
PY  - 2025
KW  - Surveys
KW  - Measurement
KW  - Explainable AI
KW  - Neural networks
KW  - Real-time systems
KW  - Digital twins
KW  - Security
KW  - Internet of Things
KW  - Optimization
KW  - Monitoring
KW  - Digital twin
KW  - Explainable AI
KW  - Cyber security
KW  - Model interpretability
DO  - 10.1109/IDCIOT64235.2025.10915028
JO  - 2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)
Y1  - 5-7 Feb. 2025
AB  - Digital Twin (DT) technology has emerged as a cornerstone in modern engineering, enabling real-time digital replication of physical assets for monitoring, diagnostics, and optimization. However, as DTs become increasingly integrated into critical infrastructure, their security has become a paramount concern. This paper presents a comprehensive survey on the integration of Explainable Artificial Intelligence (XAI) into Digital Twin security, highlighting the potential of XAI in enhancing the transparency, interpretability, and trustworthiness of AI - driven security mechanisms in DTs. We explore various XAI techniques, including model- agnostic approaches, interpretable neural networks, and rule-based systems, detailing their potential applications in bolstering the security of Digital Twins. Furthermore, we identify several key challenges in implementing XAI for DT security, such as the need for standardized metrics for interpretability, the trade-offs between model complexity and explainability, and the potential for adversarial attacks on explainable models. Through our analysis, we highlight the critical research gaps, including the security of Digital Twin (DT) systems and explores the integration of Explainable AI (XAI) to enhance transparency, interpretability, and trustworthiness. Outcomes include detailed XAI applications for DT security, identification of limitations, and a roadmap for future research. We discuss various XAI techniques, their applications, and the current challenges in implementing XAI for DT security. Our survey identifies key research gaps and offers a roadmap for future research in this emerging field.
ER  - 

TY  - CONF
TI  - Artificial Intelligence (AI) Standards and QA in Semiconductor Manufacturing
T2  - 2025 IEEE Conference on Artificial Intelligence (CAI)
SP  - 1475
EP  - 1479
AU  - R. C. Palsaniya
PY  - 2025
KW  - Technological innovation
KW  - Explainable AI
KW  - Fault detection
KW  - Quality control
KW  - Semiconductor device manufacture
KW  - Reliability
KW  - Stakeholders
KW  - Artificial intelligence
KW  - Sustainable development
KW  - Standards
KW  - AI Standards
KW  - Semiconductor Manufacturing
KW  - quality control
DO  - 10.1109/CAI64502.2025.00252
JO  - 2025 IEEE Conference on Artificial Intelligence (CAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 IEEE Conference on Artificial Intelligence (CAI)
Y1  - 5-7 May 2025
AB  - Artificial Intelligence (AI) is revolutionizing semiconductor manufacturing with enhanced efficiency, quality, and fault detection. However, with the widespread application of AI, industry-standard practices are required to ensure reliability, transparency, and compliance. This paper discusses AI's role in semiconductor quality control, the need for strict standards, and manufacturing implementation issues. It explores cutting-edge applications, including federated learning, explainable AI (XAI), and simulation-based fault detection. Lastly, it provides an overview of AI standard development for semiconductor manufacturing.
ER  - 

TY  - CONF
TI  - Enhancing Milk Yield Forecasting in Dairy Farming Using an Interpretable Machine Learning Framework
T2  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
SP  - 1549
EP  - 1557
AU  - S. B
AU  - S. N. S. R
AU  - S. K. Thangavel
AU  - S. K
AU  - M. Ramasamy
PY  - 2025
KW  - Support vector machines
KW  - Productivity
KW  - Sentiment analysis
KW  - Dairy products
KW  - Explainable AI
KW  - Machine learning
KW  - Predictive models
KW  - Nearest neighbor methods
KW  - Resource management
KW  - Farming
KW  - Machine Learning
KW  - Explainable AI
KW  - Milk Yield Prediction
KW  - Optimization
KW  - SHAP
KW  - Dairy Farming
DO  - 10.1109/ICSADL65848.2025.10933035
JO  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
Y1  - 18-20 Feb. 2025
AB  - This study employs machine learning techniques and explainable AI (XAI) to enhance milk yield predictions and optimizes dairy farming practices. Split k-means clustering, KNN, SVM combined with k-means, and regression techniques such as binomial, polynomial, and logistic regression are all used in this methodology. KNN in conjunction with SVM improves prediction accuracy even further. Explainable AI techniques, such as SHAP, provide clarity by emphasizing feature significance, while diverse representations improve data scrutiny. These methods offer practical insights into improving dairy farming operations, ensuring efficient resource utilization, and increasing milk output.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence (XAI) in Insurance
T2  - 2025 International Conference on Pervasive Computational Technologies (ICPCT)
SP  - 305
EP  - 310
AU  - N. Verma
AU  - A. K. Varshney
AU  - R. K. Singhal
AU  - M. P. Gaur
AU  - A. Garg
AU  - S. Das
PY  - 2025
KW  - Industries
KW  - Technological innovation
KW  - Explainable AI
KW  - Insurance
KW  - Standardization
KW  - Stakeholders
KW  - Risk management
KW  - Artificial intelligence
KW  - System analysis and design
KW  - Systematic literature review
KW  - Explicit Risk management
KW  - data governance
KW  - insurance value chain
KW  - artificial intelligence
KW  - and machine learning
DO  - 10.1109/ICPCT64145.2025.10939062
JO  - 2025 International Conference on Pervasive Computational Technologies (ICPCT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Pervasive Computational Technologies (ICPCT)
Y1  - 8-9 Feb. 2025
AB  - Explainable Artificial Intelligence (XAI) makes human-machine interactions more transparent and understandable. Given its access to vast amounts of private policyholder data and its critical role in promoting innovation and societal growth, the insurance sector has a great opportunity to demonstrate the capabilities of XAI. The present state of artificial intelligence (AI) applications in insurance practices and research is examined in this article, with an emphasis on their explainability. Through a thorough analysis of 419 primary research papers from 2000 to 2021 from a variety of scholarly sources, the study finds 103 publications that provide an overview of the state of XAI in insurance literature. Using an analysis and classification of these papers, it highlights the use of XAI approaches in critical insurance operations such as claims management, underwriting, and actuarial pricing, among others. In the insurance industry, the study highlights information distillation and rule extraction as the main XAI methodologies. It also promotes the development of interpretable XAI models. By breaking down complex structures into simpler, rule-based ones, this method makes it easier to design models that are easier to understand. By incorporating moral values, openness, and confidence into system design, XAI is a substantial breakthrough in AI. Stakeholders including industry practitioners, regulators, and XAI developers can gain important insights by evaluating these emphasis points within the insurance sector and determining whether regions are ready for more development. By analyzing its applications in the insurance industry, this study provides a fresh viewpoint in addition to enhancing the multidisciplinary understanding of applied XAI. Furthermore, by offering a more nuanced definition based on a thorough analysis of XAI literature within the insurance sector, it adds to the continuing conversations about a precise definition of XAI.
ER  - 

TY  - CONF
TI  - Predictive Models for Chronic Cardiac Disease with LIME and SHAP
T2  - 2024 International Conference on Augmented Reality, Intelligent Systems, and Industrial Automation (ARIIA)
SP  - 1
EP  - 6
AU  - S. Baral
AU  - S. Satpathy
AU  - R. N. Satpathy
PY  - 2024
KW  - Decision support systems
KW  - Automation
KW  - Cardiac disease
KW  - Medical services
KW  - Predictive models
KW  - Boosting
KW  - Complexity theory
KW  - Risk management
KW  - Logic
KW  - Intelligent systems
KW  - Interpretable machine learning
KW  - LIME
KW  - SHAP
KW  - Gradient Boosting and XGBoost
DO  - 10.1109/ARIIA63345.2024.11051831
JO  - 2024 International Conference on Augmented Reality, Intelligent Systems, and Industrial Automation (ARIIA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Augmented Reality, Intelligent Systems, and Industrial Automation (ARIIA)
Y1  - 20-21 Dec. 2024
AB  - With growing machine learning model complexity, there' is a rising need for transparent and interpretable techniques to understand their decision-methodologies frameworks, particularly in vital fields such as healthcare. This paper focuses on applying Local Understandable Model Agnostic Explanations (LIME) and SHapley Additive exPlanation (SHAP) techniques to interpret predictive models for chronic cardiac disease identification, aiming to clarify the model's responses locally and globally. LIME provides local interpretability by offering insights into individual predictions, allowing healthcare professionals to understand circumstances surrounding the decision why a specific decision was made for a particular situation. In contrast, SHAP delivers global interpretability by explaining the overall behaviour of the model and its key predictive features across the entire dataset. Through extensive experimentation on clinical datasets, we evaluate the effectiveness of these methods in providing actionable insights to healthcare professionals, facilitating trust and understanding of the predictive models. Our findings contribute to advancing interpretability research in healthcare and offer valuable guidance for employing these techniques in clinical decision support systems for chronic cardiac disease management.
ER  - 

TY  - CONF
TI  - Industrial Asset Management and Secure Sharing for an XAI Manufacturing Platform
T2  - 2023 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)
SP  - 1
EP  - 7
AU  - S. Reji
AU  - J. Hetterich
AU  - S. Pitsios
AU  - V. Gkolemi
AU  - S. Perez-Castanos
AU  - M. Pertselakis
PY  - 2023
KW  - Training
KW  - Technological innovation
KW  - Systematics
KW  - Architecture
KW  - Buildings
KW  - Data models
KW  - Asset management
KW  - Explainable Artificial Intelligence(XAI)
KW  - REST API
KW  - Data Provenance
KW  - Access Policies
KW  - Intellectual property rights (IPRs)
DO  - 10.1109/ICE/ITMC58018.2023.10332309
JO  - 2023 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)
IS  - 
SN  - 2693-8855
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)
Y1  - 19-22 June 2023
AB  - Explainable AI is an emerging field that aims to address how black-box decisions of AI systems are made, by attempting to understand the steps and models involved in this decision-making. Explainable AI in manufacturing is supposed to deliver predictability, agility, and resiliency across targeted manufacturing apps. In this context, large amounts of data, which can be of high sensitivity and various formats need to be securely and efficiently handled. This paper proposes an Asset Management and Secure Sharing solution tailored to the Explainable AI and Manufacturing context in order to tackle this challenge. The proposed asset management architecture enables an extensive data management and secure sharing solution for industrial data assets. Industrial data can be pulled, imported, managed, shared, and tracked with a high level of security using this design. This paper describes the solution´s overall architectural design and gives an overview of the functionalities and incorporated technologies of the involved components, which are responsible for data collection, management, provenance, and sharing as well as for overall security.
ER  - 

TY  - CONF
TI  - Explaining the Identification of Granular Crack with Deep Learning and XAI
T2  - 2024 IEEE Region 10 Symposium (TENSYMP)
SP  - 1
EP  - 6
AU  - A. Pratap
AU  - N. Sardana
PY  - 2024
KW  - Training
KW  - Deep learning
KW  - Analytical models
KW  - Accuracy
KW  - Explainable AI
KW  - Scalability
KW  - Transforms
KW  - Quality control
KW  - Convolutional neural networks
KW  - Testing
KW  - Crack Detection
KW  - Deep Learning
KW  - Explainable AI (XAI)
KW  - Industry 5.0
DO  - 10.1109/TENSYMP61132.2024.10752279
JO  - 2024 IEEE Region 10 Symposium (TENSYMP)
IS  - 
SN  - 2642-6102
VO  - 
VL  - 
JA  - 2024 IEEE Region 10 Symposium (TENSYMP)
Y1  - 27-29 Sept. 2024
AB  - Precise detection of granular fractures is crucial for various engineering applications, and Deep Learning approaches can transform imaging methodologies and material analysis. This study employed a convolutional neural network (CNN) that incorporated explainability elements to improve the trustworthiness and transparency of the model in classifying granular cracks. After conducting thorough model comparisons and analyses, a bespoke model was chosen. Extensive model comparisons and analyses led to the selection of a custom model with 91% training accuracy and optimized parameters. Fine-tuning of six pre-trained models identified VGG-16 as the top performer, achieving 99.96% training accuracy and 97.7% testing accuracy. Explainable AI (XAI) techniques, particularly the insertion method, provided robust interpretability, with attributed scores of 0.96 for insertion, 0.69 for deletion, and 0.37 for fidelity. Visualization using various attribution mapping methods reinforced the trustworthiness of this work in granular crack identification.
ER  - 

TY  - CONF
TI  - Application of explainable artificial intelligence to analyze basic features of a tender
T2  - 2023 3rd International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)
SP  - 1
EP  - 6
AU  - M. F. Molina-Miranda
AU  - X. Acaro
AU  - M. Molina
AU  - M. Quiñonez
AU  - G. Alvarez
AU  - J. Fernandez-Olivares
AU  - R. Pérez
PY  - 2023
KW  - Procurement
KW  - Logistic regression
KW  - Mechatronics
KW  - Data visualization
KW  - Forestry
KW  - Data models
KW  - Classification algorithms
KW  - explainable artificial intelligence
KW  - machine learning
KW  - tender
KW  - Random Forest
KW  - LIME
KW  - SHAP
DO  - 10.1109/ICECCME57830.2023.10253063
JO  - 2023 3rd International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 3rd International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)
Y1  - 19-21 July 2023
AB  - This work consists of predicting tenders based on machine learning techniques using classification algorithms such as Decision Tree, Logistic Regression, Random Forest, and explainability techniques LIME and SHAP. The objective is to improve transparency and efficiency in the supplier selection process for public contracts. A comparison is made between the classification algorithms to determine which performs best, resulting in the Random Forest model. To improve the model’s explainability, LIME and SHAP techniques are used, which allow visualizing and understanding the impact of each variable on the model’s decision. The proposed analysis can be a valuable tool for SMEs (Small and medium-sized enterprises) looking to participate in public procurement more efficiently. In conclusion, this project demonstrated that the LIME technique allows for data interpretation in a simpler and more understandable way with the Random Forest classification model.
ER  - 

TY  - CONF
TI  - Harnessing Explainable AI in Railway: A Decision Tree-Based Approach
T2  - 2025 20th European Dependable Computing Conference Companion Proceedings (EDCC-C)
SP  - 119
EP  - 124
AU  - M. Barbareschi
AU  - A. Emmanuele
AU  - N. Mazzocca
AU  - F. R. di Torrepadula
PY  - 2025
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Time series analysis
KW  - Predictive models
KW  - Rail transportation
KW  - Safety
KW  - Decision trees
KW  - Reliability
KW  - Synthetic data
KW  - eXplainable Artificial Intelligence
KW  - SHAP
KW  - XG-Boost
KW  - Railway Domain
KW  - Time Series Forecasting
DO  - 10.1109/EDCC-C66476.2025.00043
JO  - 2025 20th European Dependable Computing Conference Companion Proceedings (EDCC-C)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 20th European Dependable Computing Conference Companion Proceedings (EDCC-C)
Y1  - 8-11 April 2025
AB  - In recent years, Artificial Intelligence has gained significant popularity for solving various tasks, including service optimization, system monitoring, and industrial control. Despite its success, adoption in critical systems, such as the railway domain, remains limited. This is primarily due to the high stakes in these systems, where failures can lead to damage to critical infrastructure and risks to human lives. As a result, software in these domains must be deterministic, ensuring that all behaviors can be statically verified. Machine Learning models, due to their complexity, are often perceived as black-box systems and exhibit seemingly nondeterministic behavior, making their integration into such infrastructure challenging. To address this issue, one potential solution is the use of eXplainable Artificial Intelligence (XAI) techniques, which enable the construction of human-interpretable explanations for model predictions. In this paper, we propose a time-series prediction framework for the railway domain by combining XGBoost, a highly accurate tree-based model, with SHAP, a widely used explainability technique.
ER  - 

TY  - JOUR
TI  - Explainable Machine Learning Model for Alzheimer Detection Using Genetic Data: A Genome-Wide Association Study Approach
T2  - IEEE Access
SP  - 95091
EP  - 95105
AU  - T. Khater
AU  - S. Ansari
AU  - A. Saad Alatrany
AU  - H. Alaskar
AU  - S. Mahmoud
AU  - A. Turky
AU  - H. Tawfik
AU  - E. Almajali
AU  - A. Hussain
PY  - 2024
KW  - Alzheimer's disease
KW  - Genetics
KW  - Artificial intelligence
KW  - Genomics
KW  - Bioinformatics
KW  - Support vector machines
KW  - Feature extraction
KW  - Quality control
KW  - Alzheimer
KW  - artificial intelligence
KW  - GWAS
KW  - quality control
KW  - XAI
DO  - 10.1109/ACCESS.2024.3410135
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Recent research has revealed that using machine learning systems for the analysis of genetic data could reliably detect Alzheimer’s disease. The interpretability of these models, however, has been a challenge, as they frequently provided little insight into the features that contribute to their predictions. Explainable machine learning has been presented as a solution to this problem since it enables the identification of significant attributes and gives a clearer method of making predictions. In this study, Genome-Wide Association Studies were used to recognize genetic variants associated with Alzheimer’s disease, utilizing the Alzheimer’s Disease Neuroimaging Initiative dataset and quality control methods to ensure the validity and reliability of the findings. The results indicate strong connections between certain genetic variations and Alzheimer’s disease, highlighting the potential of Genome-Wide Association Studies as a valuable tool for identifying and predicting this disease. After studying and analyzing the genetic data, machine learning algorithms are utilized to train a model to detect Alzheimer. The Support Vector Machine achieved 89% accuracy as the best-performing model. Explainable machine learning has the potential to increase the accuracy and interpretability of Alzheimer’s disease detection models, giving significant insights for both academics and physicians. The explanation of the support vector machine model reveals that rs4821510 is the most important SNP in detecting AD. On top of that, the SHAP method shows that rs429358 is an indication for Alzheimer’s disease and rs4821510 presents in the healthy ones. These findings suggest that explainable machine learning can play an important role in accurately detecting Alzheimer’s disease and identifying critical genetic markers associated with the disease.
ER  - 

TY  - JOUR
TI  - Explainable Predictive Maintenance: A Survey of Current Methods, Challenges and Opportunities
T2  - IEEE Access
SP  - 57574
EP  - 57602
AU  - L. Cummins
AU  - A. Sommers
AU  - S. B. Ramezani
AU  - S. Mittal
AU  - J. Jabour
AU  - M. Seale
AU  - S. Rahimi
PY  - 2024
KW  - Explainable AI
KW  - Predictive maintenance
KW  - Surveys
KW  - Prognostics and health management
KW  - Machine learning
KW  - Computational modeling
KW  - Predictive models
KW  - Artificial intelligence
KW  - Fourth Industrial Revolution
KW  - Fifth Industrial Revolution
KW  - Explainable artificial intelligence (XAI)
KW  - predictive maintenance
KW  - industry 4.0
KW  - industry 5.0
KW  - interpretable machine learning
KW  - PRISMA
DO  - 10.1109/ACCESS.2024.3391130
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Predictive maintenance is a well studied collection of techniques that aims to prolong the life of a mechanical system by using artificial intelligence and machine learning to predict the optimal time to perform maintenance. The methods allow maintainers of systems and hardware to reduce financial and time costs of upkeep. As these methods are adopted for more serious and potentially life-threatening applications, the human operators need trust the predictive system. This attracts the field of Explainable AI (XAI) to introduce explainability and interpretability into the predictive system. XAI brings methods to the field of predictive maintenance that can amplify trust in the users while maintaining well-performing systems. This survey on explainable predictive maintenance (XPM) discusses and presents the current methods of XAI as applied to predictive maintenance while following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines. We categorize the different XPM methods into groups that follow the XAI literature. Additionally, we include current challenges and a discussion on future research directions in XPM.
ER  - 

TY  - JOUR
TI  - An Interrogative Survey of Explainable AI in Manufacturing
T2  - IEEE Transactions on Industrial Informatics
SP  - 7069
EP  - 7081
AU  - Z. Alexander
AU  - D. H. Chau
AU  - C. Saldaña
PY  - 2024
KW  - Manufacturing
KW  - Biological system modeling
KW  - Surveys
KW  - Artificial intelligence
KW  - Predictive models
KW  - Data models
KW  - Industries
KW  - Artificial intelligence (AI)
KW  - deep learning (DL)
KW  - explainable artificial intelligence (XAI)
KW  - human–computer interaction (HCI)
KW  - industry 4.0
KW  - interpretable artificial intelligence (IAI)
KW  - machine learning (ML)
KW  - manufacturing
DO  - 10.1109/TII.2024.3361489
JO  - IEEE Transactions on Industrial Informatics
IS  - 5
SN  - 1941-0050
VO  - 20
VL  - 20
JA  - IEEE Transactions on Industrial Informatics
Y1  - May 2024
AB  - Artificial intelligence (AI) is a driving force behind Industry 4.0 in manufacturing. Specifically, machine learning has been applied to all parts of the manufacturing process: from product design optimization to anomaly detection for quality control. Explainable AI (XAI) and interpretable AI (IAI) methods have been developed to provide transparency into how models make decisions. This survey presents a thorough review of who, what, when, where, why, and how both IAI and XAI methods have been used in manufacturing. Due to the multidisciplinary nature of manufacturing, this work provides the results from a systematic literature review that surveyed papers from highly rated venues in multiple manufacturing and AI-related fields to give the reader a holistic view of the space. This survey is intended to help both individuals from academia and industry quickly understand the applications, areas of research, and future work involved with creating explainable industrial solutions.
ER  - 

TY  - CONF
TI  - Enhancing Wind Turbine Power Curve Monitoring with eXplainable Artificial Intelligence Techniques
T2  - IEEE EUROCON 2023 - 20th International Conference on Smart Technologies
SP  - 758
EP  - 763
AU  - D. Astolfi
AU  - F. De Caro
AU  - A. Vaccaro
PY  - 2023
KW  - Mechanical sensors
KW  - Blades
KW  - Wind speed
KW  - Input variables
KW  - Velocity control
KW  - Rotors
KW  - Predictive models
KW  - Wind Energy
KW  - Wind Turbines
KW  - eXplainable Artificial Intelligence
KW  - Power Curve
KW  - Monitoring
KW  - Data Analysis
DO  - 10.1109/EUROCON56442.2023.10199016
JO  - IEEE EUROCON 2023 - 20th International Conference on Smart Technologies
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEEE EUROCON 2023 - 20th International Conference on Smart Technologies
Y1  - 6-8 July 2023
AB  - The power of a wind turbine has a multivariate dependence on environmental conditions and working parameters. Hence, traditional univariate models which take as input solely the wind speed may fail in effectively monitoring the performance of wind turbines. To address this limitation, the use of multivariate wind power curves, which consider multiple variables and output power, should be investigated. For this reason, the Authors of this paper advocate the role of eXplainable Artificial Intelligence (XAI) methods in the construction of multivariate data-driven power curve models. A sequential features selection is applied and the Shapley coefficients for the various input sensors are computed on a real-world data set, which is composed of a set of covariates larger than the state of the art in the literature. The experimental results of this work lead to include in the model highly explanatory variables related to the mechanical or electrical control, such as the rotational speed and the blade pitch pressures, which use has not been contemplated in the literature before.
ER  - 

TY  - JOUR
TI  - Explainable Deep Learning Models With Gradient-Weighted Class Activation Mapping for Smart Agriculture
T2  - IEEE Access
SP  - 83752
EP  - 83762
AU  - L. -D. Quach
AU  - K. N. Quoc
AU  - A. N. Quynh
AU  - N. Thai-Nghe
AU  - T. G. Nguyen
PY  - 2023
KW  - Data models
KW  - Computational modeling
KW  - Predictive models
KW  - Deep learning
KW  - Training
KW  - Agriculture
KW  - Image recognition
KW  - Artificial intelligence
KW  - Explainable artificial intelligence
KW  - XAI
KW  - agriculture
KW  - grad-CAM
KW  - deep learning
KW  - explainable AI
DO  - 10.1109/ACCESS.2023.3296792
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - Explainable Artificial Intelligence is a recent research direction that aims to explain the results of the Deep learning model. However, many recent research need to go into depth in evaluating the effectiveness of deep learning models in classifying image objects. For that reason, the research proposes two stages in the process of applying Explainable Artificial Intelligence, including: (1) assessing the accuracy of the deep learning model through evaluation methods, (2) using Grad-CAM for model interpretation aims to evaluate the feature detection ability of an image when recognized by deep learning models. The deep learning models included in the evaluation included VGG16, ResNet50, ResNet50V2, Xception, EfficientNetV2, InceptionV3, DenseNet201, MobileNetV2, MobileNet, NasNetMobile, RegNetX002, and InceptionResNetV2 on our updated VegNet dataset is available at: https://www.kaggle.com/datasets/enalis/tomatoes-dataset. The results show that the MobieNet model has high accuracy but less reliability than EfficientNetV2S and Xception. However, MobileNetV2’s accuracy is the highest when considering the ratio match rate. The research results contribute to the construction of intelligent agricultural support systems (using automatic fruit-picking robots, removing poor-quality fruits,…) from the results of the Explainable AI model to be able to use the optimal deep learning model in processing.
ER  - 

TY  - CONF
TI  - Evaluating the Impact of Explainable AI on User Trust in Financial Decision-Support Systems
T2  - 2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)
SP  - 1
EP  - 6
AU  - R. Mandava
AU  - S. S. Vellela
AU  - S. Gorintla
AU  - L. Dalavai
AU  - N. Malathi
AU  - K. Haritha
PY  - 2025
KW  - Ethics
KW  - Explainable AI
KW  - Scalability
KW  - Decision making
KW  - Closed box
KW  - Security
KW  - Risk management
KW  - Standards
KW  - Robots
KW  - Testing
KW  - Explainable AI
KW  - Financial Decision-Support Systems
KW  - User Trust
KW  - Model Interpretability
KW  - Transparency
KW  - Human-Centered AI
DO  - 10.1109/ICCRTEE64519.2025.11052919
JO  - 2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)
Y1  - 28-30 May 2025
AB  - Explainable Artificial Intelligence (XAI) when applied to financial decision-support systems (FDSS) creates transparent environments which help users improve their trust and develop better decision outcomes. The research examines the influence of XAI on finance user trust by evaluating its model interpretability alongside transparency and ethical compliance. The paper explains how three XAI mechanisms such as SHAP values, LIME, and counterfactual explanations help improve user confidence and interaction. Users demonstrate more satisfaction with trust in FDSS systems integrated with XAI compared to black-box AI systems and these systems produce improved financial decisions. Human-Oriented XAI design serves as a key financial success factor because it guarantees the achievement of ethical behavior and operational success as well as enhanced adoption rates.
ER  - 

TY  - CONF
TI  - Enhancing Results with ANFIS and XAI for Soil Contamination Monitoring and Public Health Risk Assessment
T2  - 2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE)
SP  - 1120
EP  - 1123
AU  - Y. Trofimov
AU  - A. Averkin
PY  - 2024
KW  - Accuracy
KW  - Uncertainty
KW  - Explainable AI
KW  - Urban areas
KW  - Data models
KW  - Mathematical models
KW  - Soil pollution
KW  - Risk management
KW  - Public healthcare
KW  - Synthetic data
KW  - neuro-fuzzy system
KW  - ANFIS
KW  - XAI
KW  - interpretability
KW  - environmental monitoring
KW  - fuzzy logic
KW  - health risks
DO  - 10.1109/PIERE62470.2024.10805065
JO  - 2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE)
Y1  - 15-17 Nov. 2024
AB  - The article presents a study on the application of Explainable Artificial Intelligence (XAI) principles and the neuro-fuzzy system ANFIS to improve the accuracy and interpretability of environmental soil pollution monitoring in the city of Dubna. The work explores the mathematical aspects of integrating fuzzy logic to model uncertainties and nonlinear dependencies in geoecological data, enabling transparent decision-making processes. As part of the research, a synthetic dataset was generated to enhance spatial modeling precision and address gaps in the existing data. This synthetic data, combined with the XAI framework, facilitated improved interpolation, leading to a more accurate and comprehensive understanding of pollutant distribution across different urban zones. The use of ANFIS allowed for the development of interpretable models that account for the spatial variability of pollutants and provide precise assessments of public health risks. The implementation of XAI further enhanced the transparency of inference processes and reduced uncertainty in predicting environmental risks, which is crucial for developing robust and effective environmental management strategies.
ER  - 

TY  - CONF
TI  - Preventing Health Records Risk Analysis with Explainable AI
T2  - 2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)
SP  - 308
EP  - 312
AU  - V. Pandimurugan
AU  - B. Balakiruthiga
AU  - J. Umamageswaran
AU  - S. A. Angayarkanni
AU  - V. Rajaram
AU  - A. Chinnasamy
PY  - 2024
KW  - Industries
KW  - Privacy
KW  - Ethics
KW  - Explainable AI
KW  - Standards organizations
KW  - Medical services
KW  - Organizations
KW  - Security
KW  - Risk management
KW  - Electronic medical records
KW  - Electronic Health Records
KW  - Security And Privacy
KW  - Explainable AI
KW  - Sensitivity
DO  - 10.1109/ICSSAS64001.2024.10760785
JO  - 2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)
Y1  - 23-25 Oct. 2024
AB  - The security of electronic health records plays a crucial concern in the healthcare industry due to the sensitive nature of the information and its ethical aspects. Security analysis of health records involves evaluating, managing, and preserving the integrity, confidentiality, and availability of sensitive healthcare information. Explainable AI plays a significant part in the security analysis of health records by offering transparency, interpretability, and accountability in the decision-making process, thereby enhancing trust, compliance, and proactive risk management in handling sensitive healthcare information. The proper implementing of explainable AI in practice in healthcare organizations can significantly reduce the risks associated with handling sensitivee health records and ensure patient data remains secure and confidential.
ER  - 

TY  - CONF
TI  - Explainable AI for Software Engineering
T2  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
SP  - 1
EP  - 2
AU  - C. K. Tantithamthavorn
AU  - J. Jiarpakdee
PY  - 2021
KW  - Productivity
KW  - Decision making
KW  - Tutorials
KW  - Software quality
KW  - Learning (artificial intelligence)
KW  - Predictive models
KW  - Software systems
KW  - Explainable AI
KW  - Software Engineering
DO  - 10.1109/ASE51524.2021.9678580
JO  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
IS  - 
SN  - 2643-1572
VO  - 
VL  - 
JA  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
Y1  - 15-19 Nov. 2021
AB  - The success of software engineering projects largely depends on complex decision-making. For example, which tasks should a developer do first, who should perform this task, is the software of high quality, is a software system reliable and resilient enough to deploy, etc. However, erroneous decision-making for these complex questions is costly in terms of money and reputation. Thus, Artificial Intelligence/Machine Learning (AI/ML) techniques have been widely used in software engineering for developing software analytics tools and techniques to improve decision-making, developer productivity, and software quality. However, the predictions of such AI/ML models for software engineering are still not practical (i.e., coarse-grained), not explainable, and not actionable. These concerns often hinder the adoption of AI/ML models in software engineering practices. In addition, many recent studies still focus on improving the accuracy, while a few of them focus on improving explainability. Are we moving in the right direction? How can we better improve the SE community (both research and education)?In this tutorial, we first provide a concise yet essential introduction to the most important aspects of Explainable AI and a hands-on tutorial of Explainable AI tools and techniques. Then, we introduce the fundamental knowledge of defect prediction (an example application of AI for Software Engineering). Finally, we demonstrate three successful case studies on how Explainable AI techniques can be used to address the aforementioned challenges by making the predictions of software defect prediction models more practical, explainable, and actionable. The materials are available at https://xai4se.github.io.
ER  - 

TY  - CHAP
TI  - List of Contributors
T2  - The AI Book: The Artificial Intelligence Handbook for Investors, Entrepreneurs and FinTech Visionaries
SP  - 272
EP  - 282
AU  - Susanne Chishti
PY  - 2020
DO  - 10.1002/9781119551966.contrib
PB  - Wiley
SN  - 9781119551867
UR  - http://ieeexplore.ieee.org/document/10953934
AB  - 
ER  - 

TY  - JOUR
TI  - Explainable Artificial Intelligence for Machine Learning-Based Photogrammetric Point Cloud Classification
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 5834
EP  - 5846
AU  - M. E. Atik
AU  - Z. Duran
AU  - D. Z. Seker
PY  - 2024
KW  - Point cloud compression
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Deep learning
KW  - Buildings
KW  - Explainable AI
KW  - Machine learning
KW  - Photogrammetry
KW  - Classification
KW  - explainable artificial intelligence (XAI)
KW  - feature selection
KW  - machine learning
KW  - photogrammetry
KW  - point cloud
DO  - 10.1109/JSTARS.2024.3370159
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 17
VL  - 17
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 2024
AB  - Point clouds are one of the most widely used data sources for spatial modeling. Artificial intelligence approaches have become an important tool for understanding and extracting semantic information of point clouds. In particular, the explainability of machine learning approaches for 3-D data has not been sufficiently investigated. Moreover, existing studies are generally limited to object classification issues. This is a pioneer study that addresses the classification of photogrammetric point clouds in terms of explainable artificial intelligence. In this study, the explainability of black-box machine learning models in the context of the classification of photogrammetric point clouds was investigated. Each point in the point cloud is defined using geometric and spectral features. In addition, the effect of selecting the most important of these features on the classification performance of ML models such as Random Forest, XGBoost, and LightGBM was examined. The explainability of ML models was analyzed with Shapley additive explanation (SHAP), an explainable artificial intelligence approach. SHAP analysis was compared with filter-based information gain (IG) and ReliefF methods for feature selection. Using the features selected with SHAP analysis, overall accuracy (OA) of 85.50% in the Ankeny dataset, 91.70% in the Building dataset, and 83.28% in the Cadastre dataset was achieved with LightGBM. The evaluation with XGBoost shows an OA of 85.22% for Ankeny, 91.21% for Building, and 82.47% for Cadastre. The evaluation with RF shows an OA of 83.70% for Ankeny, 89.08% for Building, and 79.36% for Cadastre.
ER  - 

TY  - CONF
TI  - Interpretable Deep Learning for Alzheimer’s Disease Through Genetic Data and Explainable Artificial Intelligence
T2  - 2024 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)
SP  - 50
EP  - 59
AU  - R. Alzoubi
AU  - A. Turky
AU  - A. Hussain
AU  - S. Foufou
PY  - 2024
KW  - Deep learning
KW  - Neuroimaging
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Transfer learning
KW  - Predictive models
KW  - Data models
KW  - Convolutional neural networks
KW  - Alzheimer's disease
DO  - 10.1109/BDCAT63179.2024.00019
JO  - 2024 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)
Y1  - 16-19 Dec. 2024
AB  - Alzheimer’s disease (AD) is a progressive neurodegenerative disorder causing cognitive decline and memory loss. With its significant impact on individuals’ lives, AD is the most prevalent form of dementia, contributing to 60-80% of all dementia cases. At the same time, symptoms may not surface until years later, making early detection vital for effective intervention. Thus, this work presents an approach to early AD detection by integrating Genome-Wide Association Studies (GWAS) with deep learning models and Explainable Artificial Intelligence (XAI). First, different classical machine learning models are developed for AD, and a Convolutional Neural Network (CNN) model is trained using the AD GWAS dataset obtained from the AD neuroimaging initiative. We then employ transfer learning to train our CNN model as a base model over the ADNI dataset. In addition, XAI methods are used to interpret the transfer learning model decision. Acknowledging the well-known limitation that classical machine learning is not inherently a generic model. The results from this study will help determine the most critical genetic markers associated with AD and provide transparency in understanding the deep learning model decisions.
ER  - 

TY  - CONF
TI  - Human-Centric Proactive Quality Control in Industry5.0: The Critical Role of Explainable AI
T2  - 2024 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)
SP  - 1
EP  - 7
AU  - P. Catti
AU  - E. Bakopoulos
AU  - A. Stipankov
AU  - N. Cardona
AU  - N. Nikolakis
AU  - K. Alexopoulos
PY  - 2024
KW  - Technological innovation
KW  - Manufacturing processes
KW  - Explainable AI
KW  - Collaboration
KW  - Process control
KW  - Quality control
KW  - Real-time systems
KW  - Manufacturing
KW  - Personnel
KW  - Predictive analytics
KW  - Explainable AI
KW  - Industry 5.0
KW  - Proactive Quality Control
KW  - Manufacturing
DO  - 10.1109/ICE/ITMC61926.2024.10794347
JO  - 2024 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)
IS  - 
SN  - 2693-8855
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)
Y1  - 24-28 June 2024
AB  - The integration of human knowledge and experience with artificial intelligence, especially in the context of Industry5.0, holds the promise of advanced capabilities for manufacturing that may facilitate reduced waste and increased efficiency. However, there is a gap between the two. This work discusses the critical role of Explainable AI (XAI) within this paradigm, fostering a collaborative environment where human operators can leverage AI-driven insights. A framework for data-driven proactive quality control is coupled with XAI and human-centric approaches to enable a path towards zero-defect manufacturing processes, improved operational efficiency, and enhanced workforce empowerment. Furthermore, practical implications, the impact of XAI and recommendations for upskilling and reskilling the manufacturing personnel are discussed with a focus on small and medium-sized enterprises.
ER  - 

TY  - CONF
TI  - An Explainable AI Tool for Operational Risks Evaluation of AI Systems for SMEs
T2  - 2023 15th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)
SP  - 69
EP  - 74
AU  - T. A. Han
AU  - D. Pandit
AU  - S. Joneidy
AU  - M. M. Hasan
AU  - J. Hossain
AU  - M. Hoque Tania
AU  - M. A. Hossain
AU  - N. Nourmohammadi
PY  - 2023
KW  - Surveys
KW  - Ethics
KW  - Law
KW  - Software
KW  - Robustness
KW  - Safety
KW  - Stakeholders
KW  - Machine learning
KW  - Ethical Considerations
KW  - Risk Management
KW  - Legal Compliance
KW  - Artificial intelligence
DO  - 10.1109/SKIMA59232.2023.10387301
JO  - 2023 15th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)
IS  - 
SN  - 2573-3214
VO  - 
VL  - 
JA  - 2023 15th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)
Y1  - 8-10 Dec. 2023
AB  - With the surge in artificial intelligence (AI) adoption by Small and Medium-sized Enterprises (SMEs), ensuring their safety, fairness, and operational assurance has become paramount. Since many SMEs operate with limited resources, they face unique challenges in ethically and securely deploying AI systems. This research delves into the core principles of AI governance, risk management, and testing, specifically tailored for SMEs, emphasising making these concepts accessible and understandable. Through collaborative efforts, including interactive workshops, meetings and surveys with twenty SME participants, we identified vital AI application areas and challenges and conceptualised an evaluation tool leveraging explainable AI. This tool assesses AI-driven systems' robustness, potential biases, and other software and hardware vulnerabilities It also addresses ethical considerations and legal compliance, emphasising establishing trust and accountability with stakeholders as a foundation for successful AI integration. In conclusion, the paper presents a pilot study that conducts a risk analysis of prevalent AI applications, specifically AI-driven language models, for SMEs. This study illustrates how the proposed evaluation tool will integrate risk levels across different application domains.
ER  - 

TY  - CONF
TI  - Thinking Responsibly About Responsible AI in Risk Management: The Darkside of AI in RM
T2  - 2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS)
SP  - 1
EP  - 5
AU  - A. B. M. Metwally
AU  - S. A. M. Ali
AU  - A. T. I. Mohamed
PY  - 2024
KW  - Uncertainty
KW  - Data integration
KW  - Regulation
KW  - Risk management
KW  - Stakeholders
KW  - Artificial intelligence
KW  - Sustainable development
KW  - Artificial intelligence
KW  - machine learning
KW  - risk management
KW  - AI Darkside
KW  - responsible AI
KW  - AI based risk management
DO  - 10.1109/ICETSIS61505.2024.10459684
JO  - 2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS)
Y1  - 28-29 Jan. 2024
AB  - Artificial Intelligence (AI) holds great potential for enhancing Risk Management (RM) through automated data integration and analysis. While the positive impact of AI in RM is acknowledged, concerns are rising about unintended consequences. This study explores factors like opacity, technology and security risks, revealing potential operational inefficiencies and inaccurate risk assessments. Through archival research and stakeholder interviews, including chief risk officers and credit managers, findings highlight the risks stemming from the absence of AI regulations, operational opacity, and information overload. These risks encompass cybersecurity threats, data manipulation uncertainties, monitoring challenges, and biases in algorithms. The study emphasizes the need for a responsible AI framework to address these emerging risks and enhance the effectiveness of RM processes. By advocating for such a framework, the authors provide practical insights for risk managers and identify avenues for future research in this evolving field.
ER  - 

TY  - CONF
TI  - Flood Risk Assessment and Interpretation Using Explainable AI
T2  - 2024 International Conference on IoT, Communication and Automation Technology (ICICAT)
SP  - 41
EP  - 45
AU  - S. Sharma
AU  - V. Tomar
AU  - S. Arora
AU  - N. Gupta
PY  - 2024
KW  - Ethics
KW  - Additives
KW  - Automation
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Cause effect analysis
KW  - Floods
KW  - Risk management
KW  - Random forests
KW  - Flood risk
KW  - Extra Tree Classifier
KW  - Data-driven methods
KW  - Shapely Additive explanation
KW  - Decision Making
DO  - 10.1109/ICICAT62666.2024.10923370
JO  - 2024 International Conference on IoT, Communication and Automation Technology (ICICAT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on IoT, Communication and Automation Technology (ICICAT)
Y1  - 23-24 Nov. 2024
AB  - Floods are the unforeseen situations to the society which causes high risk to the society. For proactive action, it is important to identify locations based on past results. It is necessary to take measures before the situation gets worst. For the benefit of society, a novel approach to map flood risk usinga balanced dataset and an Extra Tree Classifier. The Extra Tree Classifier performs better with the Shapely Additive explanation values. It shows improvement towards decision making processes to reduce the causality at the time of floods. The proposed method achieves the accuracy of 93.61% for the estimating flood danger effects on a particular location. In this paper quantitative study was done to handle the challenges to predict the flood danger using the artificial intelligence ethically.
ER  - 

TY  - CONF
TI  - Predicting and Mapping Flood Susceptibility: Leveraging Explainable AI and GIS Techniques
T2  - 2024 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)
SP  - 1
EP  - 4
AU  - A. Chakraborty
AU  - B. Kumar
AU  - S. Upadhyaya
PY  - 2024
KW  - Data analysis
KW  - Explainable AI
KW  - Prevention and mitigation
KW  - Geoscience and remote sensing
KW  - Predictive models
KW  - Feature extraction
KW  - Software
KW  - Floods
KW  - Random forests
KW  - Faces
KW  - Flood Susceptibility
KW  - Sequential Feature Selection
KW  - XAI
KW  - Partial Dependence Plots
KW  - Permutation Feature Importance
DO  - 10.1109/InGARSS61818.2024.10984094
JO  - 2024 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)
Y1  - 2-5 Dec. 2024
AB  - Flood susceptibility mapping is crucial for disaster management, but traditional methods struggle with precision and adaptability in the face of climate variability. This study employed a Random Forest Classifier (RFC) to enhance prediction accuracy in flood susceptibility mapping. Exploratory data analysis (EDA) was conducted to examine data characteristics and identify key flood-influencing variables. Feature engineering techniques were used to refine predictors and determine an optimal subset of features. The model exhibited high predictive power with an ROC-AUC value of 0.92. To address the challenge of communicating complex results, plots of partial dependence and permutation feature importance were generated. The findings were translated into actionable flood susceptibility maps using GIS software. This approach bridges the gap between advanced machine learning techniques and practical insights, enabling policy-makers to visualize flood-prone areas and implement targeted mitigation strategies.
ER  - 

TY  - CONF
TI  - Explainable and Interpretable Artificial Intelligence as a Service for Green Smart Cities and Communities
T2  - 2023 IEEE 15th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM)
SP  - 1
EP  - 5
AU  - K. Napisa
AU  - G. R. Mababangloob
AU  - M. Lubag
AU  - R. Concepcion II
AU  - M. M. Redillas
PY  - 2023
KW  - Smart cities
KW  - Explainable AI
KW  - Biological system modeling
KW  - Green products
KW  - Humanoid robots
KW  - Market research
KW  - Social factors
KW  - advancement
KW  - digital agriculture
KW  - green smart cities
KW  - sustainable agriculture
KW  - urban development
DO  - 10.1109/HNICEM60674.2023.10589252
JO  - 2023 IEEE 15th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM)
IS  - 
SN  - 2770-0682
VO  - 
VL  - 
JA  - 2023 IEEE 15th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM)
Y1  - 19-23 Nov. 2023
AB  - With the increasing usage and impact of artificial intelligence (AI) in society, AI could be utilized for improving society in terms of improving overall urban quality of life through smart cities. This, along with the rise of sustainable urban development, would lay the foundations for the concept of green smart cities and communities. Both concepts, when combined, would present a challenge due to various economical, technological, environmental, and social factors. This challenge is further amplified by the implementation of the various explainable artificial intelligence (XAI) and interpretable artificial intelligence (IAI) models, presenting a novel approach to green smart cities, with the benefits of an implementation greatly improving smart cities despite the challenge presented. This study would examine the rise of AI through exploring multiple existing AI models and classifying if they're XAI or IAI models as well as what makes them so. It would also discuss current applications of XAI and IAI models as well as how they integrate into sustainable urban development. The challenges and complexities of both XAI and IAI models and sustainable urban development would also be discussed and the potential challenges if both were to be implemented, taking into consideration the various sectors of a smart city. Emerging trends and technologies of both AI model types would also be explored wherein novel AI applications would be included for future insights into these applications into green smart cities.
ER  - 

TY  - CONF
TI  - An Efficient Explainable AI Method Combining CNN and SVM for Corn Leaf Disease Detection and Visualization
T2  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
SP  - 2958
EP  - 2962
AU  - H. I. Peyal
AU  - M. N. I. Mondal
AU  - S. Miraz
PY  - 2024
KW  - Support vector machines
KW  - Visualization
KW  - Plant diseases
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Transfer learning
KW  - Agriculture
KW  - Stability analysis
KW  - Convolutional neural networks
KW  - Convolutional Neural Networks
KW  - CNN
KW  - SVM Classifier
KW  - Plant Disease Diagnosis
KW  - Explainable AI (XAI) Techniques
KW  - Grad-CAM
KW  - SHAP
KW  - Lime
KW  - Lightweight
DO  - 10.1109/ICCIT64611.2024.11021795
JO  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 2474-9656
VO  - 
VL  - 
JA  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
Y1  - 20-22 Dec. 2024
AB  - Agriculture plays a crucial role in Bangladesh's economy, with corn serving as a key crop. Plant diseases pose significant threats to agricultural productivity and economic stability worldwide. Effective monitoring and prediction are essential to mitigate these risks, as the prevalence of infectious diseases severely disrupts agricultural production. This study focuses on diagnosing agricultural diseases and pests affecting corn stalks using convolutional neural networks (CNN) combined with an SVM classifier. The proposed model achieves a classification accuracy of 98.34%, comparable to transfer learning models such as VGG-16 (97.42%) and VGG-19 (97.13%), while utilizing approximately 0.95 million parameters—about 145 times and 151 times fewer than VGG-16 and VGG-19, respectively. The model exhibits outstanding performance, with accuracy, recall, and F1 scores nearing 97% and an exceptional Area Under Curve (AUC) score of 99%. Furthermore, the model's compact design requires minimal disk space (approximately 4 MB) and features a significantly reduced parameter count. To enhance interpretability, the study integrates explainable AI techniques, including Gradient Weighted Class Activation Mapping (Grad-CAM), Shapley Additive Explanations (SHAP), and Local Interpretable Model-Agnostic Explanations (LIME). These methods provide visual explanations through heatmaps, highlighting regions critical for classification, thereby facilitating a clearer understanding of the disease diagnosis process.
ER  - 

TY  - JOUR
TI  - Advancements in Deep Reinforcement Learning and Inverse Reinforcement Learning for Robotic Manipulation: Toward Trustworthy, Interpretable, and Explainable Artificial Intelligence
T2  - IEEE Access
SP  - 51840
EP  - 51858
AU  - R. Ozalp
AU  - A. Ucar
AU  - C. Guzelis
PY  - 2024
KW  - Robots
KW  - Artificial intelligence
KW  - Task analysis
KW  - Robot kinematics
KW  - Classification algorithms
KW  - Explainable AI
KW  - Deep reinforcement learning
KW  - Inverse problems
KW  - Manipulators
KW  - Trusted computing
KW  - Deep reinforcement learning
KW  - inverse reinforcement learning
KW  - robotic manipulation
KW  - artificial intelligence
KW  - trustworthy AI
KW  - interpretable AI
KW  - eXplainable AI
DO  - 10.1109/ACCESS.2024.3385426
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - This article presents a literature review of the past five years of studies using Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning (IRL) in robotic manipulation tasks. The reviewed articles are examined in various categories, including DRL and IRL for perception, assembly, manipulation with uncertain rewards, multitasking, transfer learning, multimodal, and Human-Robot Interaction (HRI). The articles are summarized in terms of the main contributions, methods, challenges, and highlights of the latest and relevant studies using DRL and IRL for robotic manipulation. Additionally, summary tables regarding the problem and solution are presented. The literature review then focuses on the concepts of trustworthy AI, interpretable AI, and explainable AI (XAI) in the context of robotic manipulation. Moreover, this review provides a resource for future research on DRL/IRL in trustworthy robotic manipulation.
ER  - 

TY  - CONF
TI  - Unveilling Soil Fertility Classification with Explainable AI
T2  - 2024 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)
SP  - 501
EP  - 507
AU  - K. Roufaida
AU  - M. Abdelhak
AU  - R. Khaled
PY  - 2024
KW  - Productivity
KW  - Technological innovation
KW  - Accuracy
KW  - Rural areas
KW  - Soil
KW  - Predictive models
KW  - Phosphorus
KW  - Nitrogen
KW  - Potassium
KW  - Random forests
KW  - Explainable AI
KW  - Smart farming
KW  - Machine learning
KW  - Soil fertility
KW  - Artificial intelligence
KW  - Agriculture 4.0
DO  - 10.1109/3ict64318.2024.10824403
JO  - 2024 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)
IS  - 
SN  - 2770-7466
VO  - 
VL  - 
JA  - 2024 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)
Y1  - 17-19 Nov. 2024
AB  - Population growth necessitates the urgent enhancement of sustainable food production systems. However, the overuse of fertilizers significantly undermines soil fertility, posing a dual threat to agricultural productivity and environmental integrity. This paper introduces an innovative machine learning (ML) methodology integrated with interpretable artificial intelligence (IAI) aimed at promoting sustainable soil management practices. We conduct a thorough investigation into interpretable ML models specifically designed for the classification of soil fertility. Our approach meticulously analyzes model outcomes while pinpointing critical features that influence predictions regarding soil fertility. The results of this method exhibit remarkable promise, achieving high accuracy in predicting soil fertility.
ER  - 

TY  - CONF
TI  - Enhancing Solar Energy Forecasting: 1D CNN With Explainable AI Techniques
T2  - 2024 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)
SP  - 1
EP  - 4
AU  - D. Jain
AU  - S. Upadhyaya
PY  - 2024
KW  - Temperature sensors
KW  - Solar irradiance
KW  - Technological innovation
KW  - Accuracy
KW  - Explainable AI
KW  - Weather forecasting
KW  - Predictive models
KW  - Transformers
KW  - Smart grids
KW  - Forecasting
KW  - Solar energy forecasting
KW  - 1D-CNN
KW  - Explainable AI
KW  - GEFS dataset
DO  - 10.1109/InGARSS61818.2024.10984181
JO  - 2024 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)
Y1  - 2-5 Dec. 2024
AB  - Accurate solar energy forecasting is essential for optimizing renewable energy systems and supporting SDG 7: Affordable and Clean Energy and SDG 13: Climate Action. This paper presents a 1D CNN framework for day-ahead solar irradiance prediction, using data from the Global Ensemble Forecast System and the “KENT” Mesonet station, as part of the American Meteorological Society’s Solar Energy Prediction Contest. Enhanced with residual inception squeeze-and-excitation blocks and attention mechanisms, our model improves forecast accuracy, reducing mean absolute error (MAE) by 6.56% over the contest’s best model. Explainable AI methods, including Integrated Gradients and Permutation Feature Importance, identify key influences such as downward shortwave radiation and surface temperature. Future work will explore transformer models for smart grid applications, further supporting SDG 9: Industry, Innovation, and Infrastructure.
ER  - 

TY  - JOUR
TI  - Beyond the Black Box: A Systematic Review of Explainable AI for Transparent and Trustworthy Water Quality Monitoring
T2  - IEEE Sensors Reviews
SP  - 1
EP  - 36
AU  - I. A. Aderemi
AU  - T. O. Kehinde
AU  - D. O. Ugochukwu
AU  - K. H. Ahmad
AU  - K. Y. Adjei
AU  - C. E. Chijioke
PY  - 2025
KW  - Water quality
KW  - Monitoring
KW  - Explainable AI
KW  - Data models
KW  - Water pollution
KW  - Stakeholders
KW  - Real-time systems
KW  - Biological system modeling
KW  - Sensors
KW  - Sensor phenomena and characterization
KW  - Environmental Decision Support
KW  - Explainable Artificial Intelligence (XAI)
KW  - Machine Learning
KW  - Model Interpretability
KW  - Smart Water Systems
KW  - Water Quality Monitoring
DO  - 10.1109/SR.2025.3595500
JO  - IEEE Sensors Reviews
IS  - 
SN  - 2995-7478
VO  - 
VL  - 
JA  - IEEE Sensors Reviews
Y1  - 
AB  - Water quality monitoring is essential for protecting public health, sustaining ecosystems, and achieving Sustainable Development Goal 6 (Clean Water and Sanitation). While recent advances in Artificial Intelligence (AI), particularly Machine Learning (ML), have improved the accuracy and responsiveness of water quality assessment, the opaque “black-box” nature of many AI models limits transparency, stakeholder trust, and regulatory compliance. Explainable AI (XAI) offers a viable solution by enabling human-understandable insights into model behaviour. This paper presents a PRISMA-guided systematic review of 60 peer-reviewed articles (2011–2025), sourced from Scopus, to evaluate the evolution, application, and effectiveness of XAI in water quality monitoring. Key techniques such as SHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic Explanations (LIME), and counterfactual reasoning have been applied across Random Forest, XGBoost, and LSTM models. Results indicate a surge in XAI adoption post-2022, with dominant use cases in groundwater prediction, surface water quality forecasting, and real-time monitoring in IoT-enabled smart cities. While SHAP remains the most widely used method, multimodal and hybrid frameworks are emerging to address challenges such as data heterogeneity and model complexity. The review identifies persistent barriers including computational scalability, lack of standardized evaluation metrics, and limited deployment in low-resource settings. It proposes future research directions for integrating XAI with digital twins, causal inference, and edge computing to achieve robust, transparent, and equitable water management systems.
ER  - 

TY  - JOUR
TI  - Neuro-Symbolic Explainable Artificial Intelligence Twin for Zero-Touch IoE in Wireless Network
T2  - IEEE Internet of Things Journal
SP  - 22451
EP  - 22468
AU  - M. S. Munir
AU  - K. T. Kim
AU  - A. Adhikary
AU  - W. Saad
AU  - S. Shetty
AU  - S. -B. Park
AU  - C. S. Hong
PY  - 2023
KW  - Artificial intelligence
KW  - Cognition
KW  - Measurement
KW  - Wireless networks
KW  - Uplink
KW  - Internet of Things
KW  - Bayes methods
KW  - Declarative semantics
KW  - explainable artificial intelligence (XAI)
KW  - Internet of Everything (IoE)
KW  - neuro-symbolic XAI
KW  - trustworthy artificial intelligence (AI)
KW  - zero-touch network and service management (ZSM)
DO  - 10.1109/JIOT.2023.3303713
JO  - IEEE Internet of Things Journal
IS  - 24
SN  - 2327-4662
VO  - 10
VL  - 10
JA  - IEEE Internet of Things Journal
Y1  - 15 Dec.15, 2023
AB  - Explainable artificial intelligence (XAI) twin systems will be a fundamental enabler of zero-touch network and service management (ZSM) for sixth-generation (6G) wireless networks. Thus, a reliable XAI twin system becomes essential to discretizing the physical behavior of the Internet of Everything (IoE) and identifying the reasons behind that behavior for enabling ZSM. To address the challenges of extensible, modular, and stateless management functions in ZSM, a novel neuro-symbolic XAI twin framework is proposed that to enable trustworthy ZSM for a wireless IoE. The proposed neuro-symbolic XAI twin framework consists of two learning systems: 1) implicit learner that acts as an unconscious learner in physical space and 2) explicit leaner that can exploit symbolic reasoning based on implicit learner decisions and prior evidence. The physical space of the XAI twin executes a neural-network-driven multivariate regression to capture the time-dependent wireless IoE environment while determining unconscious decisions of IoE service aggregation, such as uplink, downlink, and service provisioning. Subsequently, the virtual space of the XAI twin constructs a directed acyclic graph (DAG)-based Bayesian network that can infer a symbolic reasoning score over unconscious decisions through a first-order probabilistic language model. Furthermore, a Bayesian multiarm bandit-based learning problem is proposed for reducing the gap between the expected explained score and the current obtained score of the proposed neuro-symbolic XAI twin. Experimental results show that the proposed neuro-symbolic XAI twin can achieve around 96.26% accuracy while guaranteeing from 18% to 44% more trust score in terms of reasoning and closed-loop automation.
ER  - 

TY  - CONF
TI  - Enhancing Software Defect Prediction through Explainable AI: Integrating SHAP and LIME in a Voting Classifier Framework
T2  - 2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP)
SP  - 1
EP  - 7
AU  - B. Asal
AU  - M. Ö. Demir
PY  - 2024
KW  - Explainable AI
KW  - Decision making
KW  - Estimation
KW  - Predictive models
KW  - Data models
KW  - Software reliability
KW  - Stakeholders
KW  - Artificial intelligence
KW  - Software engineering
KW  - Software development management
KW  - Explainable Artificial Intelligence (XAI)
KW  - Software Engineering
KW  - Software Defect Prediction
KW  - SHAP
KW  - LIME
KW  - Voting Classifier
KW  - Transparency in AI
DO  - 10.1109/IDAP64064.2024.10710700
JO  - 2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP)
Y1  - 21-22 Sept. 2024
AB  - Explainable Artificial Intelligence (XAI) has become increasingly vital in the field of artificial intelligence, as it addresses the critical need for transparency and interpretability in AI models. As AI systems are increasingly deployed in high-stakes environments, understanding the decision-making process of these models is essential for building trust and ensuring responsible AI usage. XAI provides the methods to uncover the underlying mechanisms of AI models, making them more accessible and understandable to users and stakeholders. In the domain of software engineering, AI has emerged as a powerful tool for automating various tasks, including software defect prediction and cost estimation. However, the opaque nature of traditional AI models has raised concerns about their reliability and the ability to validate their outputs. This study focuses on enhancing the transparency of AI applications in software engineering by integrating XAI techniques. We employ a voting classifier trained on the KC2 dataset, coupled with SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) for local and global explainability. Our research demonstrates how the application of SHAP and LIME can provide clear and interpretable insights into the factors driving the predictions of the voting classifier. By making the model’s decision-making process more transparent, we enable developers and stakeholders to better understand and trust the AI-driven predictions. This study not only advances the field of software defect prediction but also contributes to the broader adoption of XAI in software engineering, highlighting its importance in creating more reliable, understandable, and trustworthy AI systems.
ER  - 

TY  - JOUR
TI  - Explainable Artificial Intelligence by Genetic Programming: A Survey
T2  - IEEE Transactions on Evolutionary Computation
SP  - 621
EP  - 641
AU  - Y. Mei
AU  - Q. Chen
AU  - A. Lensen
AU  - B. Xue
AU  - M. Zhang
PY  - 2023
KW  - Machine learning
KW  - Genetic programming
KW  - Task analysis
KW  - Predictive models
KW  - Adaptation models
KW  - Training
KW  - Measurement
KW  - Explainable artificial intelligence (XAI)
KW  - Genetic programming (GP)
DO  - 10.1109/TEVC.2022.3225509
JO  - IEEE Transactions on Evolutionary Computation
IS  - 3
SN  - 1941-0026
VO  - 27
VL  - 27
JA  - IEEE Transactions on Evolutionary Computation
Y1  - June 2023
AB  - Explainable artificial intelligence (XAI) has received great interest in the recent decade, due to its importance in critical application domains, such as self-driving cars, law, and healthcare. Genetic programming (GP) is a powerful evolutionary algorithm for machine learning. Compared with other standard machine learning models such as neural networks, the models evolved by GP tend to be more interpretable due to their model structure with symbolic components. However, interpretability has not been explicitly considered in GP until recently, following the surge in the popularity of XAI. This article provides a comprehensive review of the studies on GP that can potentially improve the model interpretability, both explicitly and implicitly, as a byproduct. We group the existing studies related to explainable artificial intelligence by GP into two categories. The first category considers the intrinsic interpretability, aiming to directly evolve more interpretable (and effective) models by GP. The second category focuses on post-hoc interpretability, which uses GP to explain other black-box machine learning models, or explain the models evolved by GP by simpler models such as linear models. This comprehensive survey demonstrates the strong potential of GP for improving the interpretability of machine learning models and balancing the complex tradeoff between model accuracy and interpretability.
ER  - 

TY  - CONF
TI  - A Strategic Innovations for Stock Market Optimization by Using Data Science & Explainable AI in High Frequency Trading
T2  - 2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)
SP  - 1510
EP  - 1515
AU  - P. Devi
AU  - S. S. Desai
AU  - R. Kumar
AU  - M. S. Naruka
AU  - V. Tripathi
PY  - 2024
KW  - Technological innovation
KW  - Explainable AI
KW  - Shape
KW  - Decision making
KW  - Data science
KW  - Regulation
KW  - High frequency
KW  - Stock markets
KW  - Optimization
KW  - Streams
KW  - High frequency trading (HFT)
KW  - Explainable artificial Intelligence (XAI)
KW  - Data Science
KW  - Stock Market
KW  - High frequency trading
DO  - 10.1109/ICICNIS64247.2024.10823313
JO  - 2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)
Y1  - 17-18 Dec. 2024
AB  - In the era of technology, financial markets especially stock market has been completely transformed through the integration of data science and explainable artificial intelligence. This paper presents a comprehensive analysis of evolution in the high frequency trading with the support of XAI and data science which made it possible to do financial transactions in extremely high speeds, often within milliseconds. But with the execution of these techniques HFT raises few concerns regarding transparency, interpretability and complexity. This study explores how data science shapes the strategies of HFT such as market making, arbitrage strategies: simple arbitrage, triangle arbitrage, statistical arbitrage etc. This study also explores how XAI and data science techniques such as machine learning, big data analytics, momentum trading and quantum computing are being employed in optimizing trading strategies to improve the decision making and liquidity in stock market. It includes regulations of high frequency trading at global level and challenge faced by financial sector in adopting these technologies. This study investigates the qualitative Scopus indexed journal papers and IEEE explores paper for the referencing the data to find out the authenticated literature review. This study will help out to market participants such as traders, investors, policymakers who aims to navigate the future of finance in India.
ER  - 

TY  - CONF
TI  - Integrated Explainable AI for Financial Risk Management: A Systematic Approach
T2  - 2025 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)
SP  - 1
EP  - 6
AU  - P. Murthy
AU  - S. Gaur
AU  - T. Jolly
AU  - S. R
AU  - G. Sharma
AU  - R. Rathore
PY  - 2025
KW  - Technological innovation
KW  - Systematics
KW  - Explainable AI
KW  - Decision making
KW  - Alarm systems
KW  - Predictive models
KW  - Risk management
KW  - Standards
KW  - Random forests
KW  - Synthetic data
KW  - Ai-Driven Early Warning System
KW  - Explainable Ai
KW  - Financial Risk Management
KW  - Shap
KW  - Lime
KW  - Transparency In Ai Models
DO  - 10.1109/IATMSI64286.2025.10984539
JO  - 2025 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)
IS  - 
SN  - 
VO  - 3
VL  - 3
JA  - 2025 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)
Y1  - 6-8 March 2025
AB  - This research introduces an improved AI-Driven Early Warning System for financial institutions to overcome some of the transparency challenges found with conventional AI models. Traditional black-box" systems, in particular, those that employ complex neural networks, provide limited or no ability for users to understand their decision-making processes. This section raises a number of concerns, especially within the financial sector, where regulatory frameworks require decision-making mechanisms to be transparent and interpretable. To address this challenge, XAI techniques such as SHAP and LIME are integrated with the EWS. These will explain the grounds for the choices made by the AI in order for the predictions to be legally tenable, improve decision-making, and facilitate more transparency with the system. The proposed framework was then applied to real-world financial data and demonstrated prominent improvements in accurately detecting early signals of financial risks. It provided precise insights into the factors driving these risks while meeting regulatory standards.
ER  - 

TY  - CONF
TI  - Poster Abstract: Xpi: Real-Time Progressive Inference Serving with Explainable AI in Edge-Cloud Systems
T2  - 2024 23rd ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)
SP  - 273
EP  - 274
AU  - C. Lin
AU  - Z. Chen
AU  - J. Liu
PY  - 2024
KW  - Deep learning
KW  - Accuracy
KW  - Explainable AI
KW  - Information processing
KW  - Real-time systems
KW  - edge computing
KW  - progressive inference
KW  - explainable AI
KW  - reinforcement learning
DO  - 10.1109/IPSN61024.2024.00037
JO  - 2024 23rd ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 23rd ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)
Y1  - 13-16 May 2024
AB  - The constrained computing and memory resources at the edge pose challenges for satisfying different service-level objectives (SLOs) of deep learning inference requests. In this paper, we propose a novel edge-cloud progressive inference framework Xpi, which integrates explainable AI technique to facilitate early-exit, and learning-based online execution control to satisfy different SLOs and optimize edge resource overheads. We implement Xpi on an edge-cloud platform, and conduct partial experiments on two datasets. Xpi outperforms several advanced edge-cloud progressive inference frameworks in terms of accuracy and deadline satisfaction rate.
ER  - 

TY  - CONF
TI  - Utilizing Explainable AI in Financial Risk Assessment: Enhancing User Empowerment through Interpretable Credit Scoring Models
T2  - 2025 Systems and Information Engineering Design Symposium (SIEDS)
SP  - 444
EP  - 449
AU  - H. Gonaygunta
AU  - M. H. Maturi
AU  - A. R. Yadulla
AU  - R. K. Ravindran
AU  - E. D. L. Cruz
AU  - G. S. Nadella
AU  - K. Meduri
PY  - 2025
KW  - Logistic regression
KW  - Adaptation models
KW  - Explainable AI
KW  - Standardization
KW  - Predictive models
KW  - Risk management
KW  - Decision trees
KW  - Random forests
KW  - Principal component analysis
KW  - Aquaculture
KW  - Credit risk assessment
KW  - Deep learning
KW  - Explainable artificial intelligence (XAI)
KW  - Financial risk modeling
KW  - Machine learning
KW  - and Shapley Additive explanations (SHAP)
DO  - 10.1109/SIEDS65500.2025.11021190
JO  - 2025 Systems and Information Engineering Design Symposium (SIEDS)
IS  - 
SN  - 2994-3531
VO  - 
VL  - 
JA  - 2025 Systems and Information Engineering Design Symposium (SIEDS)
Y1  - 2-2 May 2025
AB  - This research presents a framework that integrates machine learning methods with Explainable Artificial Intelligence (XAI) to enhance predictive accuracy and interpretability in risk assessment. Models ranging from Logistic Regression and Decision Trees to ensemble methods like Random Forest, CatBoost, and XGBoost are developed to address credit risk challenges and enhance decision-making transparency. Although the approach is adaptable for financial risk modeling, the framework is tested using the abalone dataset from the UCI Machine Learning Repository, which includes physical measurements and categorical attributes for performance evaluation. Rigorous preprocessing with feature engineering and standardization ensures data integrity. Model performance, measured by Mean Squared Error (MSE), shows that Random Forest and CatBoost yield superior results. Additionally, Principal Component Analysis (PCA) and feature importance analysis improve interpretability by revealing key predictive factors. Overall, the results underscore the viability of integrating XAI with machine learning for risk assessment.
ER  - 

TY  - CONF
TI  - Explainable AI for Predictive Analytics on Employee Promotion
T2  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
SP  - 1
EP  - 7
AU  - A. Bhattacharya
AU  - P. Choudhary
AU  - S. Mukhopadhyay
AU  - B. Misra
AU  - S. Chakraborty
AU  - N. Dey
PY  - 2023
KW  - Training
KW  - Productivity
KW  - Measurement
KW  - Training data
KW  - Reinforcement learning
KW  - Predictive models
KW  - Prediction algorithms
KW  - XGBoost
KW  - Employee Promotion
KW  - SMOTE
KW  - Explainable AI
KW  - LIME
KW  - SHAP
DO  - 10.1109/ICACTA58201.2023.10393141
JO  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
Y1  - 6-7 Oct. 2023
AB  - The success of a company depends upon the strategy, technology, finance, and more importantly, the performance and competence of its employees. To efficiently manage the manpower of an organization, the founders, CEO, and managers need to determine which employees must be promoted. Promotion boosts employee morale, improves the resource allocation of the organization, and helps identify talent. In this project, we have explored various machine learning algorithms to predict the promotion of an employee based on multiple employee-related factors such as education, training, rating and length of service. The analysis focuses on creating a predictive model after thorough feature analysis and preprocessing of the training data. We have trained multiple classification models such as logistic regression, decision tree, random forest, and XGBoost. Evaluation metrics, namely, precision, recall, and Fl-score, were used to evaluate the performance of each model, and the XGBoost classifier outperformed the other algorithms. In addition, we have implemented explainable artificial intelligence methods such as local interpretable model agnostic explanations. Additive explanations to derive the reason behind the predictions made by artificial intelligence. The current work managed to predict with an accuracy of 95% and detect the features that have an impact on employee promotion.
ER  - 

TY  - CONF
TI  - Computer Information Management and Network Construction based on Artificial Intelligence
T2  - 2024 Second International Conference on Data Science and Information System (ICDSIS)
SP  - 1
EP  - 4
AU  - M. Wang
AU  - S. Zhang
PY  - 2024
KW  - Radio frequency
KW  - Numerical analysis
KW  - Soft sensors
KW  - Training data
KW  - Mathematical models
KW  - Data models
KW  - Information management
KW  - artificial neural network
KW  - convolutional neural network
KW  - deep convolutional neural network
KW  - explainable artificial intelligence and information management
DO  - 10.1109/ICDSIS61070.2024.10594032
JO  - 2024 Second International Conference on Data Science and Information System (ICDSIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Second International Conference on Data Science and Information System (ICDSIS)
Y1  - 17-18 May 2024
AB  - Artificial Intelligence development of the construction industry is its digitalization, which requires the transformation of processes and models based on the use of digital platforms and twins. However, complex dynamic environments with diverse data sources, inconsistency, or incompleteness undermine the reliability and usefulness of the explainable output. The proposed Random Forest (RF) and Artificial Neural Network (ANN) methodologies for effectively combining RF with ANN to capitalize on the strengths of both models in handling diverse data sources and capturing complex relationships. A mechanism is implemented to collect data from identified sources and integrate the process to scale and handle increasing data volumes and processing loads. Structural Equation Modelling (SEM) account for measurement error in variables, making it more robust to measurement inaccuracies compared to simpler regression techniques. RF-ANN handle noisy or incomplete data and generalize patterns from training data to make predictions. Although proposed RF-ANN technique exhibits high performance compared to existing methods such as Convolutional Neural Network (CNN), Explainable Artificial Intelligence (XAI) and Deep CNN (DCNN), the outcomes include a Nash-Sutcliffe efficiency (NSE) of 0.12, Root Mean Square Error (RMSE) of 0.58, Accuracy of 0.9 and F1 value of 0.95 respectively.
ER  - 

TY  - JOUR
TI  - Explainable Data-Driven Digital Twins for Predicting Battery States in Electric Vehicles
T2  - IEEE Access
SP  - 83480
EP  - 83501
AU  - J. Nkechinyere Njoku
AU  - C. Ifeanyi Nwakanma
AU  - D. -S. Kim
PY  - 2024
KW  - Batteries
KW  - Estimation
KW  - Artificial intelligence
KW  - Predictive models
KW  - Digital twins
KW  - Electric vehicles
KW  - Long short term memory
KW  - Battery management systems
KW  - Explainable AI
KW  - Machine learning
KW  - Battery management systems
KW  - digital twins
KW  - artificial intelligence
KW  - XAI
KW  - explainable artificial intelligence
KW  - machine learning
DO  - 10.1109/ACCESS.2024.3413075
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Advancements in battery management systems (BMS) involve using digital twins to optimize battery performance in electric vehicles. The state of charge and health estimations are essential for battery efficiency and longevity. Digital twins allow for precise predictions of the state of charge and state of health by simulating battery behavior under different conditions. Using artificial intelligence (AI) in digital twins improves predictive capabilities, as demonstrated through studies employing deep neural networks (DNN) and long short-term memory networks (LSTM). However, incorporating AI presents challenges due to the opaque nature of the models, necessitating the need for explainable artificial intelligence (XAI) and trustworthy digital twin models. This study pioneered XAI methods such as SHapley Additive exPlanations, Local Interpretable Model-agnostic Explanations, and linear regression-based surrogate models to explain the predictions of DNNs and LSTMs in digital twin-supported BMSs. The results reveal that the DNN and LSTM digital twin models are more reliable for state-of-health and state-of-charge estimation due to higher  $R^{2}$  scores, lower mean residuals, and better XAI results.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence (XAI) on Hoax Detection Using Decision Tree C4.5 Method for Indonesian News Platform
T2  - 2022 International Conference of Science and Information Technology in Smart Administration (ICSINTESA)
SP  - 63
EP  - 68
AU  - J. Imanuel
AU  - L. Kintanswari
AU  - Vincent
AU  - H. Lucky
AU  - A. Chowanda
PY  - 2022
KW  - Training
KW  - Measurement
KW  - Data preprocessing
KW  - Entropy
KW  - Data models
KW  - Decision trees
KW  - Artificial intelligence
KW  - hoax detection
KW  - explainable artificial intelligence
KW  - Indonesian fake news
KW  - decision tree
KW  - C4.5 algorithm
DO  - 10.1109/ICSINTESA56431.2022.10041567
JO  - 2022 International Conference of Science and Information Technology in Smart Administration (ICSINTESA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference of Science and Information Technology in Smart Administration (ICSINTESA)
Y1  - 10-12 Nov. 2022
AB  - Hoax news can be defined as false information about events that tricks the readers into believing it as a genuine information. Explainable Artificial Intelligence or XAI is a simple algorithm that easy to understand. The example of XAI is Decision Tree. The methodology of this paper is Data Gathering to collect the data, preprocessing to give label to the parameters, Attribution Selection to determine which parameter to use by calculating the entropy and information gain result, then Decision Tree Construction using training datasets as a model for hoax detection, and calculating Evaluation Metric. This paper contains a total of 200 row of data (100 hoax and 100 fact). This research shows that the highest parameters for detecting hoax news are giving restless or panic emotion, containing hatred or angers, suggestion to share, and promising reward. By using 20% of the dataset for testing, the accuracy of this model is 82%.
ER  - 

TY  - CONF
TI  - Impact of Equivalence Assessment in the Education Sector using the XAI Model of Blockchain with ECTS
T2  - 2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
SP  - 13
EP  - 20
AU  - S. Krishnan
AU  - R. Surendran
PY  - 2024
KW  - Data privacy
KW  - Explainable AI
KW  - Reviews
KW  - Education
KW  - Memory
KW  - Transforms
KW  - Quality control
KW  - european credit transfer and accumulation system
KW  - explainable artificial intelligence
KW  - chatbots
KW  - shapley additive explanations
KW  - local interpretable model-agnostic explanations
DO  - 10.1109/ICAAIC60222.2024.10575529
JO  - 2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
Y1  - 5-7 June 2024
AB  - The procedure for obtaining an equivalency certificate for international educational recognition is typically complicated and opaque, and differs depending on the nation and system. To overcome these issues and empower students, this study suggests a revolutionary assessment tool that makes use of blockchain technology, chatbots, the European Credit Transfer and Accumulation System (ECTS), and Explainable Artificial Intelligence (XAI). Educational equivalency assessments frequently face difficulties and lack of openness in a variety of settings. The suggested solution uses blockchain for tamper-proof record keeping and secure data storage, based on the capabilities of each component. This improves the blockchain’s ability to securely store application data and evaluation results, fostering immutability and trust. Using the distributed ledger feature of blockchain promotes fairness in evaluations by preventing tampering and guaranteeing data integrity. The blockchain ensures data security and privacy by encrypting and storing data. Discuss how XAI might explain AI-driven equivalence choices, promoting fairness and trust, by reviewing pertinent material in each domain. Chatbots can improve accessibility by streamlining data collection and assisting students along the way. Transparency and efficiency are provided via ECTS computations that integrate XAI and chatbots. Emphasizing the availability of multilingual support for international students, we also address issues such as data privacy and system adaption. The study recommends further research to assess the multifaceted method in practical contexts and improve the technology for moral and efficient application. In the end, both students and institutions will benefit from this, as it can empower individuals and promote international mobility of degree equivalization.
ER  - 

TY  - JOUR
TI  - Explainable Machine Learning for Radio Environment Mapping: An Intelligent System for Electric Field Strength Monitoring
T2  - IEEE Access
SP  - 75104
EP  - 75122
AU  - Y. Kiouvrekis
AU  - T. Panagiotakopoulos
AU  - E. Nousi
AU  - I. Filippopoulos
AU  - A. Ploussi
AU  - E. Spyratou
AU  - E. P. Efstathopoulos
PY  - 2025
KW  - Machine learning
KW  - Predictive models
KW  - Radio frequency
KW  - Accuracy
KW  - Electric fields
KW  - Random forests
KW  - Population density
KW  - Explainable AI
KW  - Estimation
KW  - Buildings
KW  - Electric field strength
KW  - explainable machine learning
KW  - machine learning
KW  - radio environment map
KW  - WEB GIS
DO  - 10.1109/ACCESS.2025.3564650
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 13
VL  - 13
JA  - IEEE Access
Y1  - 2025
AB  - The accurate characterization of signal propagation is critical for optimizing wireless network performance and supporting applications such as electromagnetic field (EMF) exposure assessment and the development of Radio Environmental Maps (REMs). This study proposes a novel, explainable machine learning system to predict electric field strength across diverse urban, semi-urban, and rural environments in Cyprus. The system is trained on a rich dataset comprising 6,543 EMF measurements collected in 2023 at mobile phone and digital TV stations, following CEPT/ECC/REC/(02)04 recommendations. The dataset includes geospatial and environmental features such as antenna distance, population density, urbanization level, and detailed built environment characteristics (e.g., volume, surface, and height). We evaluate multiple machine learning models—kNN, neural networks, decision trees, random forests, XGBoost, and LightGBM—using a two-semester split for training and assessment. Best performance was achieved with the Random Forest model, which yielded the lowest RMSE among all models. Gradient boosting models (XGBoost and LightGBM) also performed well, with RMSE values slightly higher than RF while offering flexible and scalable configurations. In contrast, k-NN and neural networks showed higher RMSE values, indicating they were less effective for this specific task. Across all models, confidence intervals were narrow, demonstrating stable and reliable predictions. Explainable AI techniques revealed that antenna distance, building volume, and population density are the most influential predictors of EMF intensity. Our approach outperforms traditional signal models by incorporating urban morphology and demographic context. As part of this system, we also create a Geographic Information System (GIS) that displays electromagnetic field strength maps derived from our explainable machine learning models. This contributes a scalable, interpretable framework for EMF exposure mapping to support regulatory monitoring, urban planning, and smart city initiatives.
ER  - 

TY  - CONF
TI  - Interpreting Cervical Cancer Risk Predictions Using Optimized Random Forest and Explainable AI Techniques
T2  - 2025 8th International Conference on Trends in Electronics and Informatics (ICOEI)
SP  - 1677
EP  - 1682
AU  - S. G
AU  - N. Meenakshisundaram
PY  - 2025
KW  - Visualization
KW  - Accuracy
KW  - Explainable AI
KW  - Predictive models
KW  - Medical diagnosis
KW  - Risk management
KW  - Cervical cancer
KW  - Random forests
KW  - Medical diagnostic imaging
KW  - Resilience
KW  - Machine Learning Algorithms
KW  - Cervical Cancer
KW  - Random Forest
KW  - Explainable AI (XAI)
KW  - LIME
DO  - 10.1109/ICOEI65986.2025.11013521
JO  - 2025 8th International Conference on Trends in Electronics and Informatics (ICOEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 8th International Conference on Trends in Electronics and Informatics (ICOEI)
Y1  - 24-26 April 2025
AB  - Cervical cancer remains a major public health concern, requiring accurate and interpretable risk prediction tools for early diagnosis. Machine learning models often lack transparency, limiting their adoption in clinical settings. This study proposes a robust framework using an optimized Random Forest classifier for cervical cancer risk prediction, combined with Explainable AI (XAI) techniques such as LIME. The Random Forest model is optimized using key hyperparameters to enhance predictive accuracy. LIME provides local explanations, visualizing feature contributions for individual predictions, while global feature importance identifies the most influential risk factors. To ensure clinical relevance, feature scaling reversal is applied, enabling interpretable outputs. The proposed framework achieves high accuracy, offers transparent decision-making, and enhances trust among medical professionals. This study demonstrates the potential of explainable machine learning for improving cervical cancer diagnosis.
ER  - 

TY  - CONF
TI  - Intelligent and Automated Fault Detection and Diagnosis Strategy for HVAC Systems Based on Maintainability Rules for Construction 4.0
T2  - 2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)
SP  - 1946
EP  - 1951
AU  - M. Y. L. Chew
AU  - K. Yan
PY  - 2021
KW  - HVAC
KW  - Smart buildings
KW  - Smart cities
KW  - Fault detection
KW  - Ventilation
KW  - Cognition
KW  - Bayes methods
KW  - Maintainability
KW  - AI integrated control
KW  - HVAC System
KW  - Facility Management
DO  - 10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00291
JO  - 2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)
Y1  - 20-22 Dec. 2021
AB  - Intelligent and automated fault detection and diagnosis (IAFDD) of heating ventilation air conditioning (HVAC) systems is one important technique in the development process of construction 4.0. Data-driven IAFDD adopts various rules analyzing sensor data collected by the Internet of things (IoT) system for detecting and diagnosing potential faults for facilities. A rule-based system, named, a three-layer diagnostic Bayesian network (DBN) is developed in this study. A set of maintainability rules have been identified through an interview and survey process, supplementing the development of the DBN. The outcome of this study concretizes the development and implementation of the diagnostic Bayesian network for HVAC IAFDD.
ER  - 

TY  - CONF
TI  - CredShield: Decentralized AI for Secure and Adaptive Credit Risk Management
T2  - 2025 Fourth International Conference on Smart Technologies, Communication and Robotics (STCR)
SP  - 1
EP  - 5
AU  - H. Yadav
AU  - P. Jain
AU  - L. D
AU  - A. Mayuri
PY  - 2025
KW  - Logistic regression
KW  - Accuracy
KW  - Neural networks
KW  - Predictive models
KW  - Solids
KW  - Credit cards
KW  - Data models
KW  - Risk management
KW  - Robots
KW  - Random forests
KW  - Credit risk analysis
KW  - DBSCAN clustering
KW  - machine learning
KW  - transactional data
DO  - 10.1109/STCR62650.2025.11019560
JO  - 2025 Fourth International Conference on Smart Technologies, Communication and Robotics (STCR)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 Fourth International Conference on Smart Technologies, Communication and Robotics (STCR)
Y1  - 9-10 May 2025
AB  - The amount of credit card default prediction is large because banks and financial institutions are searching for a method to predict cards that can be chosen according to the borrower’s credibility. This work predicts how people will pay for credit with transactional and demographic data and runs a few models of machine learning, i.e. logistic regression, random forest, and neural network. Because data can be partitioned to understand and enhance the likelihood of accurate prediction success, KMeans is combined with DBSCAN clustering to improve the accuracy of prediction as the performance of data depends on the input data, which is less than the performance of defined DBSCAN. The results of the proposed method of this UCI machine learning repository dataset were 99.32% compared to traditional models that were 82.8% and 83.5%, respectively. One is the need to do data normalization, and the other is to select appropriate methods for multifold validation to get maximum performance. Ultimately, this solid analytical system aids in finer credit risk analysis, inhibits inclusion of default, and so ends up with heart-wrenching and lucrative loan approvals.
ER  - 

TY  - CONF
TI  - A Robust Data-Driven Predictive Maintenance Framework for Industrial Machinery using Explainable Machine Learning Techniques
T2  - 2023 9th International Conference on Smart Computing and Communications (ICSCC)
SP  - 138
EP  - 143
AU  - T. Simi
AU  - N. G. Resmi
PY  - 2023
KW  - Support vector machines
KW  - Degradation
KW  - Machine learning algorithms
KW  - Training data
KW  - Feature extraction
KW  - Classification algorithms
KW  - Decision trees
KW  - Predictive maintenance
KW  - Autoencoder
KW  - NASA Bearing
KW  - K-shape clustering
KW  - Gradient boost classifier
DO  - 10.1109/ICSCC59169.2023.10335030
JO  - 2023 9th International Conference on Smart Computing and Communications (ICSCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 9th International Conference on Smart Computing and Communications (ICSCC)
Y1  - 17-19 Aug. 2023
AB  - The best performance and longevity of industrial machinery are dependent on predictive maintenance. The proposed framework is a reliable and effective predictive maintenance tool that makes use of the widely used NASA-bearing dataset. It uses advanced machine learning algorithms in a multi-step procedure that includes data preparation, anomaly identification, and fault categorization. Then we tried to learn the underlying patterns in the data using an autoencoder, a potent unsupervised learning technique, and then set a threshold to detect anomalies. Anomalies are defined as deviations from the reconstructed signal, which the autoencoder is trained to recreate from the input data. Next, we employed k-shape clustering, a state-of-the-art time-series clustering method, to group similar patterns together in the latent representation obtained from the autoencoder. This enables us to obtain more accurate anomaly labels and improve fault classification accuracy. To further enhance the accuracy of the framework, we extracted several features from the data, including the poisson process, latent representation, frequency, and time domain features. These features are then used as inputs to a decision tree classifier, a popular machine-learning algorithm that can handle both categorical and numerical data. Additionally, we experimented with the gradient boost classifier, a boosting algorithm that improves the performance of the decision tree classifier. The experimental results demonstrate that our proposed framework achieves an accuracy of 95 percent using the gradient boost classifier, outperforming traditional approaches. The combination of the autoencoder-based anomaly detection and the decision tree classifier with the extracted features provides a powerful predictive maintenance framework that can improve the reliability and performance of machinery, reduce downtime and maintenance costs, and ultimately enhance overall productivity in various industrial settings.
ER  - 

TY  - JOUR
TI  - Enhancing Cybersecurity of a Hydroelectric Power Plant Through Digital Twin Modeling and Explainable AI
T2  - IEEE Access
SP  - 41887
EP  - 41908
AU  - I. Erkek
AU  - E. Irmak
PY  - 2025
KW  - Digital twins
KW  - Computer security
KW  - Production
KW  - Computer crime
KW  - Anomaly detection
KW  - Real-time systems
KW  - Optimization
KW  - Hydroelectric power generation
KW  - Explainable AI
KW  - Data models
KW  - Cyber security
KW  - digital twins
KW  - explainable artificial intelligence
KW  - hydroelectric power plant
DO  - 10.1109/ACCESS.2025.3547672
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 13
VL  - 13
JA  - IEEE Access
Y1  - 2025
AB  - Hydroelectric power plants (HEPPs) are vital components of the renewable energy infrastructure, making their operational security and efficiency critical. HEPPs face increasing vulnerability to cyber threats, which can disrupt operations and compromise energy production. This study investigates the integration of Digital Twin (DT) technology with Explainable Artificial Intelligence (XAI) to improve cybersecurity and anomaly detection in a HEPP located in Türkiye. The DT model simulates key operational parameters, such as water flow, mechanical power, and turbine speed, enabling real-time monitoring, optimization, and secure cybersecurity testing. Simulated cyberattacks on the DT have revealed vulnerabilities in the Modbus protocol, while SHapley Additive exPlanations (SHAP) analysis, an XAI technique, clarifies the influence of operational parameters on anomaly detection outcomes. Gravity with a SHAP value of 0.0001 and water density with a SHAP value of 0.0018 have been identified as the least influential parameters, suggesting limited variability or impact on the detected anomalies. These findings enable the prioritization of critical variables while reducing unnecessary monitoring efforts. The proposed integration improves the accuracy of anomaly detection, enables precise vulnerability identification, and mitigates operational risks without impacting real-world systems. The results demonstrate the effectiveness of the model in detecting anomalies and strengthening the resilience of the system against cyber threats. This approach also provides actionable insights to optimize operational processes, ensuring secure and efficient energy production. The study highlights the transformative potential of DT-XAI integration in advancing the sustainability, security, and resilience of critical energy infrastructures.
ER  - 

TY  - CONF
TI  - XCardio-Twin: An explainable framework to aid in monitoring and analysis of cardiovascular status
T2  - 2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)
SP  - 1
EP  - 6
AU  - R. Krzysiak
AU  - D. An
AU  - Y. Chen
PY  - 2023
KW  - Wearable computers
KW  - Decision making
KW  - Machine learning
KW  - Electrocardiography
KW  - Predictive models
KW  - Feature extraction
KW  - Real-time systems
KW  - healthcare
KW  - ECG
KW  - digital-twin
KW  - machine learning
KW  - cardiovascular system
KW  - XAI
DO  - 10.1109/DTPI59677.2023.10365417
JO  - 2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)
Y1  - 7-9 Nov. 2023
AB  - We present an explainable digital-twin framework designed for cardiovascular health monitoring by integrating feature extraction using the NeuroKit2 and Heartpy python libraries and predictive modeling through machine learning. The digital-twin extracts key physiological features from ECG data, such as heart rate variability, and utilized machine learning algorithms to predict individual cardiovascular status. To improve trustworthiness, interpretability and transparency behind the decision making process, SHAP (Shapley Additive Explanations) was paired to help describe the underlying contributions of each feature to the model’s decision-making process. The performance of the machine learning model scored a 90.08 % at accurately predicting abnormal or normal ECG signal. The combination of extraction of key features from ECG signals and a trained explainable machine learning model offers transparent, personalized insights into cardiovascular health.
ER  - 

TY  - CONF
TI  - Interpretable Garment Workers’ Productivity Prediction in Bangladesh Using Machine Learning Algorithms and Explainable AI
T2  - 2022 25th International Conference on Computer and Information Technology (ICCIT)
SP  - 236
EP  - 241
AU  - H. H. Sabuj
AU  - N. S. Nuha
AU  - P. R. Gomes
AU  - A. Lameesa
AU  - M. A. Alam
PY  - 2022
KW  - Productivity
KW  - Training
KW  - Industries
KW  - Clothing
KW  - Machine learning
KW  - Predictive models
KW  - Feature extraction
KW  - Productivity prediction
KW  - Regression Problem
KW  - Machine Learning Algorithms
KW  - Random Forest
KW  - Explainable AI
DO  - 10.1109/ICCIT57492.2022.10054863
JO  - 2022 25th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 25th International Conference on Computer and Information Technology (ICCIT)
Y1  - 17-19 Dec. 2022
AB  - Bangladesh’s garment industry is widely recognized and plays a significant role in the current global market. The nation’s per capita income and citizens’ living standards have risen significantly with the noteworthy hard work performed by the employees in this industry. The garment sector is more efficient once the target production can be achieved without any difficulties. But a frequent issue that comprises within this industry is, often the actual garment producing productivity of the people working there do not reach the previously determined target-productivity. The business suffers a significant loss when the productivity gap appears in this process. This approach seeks to address this issue by prediction of the actual productivity of the workers. To attain this goal, a machine learning approach is suggested for the productivity prediction of the employees, after experimentation with five machine learning models. The proposed approach displays a reassuring level of prediction accuracy, with a minimalist MAE (Mean Absolute Error) of 0.072, which is less than the existing Deep Learning model with a MAE of 0.086. This indicates that, application of this process can play a vital role in setting an accurate target production which might lead to more profit and production in the sector. Also, this work contains an explainable AI technique named SHAP for interpreting the model in order to see further information within it.
ER  - 

TY  - JOUR
TI  - Extracting Structural Elements From 3D Point Clouds in Indoor Environments via Machine Learning Techniques
T2  - IEEE Access
SP  - 94461
EP  - 94476
AU  - K. Aksu
AU  - H. Demirel
PY  - 2024
KW  - Point cloud compression
KW  - Feature extraction
KW  - Three-dimensional displays
KW  - Machine learning
KW  - Solid modeling
KW  - Accuracy
KW  - Machine learning algorithms
KW  - Three-dimensional displays
KW  - 3D point cloud
KW  - classification
KW  - explainable machine learning
KW  - indoor environment
KW  - local neighborhood
KW  - machine learning
KW  - structural element
KW  - terrestrial laser scanning
DO  - 10.1109/ACCESS.2024.3423425
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - The utilization of three-dimensional point clouds is an advanced approach for detecting the geometry of objects within a building environment. Nonetheless, a vast amount of data still needs to be manually processed. Intelligent automation frameworks could be deployed to overcome such issues. Hence, this study proposes a machine learning-based framework for successfully classifying structural components in indoor environments. The proposed framework consists of four stages: pre-processing, feature extraction, feature selection, and interpretability of classification results using an explainable machine learning method. According to the proposed framework, the chi-squared test stands out for optimum local neighborhood radius determination and feature selection. The CatBoost model has the highest accuracy of 82.96%, whereas the Random Forest model’s accuracy is 82.09%. However, the training time for the Random Forest is 27 times shorter than the CatBoost. Hence, both models could be preferred to other machine learning models for practical applications due to the good balance between accuracy and calculation efficiency. Additionally, the model with the highest accuracy, CatBoost, is evaluated using the Shapley Additive exPlanations to understand the impacts of features on predictions, and according to the results, Z coordinate and verticality had a relatively high impact on the model, while others had low impacts. The proposed framework uses machine learning to classify indoor point clouds, balancing processing time and accuracy for computational efficiency in practical applications. Hence, the framework could be utilized to automate the digitalization efforts of indoor environments effectively.
ER  - 

TY  - CONF
TI  - Dynamic Landslide Prediction, Monitoring, and Early Warning with Explainable AI: A Comprehensive Approach
T2  - 2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
SP  - 960
EP  - 965
AU  - K. E. Binu
AU  - L. T. Anoopkumar
AU  - M. Sunil
AU  - M. Jose
AU  - K. G. Preetha
PY  - 2024
KW  - Landslides
KW  - Explainable AI
KW  - Computational modeling
KW  - Disasters
KW  - Decision making
KW  - Predictive models
KW  - Terrain factors
KW  - Landslide
KW  - Explainable Artificial Intelligence (XAI)
KW  - Local Interpretable Model-agnostic Explanations (LIME)
KW  - SHapley Additive exPlanations (SHAP)
KW  - Early Warning
DO  - 10.1109/ICAAIC60222.2024.10575330
JO  - 2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
Y1  - 5-7 June 2024
AB  - Landslides are a major threat to infrastructure, human lives, and the environment. Therefore, early warning systems and precise forecasting are essential for reducing their effects. This study investigates how to enhance landslide prediction models using Explainable Artificial Intelligence (XAI) methodologies. Intending to improve complex machine learning models’ interpretability and transparency, XAI seeks to improve comprehension and confidence in the model’s predictions. XAI methodologies used are SHapley Additive exPlanations (SHAP), LIME (Local Interpretable Model-agnostic Explanations), and feature importance analysis. These techniques produce comprehensible explanations for each forecast and assist in determining the most significant variables in landslide prediction. The proposed XAI-enhanced landslide prediction model demonstrates promising results in terms of accuracy and interpretability. The research reported here helps to design more potent early warning systems for landslides by advancing explainable AI applications in geoscience and disaster risk reduction.
ER  - 

TY  - CONF
TI  - AI-Driven Credit Scoring and Credit Line Solution for the Unreserved and Self-Employed
T2  - 2024 Second International Conference on Inventive Computing and Informatics (ICICI)
SP  - 178
EP  - 184
AU  - T. Kumbhar
AU  - D. Agrawal
AU  - L. Saldanha
AU  - D. Koshti
PY  - 2024
KW  - Accuracy
KW  - Uncertainty
KW  - Explainable AI
KW  - Profitability
KW  - Motorcycles
KW  - Ensemble learning
KW  - Risk management
KW  - AI-based credit score system
KW  - Credit line
KW  - Ensemble learning
KW  - Explainable AI
KW  - Loan
KW  - Random Forest Classifier
KW  - Financial Institution
KW  - Bank
KW  - Collateral analysis
KW  - bank statements
DO  - 10.1109/ICICI62254.2024.00039
JO  - 2024 Second International Conference on Inventive Computing and Informatics (ICICI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Second International Conference on Inventive Computing and Informatics (ICICI)
Y1  - 11-12 June 2024
AB  - The financial industry's rapid evolution emphasizes the importance of accurate credit scoring, crucial for decision-making and timely access to loans. Challenges arise due to irregular income streams experienced by self-employed individuals, leading to uncertainty in demonstrating consistent repayment capacity to lenders. Self-employed individuals are often perceived as higher-risk borrowers due to income instability, resulting in stricter lending criteria, higher interest rates, or reduced loan amounts. Customer credit classes are based on default risk, with precise risk assessment vital for profitability. The proposed system employs a Random Forest Classifier Ensemble Learning model on a Two-wheeler Motorcycle Indian Dataset to enhance credit risk assessment comprehensively, especially for self-employed individuals. Credit line predictions incorporating ensemble learning on random forest are being utilized to provide loan amount estimations along with risk considerations, either by taking into account bank statements or other relevant financial data. The proposed system vouches to deliver an accurate result for those who seek aid from financial institutions. The proposed model achieves an accuracy of 87% and good MSE score of 25680.607346699337 by using the Random forest and Ensemble Learning model. The explainable AI graph serves as a valuable tool for transparency and understanding in AI systems, helping users trust and interpret the model's decisions more effectively.
ER  - 

TY  - CHAP
TI  - Chapter 13 Leveraging Artificial Intelligence for Enhanced Risk Management in Banking: A Systematic Literature Review
T2  - Artificial Intelligence Enabled Management: An Emerging Economy Perspective
SP  - 197
EP  - 214
AU  - Narayanage Jayantha Dewasiri
AU  - Dunusinghe G. Dharmarathna
AU  - Mrinalini Choudhary
PY  - 2024
KW  - Risk management
KW  - Artificial intelligence
KW  - Banking
KW  - Industries
KW  - Economic indicators
KW  - Machine learning
KW  - Complexity theory
KW  - Natural language processing
KW  - Guidelines
KW  - Fraud
DO  - 
PB  - De Gruyter
SN  - 9783111173252
UR  - http://ieeexplore.ieee.org/document/10790827
AB  - This systematic review delves into the transformative role of Artificial Intelligence (AI) in the banking industry’s risk management practices. AI, encompassing machine learning, data analytics, and natural language processing, has enhanced risk assessment, mitigation, and decision-making processes. The findings emphasise AI’s capacity to identify and assess risks, enabling proactive risk management effectively. Applications like credit scoring models, fraud detection systems, and stress testing tools play instrumental roles in optimising risk management processes. At the same time, the importance of data quality, governance, and transparency cannot be overstated in successfully implementing AI-driven risk management strategies. The implications of AI in banking are profound, offering data-driven procedures, equitable lending practices, and enhanced operational efficiency. However, data privacy concerns, model interpretability issues, and regulatory compliance complexities must be addressed carefully. Emerging trends in AI for risk management encompass Explainable AI, AI-enabled regulatory Compliance, AI for Cybersecurity Risk Management, and Natural Language Processing for Unstructured Data Analysis, along with the optimisation of efficiency through Robotic Process Automation in Risk Operations. Future research should focus on ethical considerations, dynamic stress testing models, AI’s role in climate-related risk analysis, human-AI collaboration, cybersecurity risk prediction, and the development of robust regulatory frameworks for AI integration in risk management. AI stands poised to revolutionise banking risk management. Still, responsible and ethical integration is paramount, necessitating collaborative efforts to harness its full potential while ensuring trust and stability within the sector.
ER  - 

TY  - CONF
TI  - Deep Neural Networks for Anti Money Laundering Using Explainable Artificial Intelligence
T2  - 2024 IEEE 12th International Conference on Intelligent Systems (IS)
SP  - 1
EP  - 6
AU  - G. Konstantinidis
AU  - A. Gegov
PY  - 2024
KW  - Measurement
KW  - Analytical models
KW  - Explainable AI
KW  - Decision making
KW  - Artificial neural networks
KW  - Benchmark testing
KW  - Risk management
KW  - Intelligent systems
KW  - Synthetic data
KW  - Resilience
KW  - XAI
KW  - AML
KW  - ANN
KW  - Fraud Detection
KW  - ML
DO  - 10.1109/IS61756.2024.10705194
JO  - 2024 IEEE 12th International Conference on Intelligent Systems (IS)
IS  - 
SN  - 2767-9802
VO  - 
VL  - 
JA  - 2024 IEEE 12th International Conference on Intelligent Systems (IS)
Y1  - 29-31 Aug. 2024
AB  - This paper explores the application of machine learning (ML) and explainable AI (XAI) techniques for detecting money laundering in financial transactions. A novel approach is introduced that combines a deep neural network (DNN) with SHapley Additive exPlanations (SHAP) to enhance the transparency and effectiveness of anti-money laundering (AML) systems. The proposed model demonstrates superior performance over benchmark models, achieving high precision (0.994585), recall (0.994500), F1 score (0.994551), and ROC AUC (0.994525) in identifying fraudulent transactions using a synthetic dataset derived from real financial logs. Through a global explainability analysis, key indicators of fraudulent activities, such as high transaction amounts and prolonged transaction durations, are identified. This study contributes to the AML field by improving model accuracy and providing insights into the decision-making processes of complex ML models. Future research will focus on applying local explanations and utilizing larger real world datasets to further enhance model performance and interpretability.
ER  - 

TY  - CONF
TI  - Explainable AI for Crop disease detection
T2  - 2022 4th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)
SP  - 1601
EP  - 1608
AU  - R. S
AU  - I. M
PY  - 2022
KW  - Deep learning
KW  - Productivity
KW  - Plant diseases
KW  - Visualization
KW  - Biological system modeling
KW  - Computational modeling
KW  - Transfer learning
KW  - Crop Disease
KW  - deep learning
KW  - neural networks
KW  - Convolution neural networks
KW  - ResNet
KW  - Inception V3
KW  - Explainable AI
KW  - LIME
KW  - Grad-CAM
DO  - 10.1109/ICAC3N56670.2022.10074303
JO  - 2022 4th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 4th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)
Y1  - 16-17 Dec. 2022
AB  - Agriculture is a major contributor to the Nation’s economy since it is the most important component. It is common for plants to suffer from various kinds of diseases, which may be influenced by extreme climatic conditions and pests. This further lowers the quality of the harvest. In order to calculate the overall crop yield and productivity, early detection and accurate diagnosis of plant diseases are critical, which can significantly increase productivity and yield. Farmers have difficulty identifying diseases in plant leaves. Diagnosing diseases through traditional methods requires extensive field experience & expertise. The advancement of technology has allowed numerous strategies to be developed to identify plant diseases using Artificial Intelligence and Deep Learning. To detect and classify various crop diseases, we propose two models, Inception V3 and ResNet-9, which are deployed on Plant Village Dataset and New Plant Disease Dataset. In addition to Deep Learning models, Explainable AI (XAI) tools such as LIME and Grad-CAM have been utilized for understanding the black-box nature of Deep Learning models.
ER  - 

TY  - JOUR
TI  - XAI for Industry 5.0—Concepts, Opportunities, Challenges, and Future Directions
T2  - IEEE Open Journal of the Communications Society
SP  - 2706
EP  - 2729
AU  - T. R. Gadekallu
AU  - P. Kumar Reddy Maddikunta
AU  - P. Boopathy
AU  - N. Deepa
AU  - R. Chengoden
AU  - N. Victor
AU  - W. Wang
AU  - W. Wang
AU  - Y. Zhu
AU  - K. Dev
PY  - 2025
KW  - Surveys
KW  - Artificial intelligence
KW  - Explainable AI
KW  - Smart manufacturing
KW  - Fifth Industrial Revolution
KW  - Computer science
KW  - Production systems
KW  - Collaboration
KW  - Agriculture
KW  - Technological innovation
KW  - XAI
KW  - AI
KW  - industry 5.0
KW  - smart factories
KW  - smart healthcare
KW  - E-governance
KW  - smart transportation
KW  - education 5.0
KW  - agriculture 5.0
KW  - energy 5.0
DO  - 10.1109/OJCOMS.2024.3473891
JO  - IEEE Open Journal of the Communications Society
IS  - 
SN  - 2644-125X
VO  - 6
VL  - 6
JA  - IEEE Open Journal of the Communications Society
Y1  - 2025
AB  - Industry 5.0 has become a reality now and it is a paradigm that integrates contemporary innovations and concepts. Artificial Intelligence (AI) is a key component and asset of the industrial transformation which allows intelligent devices to perform functionalities such as self-examination, assessment, and evaluation autonomously. AI-based methodologies using ML and deep learning assist manufacturers and industrialists in forecasting service requirements and minimizing downtime. Recent research has discovered a remarkable change in the processes, systems, applications, and products in industries. Also, there is a significant challenge with the explainability of the decisions provided by the models using deep learning algorithms and their inadequate ability to be coupled with each other. Therefore, Explainable artificial intelligence (XAI) is required without compromising the efficiency of the models developed using deep learning algorithms. XAI investigates and develops algorithms, techniques, and models that produce human-comprehensible explanations of AI-based systems and can increase transparency and performance. The explainability nature of XAI will help humans understand the model and the reason behind the predictions, thus improving the model’s transparency and the reliability of the outcomes. Furthermore, an Industry 5.0-enabled environment has a variety of data from varied sources, and this multi-source information must be fused to derive meaningful and optimal decisions. Therefore, all AI-integrated applications must derive actionable insights through information fusion. Hence, the adoption of XAI methodologies in Industry 5.0 can help humans make trustworthy decisions for critical applications requiring information fusion. In this paper, we present a state-of-the-art survey on adopting XAI in Industry 5.0. We discuss the adoption of XAI in various applications such as smart factories, smart Healthcare, E-Governance, smart transportation, Education 5.0, Agriculture 5.0, and Energy 5.0. Finally, some research issues and future directions of integrating XAI with Industry 5.0 are also discussed and highlighted to promote more study in the potential field.
ER  - 

TY  - CONF
TI  - Interpretable Deep Learning for Rice Leaf Disease Detection: A Self-Attention and XAI Framework
T2  - 2025 International Conference on Electrical, Computer and Communication Engineering (ECCE)
SP  - 1
EP  - 6
AU  - A. Nawer
AU  - U. H. Prity
AU  - M. Khaliluzzaman
AU  - Z. Sultana
PY  - 2025
KW  - Deep learning
KW  - Productivity
KW  - Precision agriculture
KW  - Learning systems
KW  - Accuracy
KW  - Transfer learning
KW  - Predictive models
KW  - Convolutional neural networks
KW  - Reliability
KW  - Diseases
KW  - Rice Leaf Disease Detection
KW  - CNN
KW  - Self-Attention
KW  - Transfer Learning
KW  - SHAP
KW  - Deep Learning
KW  - XAI
DO  - 10.1109/ECCE64574.2025.11013842
JO  - 2025 International Conference on Electrical, Computer and Communication Engineering (ECCE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Electrical, Computer and Communication Engineering (ECCE)
Y1  - 13-15 Feb. 2025
AB  - Rice leaf diseases pose a significant challenge to food security and agricultural productivity. To address this challenge, innovative disease detection methods are required. A deep learning-based method for detecting rice leaf diseases is proposed in this study, utilizing a Convolutional Neural Network (CNN) enhanced with a self-attention mechanism. The model was evaluated on an augmented dataset consisting of 22,974 images, which were expanded from 3,829 using data augmentation techniques. Disease identification and classification were performed using CNNs, including Self-Attention CNN, customized CNN, InceptionV3, MobileNetV2, and ResNet-50. The effectiveness of these models was demonstrated through experimental results, with the Self-Attention CNN achieving an accuracy of 0.9974, the customized CNN achieving 0.9857, InceptionV3 at 0.9665, MobileNetV2 at 0.9458, and ResNet-50 at 0.8847. SHapley Additive explanations (SHAP) were employed to provide pixel-level insights into the model’s predictions by integrating deep learning techniques with model interpretability methods. SHAP heatmaps confirmed that the model focused on disease-specific regions, aligning with domain knowledge and enhancing trust in the system. This transparency improves the reliability of the AI-driven disease detection system and supports its applicability in agriculture. The research demonstrates the effectiveness of integrating self-attention mechanisms with explainable AI, offering a scalable, interpretable, and highly accurate solution for rice leaf disease detection. By enabling precise disease identification and timely intervention, the study advances precision agriculture and promotes sustainable crop management.
ER  - 

TY  - CONF
TI  - Predictive Maintenance for Industrial Equipment: Using XGBoost and Local Outlier Factor with Explainable AI for analysis
T2  - 2024 14th International Conference on Cloud Computing, Data Science & Engineering (Confluence)
SP  - 25
EP  - 30
AU  - P. Ghadekar
AU  - A. Manakshe
AU  - S. Madhikar
AU  - S. Patil
AU  - M. Mukadam
AU  - T. Gambhir
PY  - 2024
KW  - Productivity
KW  - Explainable AI
KW  - Merging
KW  - Predictive models
KW  - Prediction algorithms
KW  - Reliability
KW  - Task analysis
KW  - Explainable AI
KW  - Industrial materials
KW  - Predictive maintenance
KW  - Local Outlier Factor
KW  - XGBoost
DO  - 10.1109/Confluence60223.2024.10463280
JO  - 2024 14th International Conference on Cloud Computing, Data Science & Engineering (Confluence)
IS  - 
SN  - 2766-421X
VO  - 
VL  - 
JA  - 2024 14th International Conference on Cloud Computing, Data Science & Engineering (Confluence)
Y1  - 18-19 Jan. 2024
AB  - In industrial operations, the need to minimize downtime and enhance productivity has produced the need for predictive maintenance techniques. Using artificial intelligence (AI) in this domain has revolutionized maintenance practices, but the lack of transparency of many AI models has obstructed their widespread acceptance and trustworthiness. This research paper explores the application of XGBoost and Local Outlier Factor algorithms in the domain of Predictive Industrial Maintenance for industrial materials. Using these advanced machine learning techniques, this research aims to predict equipment failures and improve the overall reliability of industrial processes. In addition, this research employs Explainable AI methods to provide clear and interpretable insights into the predictive models' decision-making processes. By combining the power of XGBoost and Local Outlier Factor with explainability. In this study, for predictive classification, the XGBoost gave an F1 score of 96% and for early prediction Local Outlier Factor gave an F1 score of 94% this research also explained the impact of features on Output class using the SHAP model. This research not only enhances predictive accuracy but also ensures transparency, enabling users to make informed decisions for timely maintenance and system optimization.
ER  - 

TY  - CONF
TI  - Data Engineering and ML for Real-Time Fraud Detection in Financial Transactions
T2  - 2024 IEEE 8th Conference on Energy Internet and Energy System Integration (EI2)
SP  - 91
EP  - 96
AU  - M. A. Reddy Basani
PY  - 2024
KW  - Accuracy
KW  - Federated learning
KW  - Explainable AI
KW  - Scalability
KW  - System integration
KW  - Data engineering
KW  - Real-time systems
KW  - Data models
KW  - Fraud
KW  - Financial services
KW  - Federated Learning
KW  - Explainable AI
KW  - Fraud Detection
KW  - Data Privacy
KW  - Imbalanced Datasets
KW  - Financial Institutions
KW  - Model Transparency
KW  - Collaborative Learning
KW  - Customer Confidentiality
KW  - Risk Management
DO  - 10.1109/EI264398.2024.10991757
JO  - 2024 IEEE 8th Conference on Energy Internet and Energy System Integration (EI2)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 8th Conference on Energy Internet and Energy System Integration (EI2)
Y1  - 29 Nov.-2 Dec. 2024
AB  - Fraud detection remains a critical challenge for financial institutions, especially in high-frequency transaction environments. This study proposes a real-time, AI-driven fraud detection system utilizing XGBoost and optimized data engineering to process up to 10,000 transactions per second with minimal latency of 7ms. The system addresses the issue of data imbalance in fraudulent transactions while ensuring transparency through Explainable AI (XAI), utilizing SHAP values to make model predictions interpretable. Experimental results on real-world financial transaction datasets show that the proposed system achieves 99.2% accuracy, with precision and recall scores exceeding 95% and 93%, respectively. When compared to baseline models and existing literature. our approach demonstrates superior scalability, speed, and accuracy. This solution offers a robust and efficient framework for real-time fraud detection, ensuring both performance and transparency, while maintaining low prediction latency, making it ideal for modern financial services.
ER  - 

TY  - CONF
TI  - AgroXAI: Explainable AI-Driven Crop Recommendation System for Agriculture 4.0
T2  - 2024 IEEE International Conference on Big Data (BigData)
SP  - 7208
EP  - 7217
AU  - Ö. Turgut
AU  - İ. Kök
AU  - S. Özdemir
PY  - 2024
KW  - Productivity
KW  - Explainable AI
KW  - Computational modeling
KW  - Crops
KW  - Soil
KW  - Agriculture
KW  - Internet of Things
KW  - Sustainable development
KW  - Recommender systems
KW  - Next generation networking
KW  - Explainable Artificial Intelligence (XAI)
KW  - Agriculture 4.0
KW  - Internet of Things
KW  - edge computing
KW  - crop recommendation
DO  - 10.1109/BigData62323.2024.10825771
JO  - 2024 IEEE International Conference on Big Data (BigData)
IS  - 
SN  - 2573-2978
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Big Data (BigData)
Y1  - 15-18 Dec. 2024
AB  - Today, crop diversification in agriculture is a critical issue to meet the increasing demand for food and to improve food safety and quality. This issue is considered to be the most important challenge for the next generation of agriculture due to diminishing natural resources, limited arable land and unpredictable climatic conditions caused by climate change. In this paper, we employ emerging technologies such as the Internet of Things (IoT), machine learning (ML) and explainable artificial intelligence (XAI) to improve operational efficiency and productivity in the agricultural sector. Specifically, we propose an edge computing-based explainable crop recommendation system, AgroXAI, which suggests suitable crops for a region based on weather and soil conditions. In this system, we provide local and global explanations of ML model decisions with methods such as ELI5, LIME, SHAP, which we integrate into ML models. More importantly, we provide regional alternative crop recommendations with the Counterfactual explainability method. In this way, we envision that our proposed AgroXAI system will be a platform that provides regional crop diversity in the next generation agriculture.
ER  - 

TY  - CONF
TI  - Application of Machine Learning Algorithms in Digital Twin Monitoring Systems: An Overview of Approaches, Methods, and Prospects
T2  - 2024 International Conference on Intelligent Computing and Next Generation Networks (ICNGN)
SP  - 01
EP  - 05
AU  - G. Amirkhanova
AU  - B. Amirkhanov
AU  - G. Tyulepberdinova
AU  - T. Ishmurzin
PY  - 2024
KW  - Machine learning algorithms
KW  - Smart cities
KW  - Reviews
KW  - Transportation
KW  - Predictive models
KW  - Transformers
KW  - Digital twins
KW  - Security
KW  - Reliability
KW  - Industrial Internet of Things
KW  - Digital Twin
KW  - Machine Learning
KW  - Predictive Maintenance
KW  - Anomaly Detection
KW  - Process Optimization
KW  - Reinforcement Learning
KW  - Industry 4.0
KW  - IIoT
KW  - Scenario Modeling
KW  - Time-Series Analysis
DO  - 10.1109/ICNGN63705.2024.10871832
JO  - 2024 International Conference on Intelligent Computing and Next Generation Networks (ICNGN)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Intelligent Computing and Next Generation Networks (ICNGN)
Y1  - 23-25 Nov. 2024
AB  - Digital Twins represent virtual replicas of real-world objects, processes, or systems, enabling continuous real-time monitoring, dynamic adaptation, and predictive analytics. Their integration has become central in industrial, urban, and transportation domains under the paradigms of Industry 4.0, the Industrial Internet of Things (IIoT), and smart cities. However, effectively harnessing the torrents of time-series data and complex system states integral to Digital Twins (DTs) requires advanced analytical tools. Machine learning (ML) methods-ranging from classical ensemble models to deep learning architectures and reinforcement learning-provide powerful mechanisms for anomaly detection, failure prediction, operational optimization, and scenario modeling. This review explores the essential ML algorithms, their application contexts, and the supportive infrastructures required for implementing ML-driven DT solutions. We also discuss interpretability techniques, address security and concept drift challenges, examine the role of MLOps in ensuring reliable deployment, and highlight future research directions, including physics-informed ML, multimodal data integration, and advanced transformers for time-series forecasting.
ER  - 

TY  - CONF
TI  - Deep Digital Twin Services for Personalized MPX Treatment
T2  - 2025 17th International Conference on COMmunication Systems and NETworks (COMSNETS)
SP  - 54
EP  - 59
AU  - C. I. -O. Gabriel
AU  - R. Kumar
AU  - K. Prasad
PY  - 2025
KW  - Adaptation models
KW  - Accuracy
KW  - Transfer learning
KW  - Skin
KW  - Real-time systems
KW  - Digital twins
KW  - Lesions
KW  - Convolutional neural networks
KW  - Public healthcare
KW  - Viruses (medical)
KW  - Attention mechanism
KW  - deep learning
KW  - digital twin
KW  - Mpox
KW  - transfer learning
DO  - 10.1109/COMSNETS63942.2025.10885677
JO  - 2025 17th International Conference on COMmunication Systems and NETworks (COMSNETS)
IS  - 
SN  - 2155-2509
VO  - 
VL  - 
JA  - 2025 17th International Conference on COMmunication Systems and NETworks (COMSNETS)
Y1  - 6-10 Jan. 2025
AB  - With the advent of smart healthcare services, rapid and automated diagnosis from images of skin lesions is critical to combat fast-spreading viruses such as monkeypox (Mpox or MPX) and significantly improve public health. The recent cases in Thailand reporting a suspected first case on August 21, 2024, and Sweden on August 14, 2024, among others, highlight the pandemic threat of Mpox. This study presents the Deep Digital Twin Services for Personalized Treatment (D2T-PT) model, which combines transfer learning and Digital Twin (DT) technology to improve the accuracy of Mpox detection and real-time monitoring, supported by the Squeeze-and-Excitation Block (SEB) attention mechanism, which opens up new horizons for personalized healthcare. Convolutional Neural Network (CNN) models were tested on the Monkeypox Skin Lesion Dataset (MSLD), with the advanced adaptive NasNetMobile model achieving excellent results: 100% recall, 98% ROC score, 97.78% accuracy with precision of 95%. This robust model enables physicians to make early and accurate Mpox diagnoses and monitor patient response to treatment in real-time, ultimately helping to contain the spread of the virus.
ER  - 

TY  - CONF
TI  - Potato Leaf Disease Classification Using Lightweight CNN-SVM and Real-World Web-Based Tools
T2  - 2025 International Conference on Electrical, Computer and Communication Engineering (ECCE)
SP  - 1
EP  - 5
AU  - H. I. Peyal
AU  - M. N. Islam Mondal
PY  - 2025
KW  - Support vector machines
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Transfer learning
KW  - Food security
KW  - Predictive models
KW  - Real-time systems
KW  - Convolutional neural networks
KW  - Diseases
KW  - Lightweight Convolutional Neural Networks
KW  - CNN
KW  - SVM Classifier
KW  - Potato
KW  - Explainable AI (XAI) Techniques
KW  - SHAP
KW  - LIME
DO  - 10.1109/ECCE64574.2025.11013900
JO  - 2025 International Conference on Electrical, Computer and Communication Engineering (ECCE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Electrical, Computer and Communication Engineering (ECCE)
Y1  - 13-15 Feb. 2025
AB  - Potatoes play an important role as a major agricultural product since the economic system of Bangladesh strongly relies on farming. The presence of plant diseases particularly late blight poses substantial threats to economic stability together with food security and harvest yields. Lightweight Convolutional Neural Network (CNN) and Support Vector Machine (SVM) represent a new approach to automate the detection of diseases appearing on potato leaves. The validation accuracy obtained by this model reaches an average of 98.53%. The model sustains high performance by employing 0.45 million parameters which leads to minimized storage needs equivalent to 1.75 MB and reduced computational expenses. SHAP (SHapley Additive exPlanations) together with LIME (Local Interpretable Model-agnostic Explanations) serve as explainable AI techniques used to enhance model prediction transparency. The proposed model excels beyond VGG-16 and VGG-19 transfer learning standards in all evaluation metrics including precision, recall and F1-score even though its parameter count stands at 32.8× and 44.6× lower. Mobile deployment possibilities of the simplified modelarchitecture allow farmers to receive prompt disease diagnoses along with correct decision-making resources. A web-based platform enables users to analyze potato leaf diseases by uploading images in real-time. The study creates substantial progress in agricultural productivity alongside ensuring food security for Bangladesh.
ER  - 

TY  - BOOK
TI  - Responsible AI in the Enterprise: Practical AI risk management for explainable, auditable, and safe models with hyperscalers and Azure OpenAI
SP  - 1
EP  - 
AU  - Adnan Masood
AU  - Heather Dawe
AU  - Dr. Ehsan Adeli
PY  - 2023
DO  - 
PB  - Packt Publishing
SN  - 9781803249667
UR  - http://ieeexplore.ieee.org/document/10251167
AB  - Build and deploy your AI models successfully by exploring model governance, fairness, bias, and potential pitfalls Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn ethical AI principles, frameworks, and governanceUnderstand the concepts of fairness assessment and bias mitigationIntroduce explainable AI and transparency in your machine learning modelsBook DescriptionResponsible AI in the Enterprise is a comprehensive guide to implementing ethical, transparent, and compliant AI systems in an organization. With a focus on understanding key concepts of machine learning models, this book equips you with techniques and algorithms to tackle complex issues such as bias, fairness, and model governance. Throughout the book, you’ll gain an understanding of FairLearn and InterpretML, along with Google What-If Tool, ML Fairness Gym, IBM AI 360 Fairness tool, and Aequitas. You’ll uncover various aspects of responsible AI, including model interpretability, monitoring and management of model drift, and compliance recommendations. You’ll gain practical insights into using AI governance tools to ensure fairness, bias mitigation, explainability, privacy compliance, and privacy in an enterprise setting. Additionally, you’ll explore interpretability toolkits and fairness measures offered by major cloud AI providers like IBM, Amazon, Google, and Microsoft, while discovering how to use FairLearn for fairness assessment and bias mitigation. You’ll also learn to build explainable models using global and local feature summary, local surrogate model, Shapley values, anchors, and counterfactual explanations. By the end of this book, you’ll be well-equipped with tools and techniques to create transparent and accountable machine learning models.What you will learnUnderstand explainable AI fundamentals, underlying methods, and techniquesExplore model governance, including building explainable, auditable, and interpretable machine learning modelsUse partial dependence plot, global feature summary, individual condition expectation, and feature interactionBuild explainable models with global and local feature summary, and influence functions in practiceDesign and build explainable machine learning pipelines with transparencyDiscover Microsoft FairLearn and marketplace for different open-source explainable AI tools and cloud platformsWho this book is forThis book is for data scientists, machine learning engineers, AI practitioners, IT professionals, business stakeholders, and AI ethicists who are responsible for implementing AI models in their organizations.
ER  - 

TY  - CONF
TI  - Incorporating Random Forests and Explainable AI for Upper Motor Function Assessment with the Wolf Motor Function Test
T2  - 2025 15th International Conference on Electrical Engineering (ICEENG)
SP  - 1
EP  - 6
AU  - M. A. Abdelaziz
AU  - M. I. Awad
AU  - S. A. Maged
AU  - M. Gaber
PY  - 2025
KW  - Adaptation models
KW  - Accuracy
KW  - Explainable AI
KW  - Training data
KW  - Stroke (medical condition)
KW  - Motors
KW  - Robustness
KW  - Resource description framework
KW  - Random forests
KW  - Testing
KW  - eXplainable Artificial Intelligence
KW  - Wolf Motor Function Test
KW  - Rehabilitation
KW  - Random Forest
DO  - 10.1109/ICEENG64546.2025.11031277
JO  - 2025 15th International Conference on Electrical Engineering (ICEENG)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 15th International Conference on Electrical Engineering (ICEENG)
Y1  - 12-15 May 2025
AB  - Rehabilitation therapies play a crucial role in restoring upper motor functions among individuals with neurological impairments. One widely accepted assessment tool for evaluating motor functions in individuals with neurological disorders, such as stroke, is the Wolf Motor Function Test (WMFT). This study highlights the potential of eXplainable Artificial Intelligence (XAI) techniques in enhancing upper motor functions rehabilitation using the WMFT. By integrating explainability into AI-driven rehabilitation systems, not only does it improve clinical decision-making, but it also empowers patients in their recovery journey. This integration contributes to the advancement of personalized and effective rehabilitation strategies for individuals with neurological impairments. The proposed system is both simple and effective, as it combines real- time sensor data collected during WMFT exercises with Random Forests classification and the Anchor XAI algorithm. This integration allows for the provision of personalized and adaptive rehabilitation strategies. To evaluate the accuracy of the developed model, a 7-fold cross-validation was conducted, resulting in an impressive accuracy rate of 92.80%. Additionally, the precision of the XAI model (Anchor) ranged between 0.94 and 0.97, further demonstrating its reliability.
ER  - 

TY  - CONF
TI  - Enhancing Crop Disease Prediction: A Hybrid Approach Integrating Deep Learning, Ensemble Methods, and Explainable AI
T2  - 2025 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
SP  - 1
EP  - 6
AU  - Y. R. Suman
AU  - M. Varun
AU  - G. Sathiya Prakash
AU  - H. Nithin
AU  - N. R. Deepak
AU  - B. Omprakash
PY  - 2025
KW  - Visualization
KW  - Temperature distribution
KW  - Accuracy
KW  - Explainable AI
KW  - Crops
KW  - Food security
KW  - Robustness
KW  - Sensors
KW  - Convolutional neural networks
KW  - Diseases
DO  - 10.1109/ICKECS65700.2025.11035873
JO  - 2025 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
Y1  - 28-29 April 2025
AB  - This review examines how hybrid models, grouped learning, and explain/reason-able AI can enhance the sensing and detection of crop diseases, which is vital for increasing farm productivity and ensuring global food security. A novel approach combining ResNet50 and VGG16 provides high accuracy in detecting diseases in crops such as cashew, tomato, cassava, and maize. By employing grouping techniques with tools like CNN, ResNeXt, and InceptionV3, the system turned out remarkably more effective by combining their individual strengths. To build trust, the research also incorporates explainable AI methods like S.H.A.P and L.I.M.E, which help farmers and experts understand how the AI arrives at its predictions. Incorporating weather data, such as temperature and humidity, further enhances the system’s accuracy, as these factors play a vital role in how diseases spread. This approach not only delivers accurate results but also emphasizes the importance of transparency in AI tools. By combining advanced technology with clear explanations, this study showcases how AI can help manage crop diseases sustainably, supporting farmers to ensure a stable food supply for the future. This unique method has the potential to change farming practices, reduce crop losses, and contribute to global food security.
ER  - 

TY  - CONF
TI  - Explainable Machine Learning for Regime-Based Asset Allocation
T2  - 2020 IEEE International Conference on Big Data (Big Data)
SP  - 5480
EP  - 5485
AU  - R. Zhang
AU  - C. Yi
AU  - Y. Chen
PY  - 2020
KW  - Data visualization
KW  - Macroeconomics
KW  - Numerical models
KW  - Resource management
KW  - Stock markets
KW  - Optimization
KW  - Portfolios
KW  - Hierarchical Clustering
KW  - asset allocation
KW  - regime-switching
KW  - machine learning
KW  - explainable AI
DO  - 10.1109/BigData50022.2020.9378332
JO  - 2020 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Big Data (Big Data)
Y1  - 10-13 Dec. 2020
AB  - This paper explores an explainable AI model in the financial industry. Macroeconomic and market data serve as inputs of Hierarchical Clustering to distinguish among different economic regimes. Compared with traditional models such as Investment Clock, this method can adjust the classification standard in time according to recent market sentiment. The regime, therefore, can be interpreted by not only macro indicators but also investors' mood swings using Artificial Intelligence. When we compute the statistical characteristics of returns of each asset, we find that they can be well distinguished among regimes. This method can also identify the abnormally large wave of the stock market from 2015 to 2016 by separating it as an unusual regime, which cannot be realized by traditional methods. The clustering technique enables us to explain and understand the current market status and predict different assets' performances. Therefore, thanks to the superior interpretability of AI, the mean and variance of returns in each regime are estimated and viewed as viewpoints of the Black-Litterman asset allocation model to construct portfolios. To simulate the real situation, a dynamic backtesting method is used and asset weights change because of the rolling time windows. The results show that equipped with a simple timing strategy, the clustering technique can improve the results and yield excess returns. Some other machine learning techniques are also applied in an attempt to improve the model.
ER  - 

TY  - CONF
TI  - Artificial Intelligence in Credit Risk Management of Peer-to-Peer Lending Financial Technology: Systematic Literature Review
T2  - 2023 6th International Conference of Computer and Informatics Engineering (IC2IE)
SP  - 329
EP  - 334
AU  - I. D. C. Arifah
AU  - I. U. Nihaya
PY  - 2023
KW  - Systematics
KW  - Merging
KW  - Machine learning
KW  - Network analyzers
KW  - Peer-to-peer computing
KW  - Risk management
KW  - Informatics
KW  - credit risk
KW  - peer-to-peer lending
KW  - artificial intelligence
DO  - 10.1109/IC2IE60547.2023.10331487
JO  - 2023 6th International Conference of Computer and Informatics Engineering (IC2IE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 6th International Conference of Computer and Informatics Engineering (IC2IE)
Y1  - 14-15 Sept. 2023
AB  - Peer-to-peer (P2P) lenders face regulatory, compliance, application, and data security risks. A complete methodology that includes more than statistical and economic methods is needed to conduct credit assessments effectively. This study uses systematic literature network analysis and artificial intelligence to comprehend risk management in P2P lending financial technology. This study suggests that explainable AI (XAI) is better at identifying, analyzing, and evaluating financial industry risks, including financial technology. This is done through human agency, monitoring, transparency, and accountability. The LIME Framework and SHAP Value are widely used machine learning frameworks for data integration to speed up and improve credit score analysis using bank-like criteria. Thus, machine learning is expected to be used to develop a precise and rational individual credit evaluation system in peer-to-peer lending to improve credit risk supervision and forecasting while reducing default risk.
ER  - 

TY  - CHAP
TI  - Ensuring Ethical Use of AI in Project Management
T2  - Project Management with AI For Dummies
SP  - 197
EP  - 211
AU  - Daniel Stanton
PY  - 2025
KW  - Artificial intelligence
KW  - Ethics
KW  - Project management
KW  - Stakeholders
KW  - Privacy
KW  - Guidelines
KW  - Decision making
KW  - Monitoring
KW  - Security
KW  - Global Positioning System
DO  - 
PB  - Wiley
SN  - 9781394320868
UR  - http://ieeexplore.ieee.org/document/10964473
AB  - Summary <p>This chapter explores the key ethical considerations project managers must address when using AI, including understanding AI ethics, promoting fairness and transparency in AI&#x2010;driven decisions, and navigating ethical dilemmas in AI&#x2010;powered projects. AI has the power to significantly influence decision&#x2010;making processes within projects, from resource allocation to hiring decisions. Therefore, it's critical for project managers to ensure that AI is used ethically, with a strong emphasis on fairness, privacy, and accountability. Tools like Fiddler AI help project managers audit AI systems for fairness by continuously monitoring AI&#x2010;driven decisions, detecting biased patterns, and providing insights into how AI models reach their conclusions. One effective way to ensure fairness and transparency in AI&#x2010;driven decisions is to involve stakeholders in the decision&#x2010;making process. Transparency in AI ensures that the decision&#x2010;making process is clear and understandable to all stakeholders.</p>
ER  - 

TY  - CONF
TI  - AI Advances: Enhancing Banking Security with Fraud Detection
T2  - 2024 First International Conference on Technological Innovations and Advance Computing (TIACOMP)
SP  - 289
EP  - 294
AU  - F. T. Johora
AU  - R. Hasan
AU  - S. F. Farabi
AU  - M. Z. Alam
AU  - I. Sarkar
AU  - A. A. Mahmud
PY  - 2024
KW  - Deep learning
KW  - Ethics
KW  - Technological innovation
KW  - Banking
KW  - Speech recognition
KW  - Real-time systems
KW  - Fraud
KW  - Security
KW  - Stakeholders
KW  - Artificial intelligence
KW  - Fraud Detection
KW  - traditional fraud detection
KW  - Artificial Intelligence
KW  - Banking Security
KW  - Risk Management
DO  - 10.1109/TIACOMP64125.2024.00055
JO  - 2024 First International Conference on Technological Innovations and Advance Computing (TIACOMP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 First International Conference on Technological Innovations and Advance Computing (TIACOMP)
Y1  - 29-30 June 2024
AB  - In the contemporary financial realm, safeguarding against banking fraud and managing associated risks is paramount. In this pursuit, the integration of artificial intelligence (AI) stands as a beacon of promise, offering multifaceted solutions that outshine traditional fraud detection mechanisms. This study delves into the expansive applications of AI, delineating its role in identifying, pre-empting, and navigating fraudulent activities within the banking sector, juxtaposed against conventional fraud detection methodologies. AI revolutionizes banking fraud prevention and risk management by leveraging its rapid analysis capabilities to detect anomalies and flag fraudulent activities in real-time. Deep learning, particularly through neural networks trained on historical fraud data, excels in discerning intricate patterns and forecasting fraudulent transactions with remarkable accuracy. Natural Language Processing (NLP) enhances Know Your Customer (KYC) protocols, ensuring the authenticity of customers by scrutinizing textual data from diverse sources. Graph analytics visually map transactional relationships, spotlighting suspicious activities like rapid fund transfers indicative of money laundering. Predictive analytics transcends conventional credit scoring by integrating diverse datasets, offering holistic insights into customer creditworthiness. User-friendly interfaces like AI-powered chatbots facilitate immediate reporting of suspicious activities alongside advanced biometric authentication mechanisms such as facial and voice recognition. Adaptability inherent in AI ensures dynamic updates to combat evolving fraud strategies, extending beyond fraud detection to phishing, IoT integration, and cross-channel analysis. Additionally, AI's capability to simulate economic scenarios empowers proactive risk management and streamlines regulatory compliance processes, marking a transformative shift in banking security and efficiency.
ER  - 

TY  - CONF
TI  - A Lightweight CNN-SVM Explainable AI Approach for Classification and Visualization of Grape Leaf Disease
T2  - 2024 3rd International Conference on Advancement in Electrical and Electronic Engineering (ICAEEE)
SP  - 1
EP  - 5
AU  - H. I. Peyal
AU  - Z. M. Leion
AU  - M. N. Abdal
AU  - M. I. Islam
AU  - S. Miraz
AU  - M. R. Remon
AU  - M. M. R. Kontho
AU  - N. Tasnim
PY  - 2024
KW  - Productivity
KW  - Heating systems
KW  - Visualization
KW  - Plant diseases
KW  - Pathogens
KW  - Accuracy
KW  - Computational modeling
KW  - lightweight CNN-SVM architecture
KW  - VGG-16
KW  - VGG-19
KW  - Area Under Curve (AUC) score
KW  - Explainable AI
KW  - Gradient Weighted Class Activation Mapping (Grad-CAM)
KW  - heatmap
KW  - region
DO  - 10.1109/ICAEEE62219.2024.10561852
JO  - 2024 3rd International Conference on Advancement in Electrical and Electronic Engineering (ICAEEE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd International Conference on Advancement in Electrical and Electronic Engineering (ICAEEE)
Y1  - 25-27 April 2024
AB  - Grape is highly esteemed as a significant agricultural crop in Bangladesh. Plant diseases primarily result from the presence of pathogens and pest insects, leading to a significant decline in productivity if not properly addressed. The fundamental aim of this research is to employ a streamlined CNN-SVM architecture, utilizing deep learning techniques, to accurately categorize grape leaves into three distinct disease classes and one healthy class. The proposed model surpasses the accuracy of the previously trained transfer learning models VGG-16 and VGG-19 while having approximately $257\times$ to $267\times$ times fewer parameters (0.537 M). On average, the proposed model achieves a classification accuracy of 99.18%, which is significantly higher than the 93.42% and 91.94% achieved by the transfer learning models, respectively. With a precision, recall, and F1 score close to 99%, the suggested model provides excellent results. The model’s outstanding performance is further validated by its remarkable Area Under Curve (AUC) score of 99.98%. In addition to using less disc space (about 6 MB), the suggested model because of being lightweight shows a significant decrease in parameters. To visually display the disease identified by the proposed model, a transparent AI methodology has been utilized, specifically the Gradient Weighted Class Activation Mapping (Grad-CAM) technique. To better understand which area was responsible for the classification, a heatmap has been created.
ER  - 

TY  - JOUR
TI  - XAI-Powered Smart Agriculture Framework for Enhancing Food Productivity and Sustainability
T2  - IEEE Access
SP  - 168412
EP  - 168427
AU  - R. John Martin
AU  - R. Mittal
AU  - V. Malik
AU  - F. Jeribi
AU  - S. Tabrez Siddiqui
AU  - M. Alamgir Hossain
AU  - S. L. Swapna
PY  - 2024
KW  - Smart agriculture
KW  - Soil measurement
KW  - Farming
KW  - Feature extraction
KW  - Internet of Things
KW  - Data models
KW  - Accuracy
KW  - Crop yield
KW  - Computational modeling
KW  - Optimization
KW  - Sustainable development
KW  - Explainable AI
KW  - Machine learning
KW  - Smart agriculture
KW  - sustainable food production
KW  - precision farming
KW  - explainable AI
KW  - machine learning
DO  - 10.1109/ACCESS.2024.3492973
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - A vital component of maintaining the world’s expanding populace is farming. In agriculture field, factors such as soil quality, weather patterns, and crop yields are essential components of usual possessions that affect farming manufacture. Despite advancements, prevailing smart systems quiet struggle with handling big amounts in prediction claims, often facing difficulties in balancing prediction accuracy and learning efficiency. To ensure sustainable food production, integrating advanced machineries such as machine learning and artificial intelligence in agriculture is essential. This study proposes an explainable AI (XAI)-based smart agriculture system to provide holistic recommendation for precision farming aimed at improving productivity while reducing environmental impact. We compiled a comprehensive weather, soil, and crop dataset from official and verified sources in India. From this dataset, we extracted and optimized features using pre-trained architectures and enhanced barnacles mating optimization (EBMO) algorithm, addressing the high-measure mentality and computational complexity issues often encountered in agricultural data analysis. The selected features were analyzed to provide holistic recommendations for precision farming using baseline ML models such as support vector machine, random forest, neural network, and decision tree. Additionally, we integrate the XAI framework with an interpretable recommendation/s for optimizing the agricultural practices. The study developed an XAI-based smart agriculture system that provides holistic recommendations for precision farming to boost productivity. By using a comprehensive dataset and optimizing features with the EBMO algorithm, the research achieved high accuracy in crop yield predictions, particularly with the XLNet+SVM model, which outperformed existing models across various crops. The integration of SHapley Additive explanation (SHAP) and Local Interpretable Model-agnostic Explanation (LIME) further enabled interpretable AI-driven insights, enhancing transparency in decision-making. The results demonstrated significant improvements in prediction accuracy, resource management, and sustainability, offering valuable contributions to global food security through smart agriculture. The ability to explain these AI decisions further supports the adoption of AI technologies in agriculture, fostering resilient agricultural systems that are essential for feeding the world’s growing population.
ER  - 

