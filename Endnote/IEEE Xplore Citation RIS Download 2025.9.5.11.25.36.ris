TY  - JOUR
TI  - Predicting Hospital Stay Length Using Explainable Machine Learning
T2  - IEEE Access
SP  - 90571
EP  - 90585
AU  - B. S. Alsinglawi
AU  - F. Alnajjar
AU  - M. S. Alorjani
AU  - O. M. Al-Shari
AU  - M. N. Munoz
AU  - O. Mubin
PY  - 2024
KW  - Predictive models
KW  - Hospitals
KW  - Feature extraction
KW  - Machine learning
KW  - Prediction algorithms
KW  - Data models
KW  - Benchmark testing
KW  - Healthcare decision support systems
KW  - explainable artificial intelligence
KW  - machine learning
KW  - XGBOOST
DO  - 10.1109/ACCESS.2024.3421295
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Efficient bed management minimizes hospital costs and improves efficiency and patient outcomes. This study presents a predictive hospital-ICU length of stay (LOS) framework at admission, where it leverages hospital EHR. Our work utilizes supervised machine learning classification models to predict ICU patients’ LOS in hospital clinical information systems (CIS). Our research marks the first known instance of utilizing explainable artificial intelligence (xAI) for the purpose of explainable machine learning applied to real data collected from hospital stays. We evaluated the predictive classification models using a range of performance metrics (Accuracy, AUC, Sensitivity, Specificity, F1- score, Precision, Recall and more) to predict short and long ICU lengths of stay upon hospital admission. XGBoost predicted short and long LOS with a 98 % AUC. This study shows how hospitals and ICUs might use machine learning to forecast patients on admission. Our study extends clinical information systems for hospitals to provide robust and trustworthy LOS, predictive models by using xAI to explain predictive model outputs.
ER  - 

TY  - CONF
TI  - Optimizing DevOps Methodologies with the Integration of Artificial Intelligence
T2  - 2024 3rd International Conference for Innovation in Technology (INOCON)
SP  - 1
EP  - 5
AU  - M. S. Ali
AU  - D. Puri
PY  - 2024
KW  - Ethics
KW  - Technological innovation
KW  - DevOps
KW  - Reviews
KW  - Decision making
KW  - Organizations
KW  - Software
KW  - DevOps
KW  - Artificial Intelligence
KW  - Automation
KW  - AI-driven DevOps
KW  - CI/CD Pipelines
KW  - Infrastructure Provisioning
DO  - 10.1109/INOCON60754.2024.10511490
JO  - 2024 3rd International Conference for Innovation in Technology (INOCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd International Conference for Innovation in Technology (INOCON)
Y1  - 1-3 March 2024
AB  - This research paper aims to explore the integration of artificial intelligence (AI) techniques in DevOps practices to enhance efficiency, automation, and decision-making capabilities. The paper provides a comprehensive review of existing literature on the subject and presents a case study illustrating the practical application of AI in a real-world DevOps environment. By analyzing the benefits, challenges, and potential ethical considerations, this study seeks to provide insights into how AI can be leveraged to optimize DevOps workflows. The paper provides a comprehensive review of existing literature on the subject and presents a case study illustrating the practical application of AI in a real-world DevOps environment. By analyzing the benefits, challenges, and potential ethical considerations, this study seeks to provide insights into how AI can be leveraged to optimize DevOps workflows.
ER  - 

TY  - JOUR
TI  - Comparative Analysis of Automated Machine Learning for Hyperparameter Optimization and Explainable Artificial Intelligence Models
T2  - IEEE Access
SP  - 84966
EP  - 84991
AU  - M. S. Khan
AU  - T. Peng
AU  - H. Akhlaq
AU  - M. Adeel Khan
PY  - 2025
KW  - Automated machine learning
KW  - Predictive models
KW  - Computational modeling
KW  - Structural beams
KW  - Analytical models
KW  - Data models
KW  - Hyperparameter optimization
KW  - Accuracy
KW  - Machine learning
KW  - Training
KW  - Automated machine learning
KW  - comparative analysis
KW  - hyperparameter optimization
KW  - moment capacity
KW  - optuna
KW  - random forest
KW  - SHAP
DO  - 10.1109/ACCESS.2025.3566427
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 13
VL  - 13
JA  - IEEE Access
Y1  - 2025
AB  - Artificial intelligence (AI) has been increasingly applied to solve complex real-world problems. One of the most significant challenges in AI lies in selecting and fine-tuning the optimal algorithm for a given task. Automated Machine Learning (AutoML) models have emerged as a promising solution to address this challenge by systematically exploring hyperparameter spaces to identify optimal configurations efficiently. This study addresses critical gaps in the current literature by conducting a comprehensive comparative analysis of AutoML frameworks for hyperparameter optimization and evaluating the effectiveness of various explainability techniques for enhancing model interpretability. For this purpose, Random forest (RF) is selected as the base model and integrated with nine different AutoML frameworks, namely Random search (RS), Grid search (GS), Hyperopt, TPOT, Optuna, GP Minimize, Forest Minimize, GBRT Minimize, and Dummy Minimize. The study focuses on predicting the ultimate moment capacity of Ultra-High-Performance Concrete (UHPC) beams and U-shaped girders. Furthermore, the insights from SHapley Additive exPlanations (SHAP) are also compared with those derived from alternative explainability methods, including Local interpretable model-agnostic explanations (LIME), partial dependence plots (PDP), and sklearn permutation importance rankings to examine the contributions of individual parameters to the ultimate moment capacity predictions of UHPC beams using the best-performing AutoML model. The findings demonstrate that Optuna consistently outperforms its counterparts, achieving the highest predictive accuracy and the lowest computational training time. The findings also highlight SHAP’s superiority in offering detailed, consistent, and actionable insights, making it the preferred method for both global feature importance and individual feature analysis in high-stakes engineering applications.
ER  - 

TY  - CHAP
TI  - Explaining Artificial Intelligence, Machine Learning, and Deep Learning Models
T2  - Risk Modeling: Practical Applications of Artificial Intelligence, Machine Learning, and Deep Learning
SP  - 55
EP  - 70
AU  - Terisa Roberts
AU  - Stephen J. Tonna
PY  - 2022
KW  - Biological system modeling
KW  - Machine learning
KW  - Predictive models
KW  - Machine learning algorithms
KW  - Brain modeling
KW  - Risk management
KW  - Data models
KW  - Adaptation models
KW  - Stakeholders
KW  - Logic
DO  - 10.1002/9781119824961.ch4
PB  - Wiley
SN  - 9781119824954
UR  - http://ieeexplore.ieee.org/document/10953643
AB  - Summary <p>The lack of transparency is attributed to the model development process that is defined and specified by a machine or data&#x2010;driven rather than a human, comparing, a deep learning neural network to an econometric or other statistical model. This chapter discusses selected methods that are applied to optimize machine learning algorithms. Artificial Intelligence (AI) and machine learning algorithms are said to utilize fewer statistical assumptions but optimize hyperparameters based on an objective function. In general, models, analytical algorithms, and quantitative computations used in risk management are subject to strict regulatory guidance, like the independent assessment of the conceptual soundness of the analytical approach. The chapter emphasizes the importance of standardized and consistent data management processes to improve trust, transparency and governance of AI and machine learning. It explores a few approaches and methods that are popular in the risk management domain.</p>
ER  - 

TY  - CONF
TI  - Transparency in Detecting Defects of a Printed Circuit Board: Harnessing XAI for Improved Quality Control in Electronic Manufacturing Industries
T2  - 2024 IEEE 10th International Conference on Smart Instrumentation, Measurement and Applications (ICSIMA)
SP  - 287
EP  - 292
AU  - A. M. M. Ali
AU  - S. Sahlan
AU  - N. Khamis
AU  - X. Ziyi
AU  - F. ‘. N. Rashid
PY  - 2024
KW  - Manufacturing industries
KW  - Deep learning
KW  - Accuracy
KW  - Explainable AI
KW  - Printed circuits
KW  - Process control
KW  - Quality control
KW  - PCB defect detection
KW  - Deep learning
KW  - CNN
KW  - explainable artificial intelligence
KW  - LIME
KW  - Grad-CAM
DO  - 10.1109/ICSIMA62563.2024.10675544
JO  - 2024 IEEE 10th International Conference on Smart Instrumentation, Measurement and Applications (ICSIMA)
IS  - 
SN  - 2640-6535
VO  - 
VL  - 
JA  - 2024 IEEE 10th International Conference on Smart Instrumentation, Measurement and Applications (ICSIMA)
Y1  - 30-31 July 2024
AB  - In electronic manufacturing industries, defects emerging from Printed Circuit Boards (PCBs) may cause severe malfunctions of the final product. This paper aims to address this issue by proposing the utilization of artificial intelligence (AI) techniques to detect common defects in PCB through the implementation of deep learning (DL) models. The paper utilizes Convolutional Neural Networks (CNNs) and their derived models, in particular, region-based convolutional neural network (R-CNN), Fast R-CNN, and Faster R-CNN. Analysis of the utilization of these techniques in detecting common defects of the PCB, which are classified as missing hole, open circuit, short, spur and spurious circuit are performed in Google Colab platform using Python, with data retrieved from Kaggle. From the results obtained, the best performance parameters for color input for the defect classes are 95.3%, 83.2%, 89.7%, 85.6% and 82.7% respectively. To better understand the results obtained, explainable AI (XAI) techniques, LIME and Grad-CAM were utilized. The implemented technique successfully shows the decision logic for the defect classes through the interpretation images obtained.
ER  - 

TY  - CONF
TI  - Explainable AI with Capsule Networks for Credit Risk Assessment in Financial Systems
T2  - 2025 International Conference on Next Generation Information System Engineering (NGISE)
SP  - 1
EP  - 6
AU  - J. S. Kadyan
AU  - M. Sharma
AU  - S. Kadyan
AU  - S. Gupta
AU  - N. K. Hamid
AU  - B. Kiran Bala
PY  - 2025
KW  - Accuracy
KW  - Explainable AI
KW  - Transforms
KW  - Vectors
KW  - Spatial databases
KW  - Regulation
KW  - Risk management
KW  - Stakeholders
KW  - Reliability
KW  - Random forests
KW  - Explainable AI
KW  - Capsule Networks
KW  - Credit Risk Assessment
KW  - SHAP
KW  - Financial Systems
KW  - Market Research Analysis
DO  - 10.1109/NGISE64126.2025.11085369
JO  - 2025 International Conference on Next Generation Information System Engineering (NGISE)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2025 International Conference on Next Generation Information System Engineering (NGISE)
Y1  - 28-29 March 2025
AB  - This study introduces an Explainable AI (XAI) framework leveraging Capsule Networks (CapsNet) for credit risk assessment in financial systems. Traditional credit risk models often prioritize predictive accuracy over interpretability, creating challenges in stakeholder trust and regulatory compliance. CapsNet addresses this by maintaining spatial relationships in data while offering robust predictive capabilities. The proposed framework integrates data preprocessing techniques such as handling missing values and balancing imbalanced datasets, ensuring reliable inputs for CapsNet. Explainability is enhanced through Capsule Activation Vectors and attention mechanisms, elucidating the influence of critical features like income, loan amount, and credit history on predictions. The study employs SHAP values to identify key determinants of loan default, with income emerging as the most significant predictor. Experimental results validate CapsNet's superior performance, achieving 90% accuracy and F1-score, outperforming models like Logistic Regression and Random Forest. Additionally, the framework provides interpretative insights into model decisions, fostering transparency and trust. This work represents a significant step towards integrating explainable AI into credit risk management, ensuring compliance with financial regulations while improving operational efficiency and decision-making in the banking sector. By addressing the dual challenges of accuracy and interpretability, this research demonstrates how XAI frameworks can transform financial systems, supporting ethical and knowledge-based practices.
ER  - 

TY  - CONF
TI  - Enhancing Cybersecurity in Digital Twin Systems: Mitigating Challenges and Defending Against Threats
T2  - 2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
SP  - 1
EP  - 6
AU  - H. Muniyappan
AU  - R. M
AU  - S. Pavithra
PY  - 2025
KW  - Smart agriculture
KW  - Adaptation models
KW  - Explainable AI
KW  - Standards organizations
KW  - Organizations
KW  - Robustness
KW  - Real-time systems
KW  - Digital twins
KW  - Computer security
KW  - Anomaly detection
KW  - Digital Twins
KW  - Artificial Intelligence
KW  - Explainable AI
KW  - Cybersecurity
KW  - Anomaly Detection
DO  - 10.1109/ICDSAAI65575.2025.11011764
JO  - 2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
Y1  - 28-29 March 2025
AB  - Digital Twins (DTs) are precise copies of actual systems that exist to which people can interact dynamically. Employed mainly in manufacturing, healthcare and smart city sectors, DTs facilitate the improvement of performance and decision-making. This integration raises the risk of cybersecurity threats including loss of data; wrong access; and advanced persistent threats. Since through integrating DT many other infrastructures are attained by the attackers it becomes a very significant agenda to counter them. In this paper, the strategy of using AI and XAI to enhance cybersecurity in DT systems is analyzed. Some of the AI techniques are Anomaly detection, analysis that predicts and Automated response facilities to provide Security precautionary measures. Such methods are improved by XAI through the addition of the process by which the decisions were made and the trust with the parties involved. Mirroring literature review, deficits of existing DT cybersecurity frameworks, analyzed, include overall issues of scalability, insufficient new threat integration, and increased opacity. For such reasons, the current paper offers a detailed cybersecurity framework that adopts both AI and XAI to fill the existing research gaps. The framework has demonstrated the application of improvements in threat detection, downloads response time and system robustness using real life cases. The presented work is intended for those states where it is possible to create safe and secure DT environments, efficient and capable of countering modern cybersecurity threats. They are aimed to cover existing voids in order to create a socially responsible approach to the use of DT in sensitive domains.
ER  - 

TY  - JOUR
TI  - Improving Plant Disease Classification With Deep-Learning-Based Prediction Model Using Explainable Artificial Intelligence
T2  - IEEE Access
SP  - 100005
EP  - 100014
AU  - N. Nigar
AU  - H. Muhammad Faisal
AU  - M. Umer
AU  - O. Oki
AU  - J. Manappattukunnel Lukose
PY  - 2024
KW  - Accuracy
KW  - Plant diseases
KW  - Feature extraction
KW  - Solid modeling
KW  - Deep learning
KW  - Computational modeling
KW  - Predictive models
KW  - Explainable AI
KW  - Accuracy
KW  - Plant diseases
KW  - Feature extraction
KW  - Solid modeling
KW  - Deep learning
KW  - Computational modeling
KW  - Predictive models
KW  - Detection algorithms
KW  - Plant disease detection
KW  - deep learning
KW  - explainable artificial intelligence
KW  - prediction model
DO  - 10.1109/ACCESS.2024.3428553
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Plant diseases can have profound effects on the economy, impacting both local and global scales. These diseases can lead to substantial losses in agricultural productivity, affecting crop yields and quality. In this context, deep learning algorithms are widely acknowledged as effective solutions. However, the use of these black-box approaches raises concerns about trust in interpreting and validating the decisions generated by the models. This study proposes an explainable artificial intelligence (XAI) based plant disease classification system to classify and identify distinct ailments with improved accuracy. The system correctly identifies 38 different plant diseases with accuracy, precision, and recall as 99.69%, 98.27%, and 98.26%, respectively. These predictions are subjected to additional analysis employing the local interpretable model-agnostic explanations (LIME) framework to produce visual explanations aligning with prior beliefs and adhering to established best practices in explanations. This system will serve as a promising avenue for revolutionizing disease detection, fostering informed decision-making, and ultimately contributing to global food security.
ER  - 

TY  - CONF
TI  - Harnessing Explainable Artificial Intelligence to Model the Causes of Unplanned Power Outages in Renewable Integrated Electrical Grids
T2  - 2025 IEEE 34th International Symposium on Industrial Electronics (ISIE)
SP  - 1
EP  - 7
AU  - S. Twala
AU  - R. Naidoo
PY  - 2025
KW  - Renewable energy sources
KW  - Thermal expansion
KW  - Load shedding
KW  - Optimal scheduling
KW  - Predictive models
KW  - Power system reliability
KW  - Predictive analytics
KW  - Thermal energy
KW  - Power generation
KW  - Load modeling
KW  - Explainable artificial intelligence (XAI)
KW  - load shedding
KW  - unplanned grid power outages
KW  - unserved energy
KW  - renewable integration
DO  - 10.1109/ISIE62713.2025.11124607
JO  - 2025 IEEE 34th International Symposium on Industrial Electronics (ISIE)
IS  - 
SN  - 2163-5145
VO  - 
VL  - 
JA  - 2025 IEEE 34th International Symposium on Industrial Electronics (ISIE)
Y1  - 20-23 June 2025
AB  - Several developing countries not only suffer from electricity shortages due to low electrification rates, but also experience frequent power cuts and rolling black-outs, such as load shedding. These chronic grid-scale power outages are unplanned, and their causes are unknown, and have not been studied in detail in the current literature. Furthermore, their impact on economic growth and quality of life is well known to be detrimental. This study presents an innovative, explainable artificial intelligence grid outage model (XAI-GOM) that integrates Exponential Gaussian Process Regression with SHAP-based explanations to predict unplanned grid-scale load shedding outages. It provides data-driven and explanation-based policy recommendations on how electrical utilities can mitigate future power outages in the grid. When applied to a case study of a national grid, XAI-GOM's near-diagonal distribution reveals its strength in capturing complex nonlinear relationships by learning from 31 features, such as renewable generation and import/export flows. The results demonstrate that unplanned outages in coal-based power plants, combined with insufficient nuclear generation, remain the most significant contributors to elevated load shedding power outages. Based on the XAI-GOM, we can recommend enhanced asset management with predictive analytics to detect early signs of deterioration in thermal generation, preventing breakdowns that cause power outages. We can also make an XAI-GOM-informed recommendation to policymakers to expand solar, wind, and concentrated solar power with strong grid integration to mitigate variability and protect against unplanned thermal outages.
ER  - 

TY  - CONF
TI  - Predictive Machine Learning Modelling of Tropsopheric Ozone Using Geospatial and Atmospheric Data
T2  - 2024 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)
SP  - 1
EP  - 4
AU  - A. Tadepalli
AU  - H. P. Allam
AU  - S. Upadhyaya
PY  - 2024
KW  - Radio frequency
KW  - Gases
KW  - Correlation
KW  - Explainable AI
KW  - Atmospheric modeling
KW  - Biological system modeling
KW  - Artificial neural networks
KW  - Predictive models
KW  - Data models
KW  - Population density
KW  - Ozone prediction
KW  - Machine Learning
KW  - Explainable AI
DO  - 10.1109/InGARSS61818.2024.10983976
JO  - 2024 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)
Y1  - 2-5 Dec. 2024
AB  - Efficient prediction of tropospheric ozone $\left({O}_{3}\right)$ is crucial to monitor its harmful effects on the ecosystem and thereby take necessary actions. Owing to the complex formation mechanisms and interrelated meteorological parameters, accurate prediction of $O_{3}$ using physics-based model is highly complex. Thus, machine learning techniques are currently being exploited to understand the underlying correlations and develop a good prediction model. This study thus focuses on predicting the long-term ground level ozone using a multitude of geographical features and finally analyzing the contributing factors in the prediction of ozone. To achieve this, various Exploratory Data Analysis (EDA) techniques are performed to visualize the data, followed by data modelling using linear regression, random forest (RF), and artificial neural network models. Finally, different Explainable AI (XAI) techniques are employed to identify the most contributing features in the ozone prediction. The results show that RF has a better performance and that latitude, altitude, NO2 emissions, population density, and nightlight are the significant features.
ER  - 

TY  - CONF
TI  - Explainable Multi-Stage Churn Prediction using Graph Neural Network in Telecom Sector
T2  - 2025 8th International Conference on Trends in Electronics and Informatics (ICOEI)
SP  - 1100
EP  - 1105
AU  - N. M
AU  - E. Daniel
AU  - S. Durga
AU  - S. Seetha
PY  - 2025
KW  - Training
KW  - Support vector machines
KW  - Consumer behavior
KW  - Explainable AI
KW  - Computational modeling
KW  - Predictive models
KW  - Prediction algorithms
KW  - Graph neural networks
KW  - Telecommunications
KW  - Churn
KW  - Explainable AI
KW  - SHAP
KW  - LIME
KW  - Multi-stage prediction
KW  - Graph based model
KW  - Churn analysis
DO  - 10.1109/ICOEI65986.2025.11013438
JO  - 2025 8th International Conference on Trends in Electronics and Informatics (ICOEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 8th International Conference on Trends in Electronics and Informatics (ICOEI)
Y1  - 24-26 April 2025
AB  - Customer churn prediction is a key task for telecom companies, as retaining customer is more effective than getting new customers. Traditional churn prediction model often lacks the ability to provide insights, making it hard to understand the factors leading for customer churn. This project proposes an explainable multi-stage churn prediction model using Graph Neural Networks (GNNs) to enhance both accuracy and efficiency of predictions made in the telecom industry. This approach models customer interactions as graph, capturing complex relations and dependencies between customers. Using techniques like LIME and SHAP, the model provides clear, actionable explanations for decision making. Experimental results demonstrate GNN based model maintained high level of explainability, making it an effective tool for telecom companies to manage churn and improve customer retain strategies.
ER  - 

TY  - CONF
TI  - Explainable Machine Learning to Improve Assembly Line Automation
T2  - 2021 4th International Conference on Artificial Intelligence for Industries (AI4I)
SP  - 81
EP  - 85
AU  - S. S. Sheuly
AU  - M. U. Ahmed
AU  - S. Begum
AU  - M. Osbakk
PY  - 2021
KW  - Manufacturing industries
KW  - Support vector machines
KW  - Automation
KW  - Manufacturing processes
KW  - Manufactured products
KW  - Digital twin
KW  - Manuals
KW  - Explainable Machine Learning
KW  - Power transfer unit
KW  - Hybrid approach
DO  - 10.1109/AI4I51902.2021.00028
JO  - 2021 4th International Conference on Artificial Intelligence for Industries (AI4I)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 4th International Conference on Artificial Intelligence for Industries (AI4I)
Y1  - 20-22 Sept. 2021
AB  - Faulty manufactured product causes huge economic loss in the manufacturing industry. A local company produces a power transfer unit (PTU) for the vehicle industry and in this production 3% of PTU are rejected due to a mismatch of shim (a small mechanical part supporting PTU). Today the dimension of a shim is predicted manually by human experts. However, there are several problems due to the manual prediction of shim dimension, automatic central control from the cloud cannot be done. Additionally, it increases rejection rates and as a consequence decreases the reliability of the systems. To solve these problems, in this study shim prediction is implemented in the manufacturing of PTU with explainable Machine Learning (ML) which automates the manual shim selection process in the assembly line and explains the ML prediction. A hybrid approach that combines support vector regression (SVR) and k nearest neighbours (kNN) for the first part of the assembly line and Partial Least Squares (PLS) and kNN for the second part of the assembly line is used for shim prediction. The hybrid approach is selected due to better performance compared to the single ML model approach. Then, the most important features of the hybrid approach were identified with SHAP (SHapley Additive exPlanations). The result indicates due to this improved automation faulty PTU rate decreased from 3% to only 1%. Additionally, it enabled control from the cloud and increased reliability. From the explanation of the hybrid approach, it is evident that one of the features values has more impact on the prediction output and controlling this feature will reduce the rejection rate.
ER  - 

TY  - CONF
TI  - Integrating XAI in Metaverse for Operator 5.0: An Analytical Review
T2  - 2024 21st International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)
SP  - 1
EP  - 6
AU  - H. B. Santoso
AU  - D. K. Baroroh
AU  - N. T. Hong Van
PY  - 2024
KW  - Metaverse
KW  - Reviews
KW  - Decision making
KW  - Collaboration
KW  - Computer architecture
KW  - Telecommunications
KW  - Fourth Industrial Revolution
KW  - Metaverse
KW  - Operator 5.0
KW  - Explainable Artificial Intelligence
KW  - Industry 5.0
KW  - human-machine collaboration
DO  - 10.1109/ECTI-CON60892.2024.10594883
JO  - 2024 21st International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)
IS  - 
SN  - 2837-6471
VO  - 
VL  - 
JA  - 2024 21st International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)
Y1  - 27-30 May 2024
AB  - The concept development of Industry 4.0 to Industry 5.0 has shifted the existing technology integration in manufacturing. Technology should be able to support the creation of sustainable, human-centered, and resilient manufacturing operations. Answering this call, operator 5.0 has been conceptualized for development involving current technology such as Metaverse and XAI. However, extant research omits how XAI should be able to be integrated to achieve a human-centric operator. Through an analytical review of extant literature, this paper tries to conceptualize the integration of XAI into a metaverse environment to support the human-technology collaboration for Operator 5.0. Our study attempts to propose a conceptual approach to enhance the human-cyber physical system using XAI. XAI can improve the algorithm rationalization that can be a valuable input to operators.
ER  - 

TY  - JOUR
TI  - An Improved and Explainable Electricity Price Forecasting Model via SHAP-Based Error Compensation Approach
T2  - IEEE Transactions on Artificial Intelligence
SP  - 159
EP  - 168
AU  - L. Heistrene
AU  - J. Belikov
AU  - D. Baimel
AU  - L. Katzir
AU  - R. Machlev
AU  - K. Levy
AU  - S. Mannor
AU  - Y. Levron
PY  - 2025
KW  - Predictive models
KW  - Mathematical models
KW  - Explainable AI
KW  - Accuracy
KW  - Forecasting
KW  - Electricity
KW  - Artificial intelligence
KW  - Convolutional neural network (CNN)
KW  - electricity price forecasting
KW  - explainable artificial intelligence
KW  - long short-term memory (LSTM)
KW  - price spike
KW  - Shapley additive explanations (SHAP)
KW  - explainable artificial intelligence (XAI)
KW  - XGBoost
DO  - 10.1109/TAI.2024.3455313
JO  - IEEE Transactions on Artificial Intelligence
IS  - 1
SN  - 2691-4581
VO  - 6
VL  - 6
JA  - IEEE Transactions on Artificial Intelligence
Y1  - Jan. 2025
AB  - Forecasting errors in power markets, even as small as 1%, can have significant financial implications. However, even high-performance artificial intelligence (AI) based electricity price forecasting (EPF) models have instances when their prediction error is much higher than those shown by mean performance metrics. To date, explainable AI has been used to enhance the model transparency and trustworthiness of AI-based EPF models. However, this article demonstrates that insights from explainable AI (XAI) techniques can be expanded beyond its primary task of explanatory visualizations. This work presents a XAI-based error compensation approach to improve model performance and identify irregular predictions. The first phase of the proposed approach involves error quantification through a Shapley additive explanations (SHAP) based corrector model that fine-tunes the base predictor's forecasts. Using this corrector model's SHAP explanations, the proposed approach distinguishes high-accuracy predictions from lower ones in the second stage. Additionally, these explanations are more simplified than the base model, making them easier for nonexpert users such as bidding agents. Performance enhancement and insightful user-centric explanations are crucial for real-world scenarios such as price spikes during network congestion, high renewable penetration, and fluctuating fuel costs. Case studies discussed here show the efficacy of the proposed approach independent of model architecture, feature combination, or behavioral patterns of electricity prices in different markets.
ER  - 

TY  - JOUR
TI  - Automatic Generation of Insights From Workers’ Actions in Industrial Workflows With Explainable Machine Learning: A Proposed Architecture With Validation
T2  - IEEE Industrial Electronics Magazine
SP  - 17
EP  - 29
AU  - F. de Arriba-Pérez
AU  - S. García-Méndez
AU  - J. Otero-Mosquera
AU  - F. J. González-Castaño
AU  - F. Gil-Castiñeira
PY  - 2024
KW  - Task analysis
KW  - Productivity
KW  - Industries
KW  - Machinery
KW  - Wearable sensors
KW  - Machine learning
KW  - Workstations
KW  - Quality assessment
KW  - Next generation networking
KW  - Manufacturing processes
KW  - Natural languages
KW  - Trust management
KW  - Explainable AI
DO  - 10.1109/MIE.2023.3284203
JO  - IEEE Industrial Electronics Magazine
IS  - 2
SN  - 1941-0115
VO  - 18
VL  - 18
JA  - IEEE Industrial Electronics Magazine
Y1  - June 2024
AB  - New technologies, such as machine learning (ML), have provided great potential for evaluating industry workflows and automatically generating key performance indicators (KPIs). However, despite established standards for measuring the efficiency of industrial machinery, there is no precise equivalent for workers’ productivity, which would be highly desirable given the lack of a skilled workforce for the next generation of industry workflows. Therefore, an ML solution combining data from manufacturing processes and workers’ performance for that goal is required. Additionally, in recent times intense effort has been devoted to explainable ML approaches that can automatically explain their decisions to a human operator, thus increasing their trustworthiness. We propose to apply explainable ML solutions to differentiate between expert and inexpert workers in industrial workflows, which we validate at a quality assessment industrial workstation. Regarding the methodology used, input data are captured by a manufacturing machine and stored in a NoSQL database. Data are processed to engineer features used in automatic classification and to compute workers’ KPIs to predict their level of expertise (with all classification metrics exceeding 90%). These KPIs and the relevant features in the decisions are textually explained by natural language expansion on an explainability dashboard. These automatic explanations made it possible to infer knowledge from expert workers for inexpert workers. The latter illustrates the interest of research in self-explainable ML for automatically generating insights to improve productivity in industrial workflows.
ER  - 

TY  - JOUR
TI  - Explainable AI for 6G Use Cases: Technical Aspects and Research Challenges
T2  - IEEE Open Journal of the Communications Society
SP  - 2490
EP  - 2540
AU  - S. Wang
AU  - M. A. Qureshi
AU  - L. Miralles-Pechuán
AU  - T. Huynh-The
AU  - T. R. Gadekallu
AU  - M. Liyanage
PY  - 2024
KW  - 6G mobile communication
KW  - Artificial intelligence
KW  - 5G mobile communication
KW  - Explainable AI
KW  - Resource management
KW  - Security
KW  - Closed box
KW  - B5G
KW  - 6G
KW  - AI
KW  - XAI
KW  - explainability
DO  - 10.1109/OJCOMS.2024.3386872
JO  - IEEE Open Journal of the Communications Society
IS  - 
SN  - 2644-125X
VO  - 5
VL  - 5
JA  - IEEE Open Journal of the Communications Society
Y1  - 2024
AB  - Around 2020, 5G began its commercialization journey, and discussions about the next-generation networks (such as 6G) emerged. Researchers predict that 6G networks will have higher bandwidth, coverage, reliability, energy efficiency, and lower latency, and will be an integrated “human-centric” network system powered by artificial intelligence (AI). This 6G network will lead to many real-time automated decisions, ranging from network resource allocation to collision avoidance for self-driving cars. However, there is a risk of losing control over decision-making due to the high-speed, data-intensive AI decision-making that may go beyond designers’ and users’ comprehension. To mitigate this risk, explainable AI (XAI) methods can be used to enhance the transparency of the black-box AI decision-making process. This paper surveys the application of XAI towards the upcoming 6G age, including 6G technologies (such as intelligent radio and zero-touch network management) and 6G use cases (such as industry 5.0). Additionally, the paper summarizes the lessons learned from recent attempts and outlines important research challenges in applying XAI for 6G use cases soon.
ER  - 

TY  - CONF
TI  - Critical Infrastructure Security Through Digital Twin Synergy and Explainable Anomaly Detection
T2  - 2025 13th International Conference on Smart Grid (icSmartGrid)
SP  - 641
EP  - 649
AU  - S. Sagiroglu
AU  - H. İ. Bülbül
AU  - E. Bekiroglu
AU  - E. Irmak
AU  - I. Erkek
PY  - 2025
KW  - Adaptation models
KW  - Explainable AI
KW  - Reviews
KW  - Federated learning
KW  - Digital twins
KW  - Critical infrastructure
KW  - Smart grids
KW  - Anomaly detection
KW  - Unsupervised learning
KW  - Resilience
KW  - cyber security
KW  - digital twin
KW  - explainable ai
DO  - 10.1109/icSmartGrid66138.2025.11071824
JO  - 2025 13th International Conference on Smart Grid (icSmartGrid)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 13th International Conference on Smart Grid (icSmartGrid)
Y1  - 27-29 May 2025
AB  - Focusing on anomaly detection in industrial control systems and critical infrastructures, this study discusses the potential integration of digital twin (DT) and explainable artificial intelligence (XAI) technologies. While digital twins create virtual copies of physical systems, allowing monitoring and simulating system behaviors; XAI makes the decision processes of anomaly detection systems transparent, making operator interventions more effective. In the literature, it is stated that digital twins are generally used in technically focused applications, but there are deficiencies in cybersecurity and transparency. In addition, hybrid approaches developed by combining supervised and unsupervised learning methods provide high accuracy rates in anomaly detection. This study focuses on how XAI methods are used to increase security, especially in critical infrastructures, and how these technologies can be integrated with cyber resilience in the future. In addition, the diversity of data sets and models used emphasizes the importance of different methodological approaches for each application area. Finally, it is stated that this study aims to provide a resource that will guide new research areas and technological integrations in anomaly detection and security strategies.
ER  - 

TY  - CONF
TI  - Agile Project Status Prediction Using Interpretable Machine Learning
T2  - 2024 IEEE 12th International Conference on Intelligent Systems (IS)
SP  - 1
EP  - 8
AU  - A. A. ForouzeshNejad
AU  - F. Arabikhan
AU  - N. Williams
AU  - A. Gegov
AU  - O. Sari
AU  - M. Bader
PY  - 2024
KW  - Industries
KW  - Support vector machines
KW  - Measurement
KW  - Accuracy
KW  - Agile project management
KW  - Predictive models
KW  - Nearest neighbor methods
KW  - Prediction algorithms
KW  - Information technology
KW  - Forecasting
KW  - Interpretable Machine Learning
KW  - Decision Tree
KW  - Agile Project Management
KW  - Project Status Prediction
DO  - 10.1109/IS61756.2024.10705197
JO  - 2024 IEEE 12th International Conference on Intelligent Systems (IS)
IS  - 
SN  - 2767-9802
VO  - 
VL  - 
JA  - 2024 IEEE 12th International Conference on Intelligent Systems (IS)
Y1  - 29-31 Aug. 2024
AB  - Monitoring and forecasting the progress of information technology projects stands as a significant challenge in project management. Over the past two decades, agile project management has become a crucial factor influencing project success. Despite this, existing research has not presented a comprehensive model capable of predicting project outcomes based on agile features. In light of this, this study aims to develop a predictive model for information technology project outcomes using agility metrics. The results indicate that metrics related to teamwork and the team's capabilities, along with their collective experience, have the most significant impact on project success. The study employs the Decision Tree as an interpretable model to establish rules and predict project success. The accuracy of the model designed in this study is an impressive 97%, surpassing the accuracy of SVM at 71 % and KNN at 82%.
ER  - 

TY  - CONF
TI  - Harnessing Artificial Intelligence for Supply Chain Optimization: Enhanced Demand Prediction and Cost Reduction
T2  - 2023 2nd International Engineering Conference on Electrical, Energy, and Artificial Intelligence (EICEEAI)
SP  - 1
EP  - 6
AU  - S. M. Aljazzar
PY  - 2023
KW  - Procurement
KW  - Ethics
KW  - Costs
KW  - Explainable AI
KW  - Supply chains
KW  - Transportation
KW  - Demand forecasting
KW  - Artificial intelligence (AI)
KW  - Supply chain optimization
KW  - Demand forecasting
KW  - Risk management
KW  - Ethical considerations
DO  - 10.1109/EICEEAI60672.2023.10590108
JO  - 2023 2nd International Engineering Conference on Electrical, Energy, and Artificial Intelligence (EICEEAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 2nd International Engineering Conference on Electrical, Energy, and Artificial Intelligence (EICEEAI)
Y1  - 27-28 Dec. 2023
AB  - This article investigates the significant effect of artificial intelligence (AI) on the supply chain, stressing its capacity to fundamentally change various aspects of operations. The research seeks to offer an extensive review of artificial intelligence (AI) applications in the supply chain. This includes analyzing how AI may be used for demand forecasting, optimizing transportation, managing warehouses, optimizing procurement, and mitigating risks. The article showcases the concrete advantages of AI-driven solutions through the analysis of real-life scenarios. These benefits include increased precision in demand forecasting, better transportation routes, higher efficiency in warehouses, faster procurement procedures, and efficient techniques for mitigating risks. The study's findings demonstrate that AI possesses the capacity to greatly boost supply chain performance through the improvement of efficiency, reduction of costs, and increase of robustness. Nevertheless, the article also recognizes the difficulties linked to the application of AI, such as substantial upfront investment needs, worries about data privacy and security, the absence of transparency and comprehensibility, possible job displacement, and ethical considerations. In order to tackle these difficulties and enhance the influence of artificial intelligence (AI) in the supply chain, the paper suggests potential avenues for future research. These include making it a priority to create explainable AI (XAI) algorithms, taking steps to lower the risk of job loss, setting moral standards for the proper use of AI, combining AI with new technologies like blockchain, the Internet of Things (IoT), and digital twins, and coming up with AI solutions for complicated supply chain situations. Through tackling these obstacles and investigating novel areas of study, artificial intelligence (AI) can persist in its crucial function of defining the forthcoming trajectory of the supply chain, which will result in the promotion of innovation, effectiveness, and sustainability throughout the worldwide economy.
ER  - 

TY  - CONF
TI  - Implementing Explainable AI for Proactive Regulatory Compliance and Auditing in Financial Markets
T2  - 2025 International Conference on Networks and Cryptology (NETCRYPT)
SP  - 529
EP  - 534
AU  - N. Malali
AU  - S. R. Praveen Madugula
PY  - 2025
KW  - Training
KW  - Radio frequency
KW  - Accuracy
KW  - Uncertainty
KW  - Explainable AI
KW  - Data preprocessing
KW  - Boosting
KW  - Risk management
KW  - Random forests
KW  - Testing
KW  - Regulatory Compliance
KW  - Compliance audit
KW  - Financial Markets
KW  - Credit Risk
KW  - Artificial Intelligence (AI)
KW  - Explainable AI (XAI) Machine Learning
KW  - Random Forest
KW  - Gradient Boosting
KW  - Classification Models
KW  - Data Preprocessing
DO  - 10.1109/NETCRYPT65877.2025.11102636
JO  - 2025 International Conference on Networks and Cryptology (NETCRYPT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Networks and Cryptology (NETCRYPT)
Y1  - 29-31 May 2025
AB  - A credit risk is the possibility of monetary loss due to the borrower's partial or whole failure to fulfil their obligations. This manuscript aims to examine XAI for pioneering feasible regulatory compliance and auditing in the credit risk assessment and loan performance analysis of the financial industry. The methodology integrates machine learning models, including RF and GB, to evaluate credit risk through a dataset comprising 887,379 entries across 74 columns. The study under discussion also focuses on data preprocessing required for correct label encoding and the division of data into training and testing sets. Evaluation metrics, including the ROC curve, accuracy, precision, recall, and F1score, are utilized to assess the model. This means that Gradient Boosting is more accurate and probably the best model, with an accuracy of 98.56% and ROC-AUC of 98.26%, while Random Forest has high precision though low recall. In addition, LIME is the XAI approach used in the research to improve the credit risk model's interpretability. The goal of this work is to deliver usable and pro-active methods for machine learning interpretation in order to assist financial institutions with regulatory compliance and mitigating risks and uncertainty in the financial environment effectively.
ER  - 

TY  - JOUR
TI  - Driver's Hand-Foot Coordination and Global-Regional Brain Functional Connectivity Under Fatigue: Via Graph Theory and Explainable Artificial Intelligence
T2  - IEEE Transactions on Intelligent Vehicles
SP  - 3493
EP  - 3508
AU  - Y. Wu
AU  - W. Li
AU  - J. Zhang
AU  - B. Tang
AU  - J. Xiang
AU  - S. Li
AU  - G. Guo
PY  - 2024
KW  - Fatigue
KW  - Graph theory
KW  - Feature extraction
KW  - Electroencephalography
KW  - Sleep
KW  - Driver behavior
KW  - Driving fatigue
KW  - explainable artificial intelligence (XAI)
KW  - functional connectivity
KW  - graph theory
KW  - hand-foot coordination
DO  - 10.1109/TIV.2023.3339673
JO  - IEEE Transactions on Intelligent Vehicles
IS  - 2
SN  - 2379-8904
VO  - 9
VL  - 9
JA  - IEEE Transactions on Intelligent Vehicles
Y1  - Feb. 2024
AB  - Driving fatigue impairs the driver's hand-foot coordination, a significant factor in traffic accidents. However, the functional brain connectivity underlying driver coordination remains unclear. In this study, we conducted a monotonous long-distance driving experiment (approximately 2 hours) with thirty participants, who rated their fatigue level every 6 minutes using the Karolinska sleepiness scale (KSS). We then categorized the ratings into three fatigue levels: alertness or slight fatigue (AF), moderate fatigue (MF), and severe fatigue (SF). Emergency avoidance scenarios occurred randomly during the experiment, requiring the driver to brake and steer simultaneously to prevent collisions. The multi-band brain functional connectivity network and the critical connections were utilized for global-regional brain functional connectivity analysis and explainable artificial intelligence (XAI) analyses. We found that inter-region connectivity between cognitive and motor regions in the frontal-central and frontal-parietal regions decreased as the driver's fatigue increased to MF. Simultaneously, intra-region connectivity within the central region decreased, impairing intra-motor region connectivity. These diminished the driver's coordination ability. However, profound fatigue triggered an inter-regional and intra-regional reorganization of the brain functional network, reflecting anti-fatigue characteristics but insufficient to enhance driving performance. The XAI results indicated that the beta-band brain functional connection network dataset achieved the highest accuracy (0.941) on the Resnet18 model and revealed fatigue differences in the cognitive and motor regions of the dataset. Our study demonstrated that the driver's hand-foot coordination had distinct brain functional connectivity characteristics under different fatigue states, which could inform the design of neural network detection algorithms for driving fatigue.
ER  - 

TY  - JOUR
TI  - Using an Interpretable Machine Learning Framework to Understand the Relationship of Mobility and Reliability Indices on Truck Drivers’ Route Choices
T2  - IEEE Transactions on Intelligent Transportation Systems
SP  - 13419
EP  - 13428
AU  - X. Kong
AU  - Y. Zhang
AU  - W. L. Eisele
AU  - X. Xiao
PY  - 2022
KW  - Reliability
KW  - Vehicles
KW  - Trajectory
KW  - Indexes
KW  - Real-time systems
KW  - Routing
KW  - Machine learning
KW  - Interpretable machine learning
KW  - planning time index
KW  - route choice
KW  - travel time index
DO  - 10.1109/TITS.2021.3124221
JO  - IEEE Transactions on Intelligent Transportation Systems
IS  - 8
SN  - 1558-0016
VO  - 23
VL  - 23
JA  - IEEE Transactions on Intelligent Transportation Systems
Y1  - Aug. 2022
AB  - This paper explores the relationships between travel time index and planning time index, two proven indices used to measure real-time congestion and travel time reliability, and their impacts on truck drivers’ route choices through training of a predictive model based on a machine learning algorithm—eXtreme Gradient Boost (XGBoost). Moreover, this study adopts an interpretable machine learning framework called SHapley Additive ExPlanation (SHAP) in the predictive model to reveal the insights usually hidden inside the machine learning “black box.” The predictive model is trained through a truck trajectory dataset provided by the Maryland Department of Transportation State Highway Administration and INRIX. The classical logistic regression model is adopted as the baseline model. The results show the XGBoost model can better handle nonlinearity and provide more reliable predictions. Through the SHAP framework, the results indicate that mobility and reliability indices and total trip time nonlinearly influence route choices. Truck drivers are more sensitive to real-time congestion information and reliability information when the differences in mobility and reliability indices on candidate routes reach certain thresholds. Moreover, the interaction study on trip time and mobility index shows that truck drivers are more sensitive to real-time congestion information if the candidate routes’ travel time is a larger portion of the total trip time.
ER  - 

TY  - JOUR
TI  - Artificial Intelligence to Advance Earth Observation: A review of models, recent trends, and pathways forward
T2  - IEEE Geoscience and Remote Sensing Magazine
SP  - 2
EP  - 25
AU  - D. Tuia
AU  - K. Schindler
AU  - B. Demir
AU  - X. X. Zhu
AU  - M. Kochupillai
AU  - S. Džeroski
AU  - J. N. van Rijn
AU  - H. H. Hoos
AU  - F. Del Frate
AU  - M. Datcu
AU  - V. Markl
AU  - B. Le Saux
AU  - R. Schneider
AU  - G. Camps-Valls
PY  - 2024
KW  - Adaptation models
KW  - Artificial intelligence
KW  - Earth
KW  - Data models
KW  - Computational modeling
KW  - Monitoring
KW  - Data mining
DO  - 10.1109/MGRS.2024.3425961
JO  - IEEE Geoscience and Remote Sensing Magazine
IS  - 
SN  - 2168-6831
VO  - 
VL  - 
JA  - IEEE Geoscience and Remote Sensing Magazine
Y1  - 
AB  - Earth observation (EO) is increasingly used for mapping and monitoring processes occurring at the surface of Earth. Data acquired by satellites nowadays allow us to have a global view, consistent in time, of the state of our forests, oceans, and growing urban areas. However, such a wealth of data has little value without appropriate processing chains able to convert the pixel values to information useful for decision makers.
ER  - 

TY  - CONF
TI  - An Integrated, Explainable and Uncertainty-Aware Satellite-Based Precipitation Product for the Mediterranean Region
T2  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
SP  - 7830
EP  - 7834
AU  - P. Kossieris
AU  - I. Tsoukalas
AU  - L. Brocca
AU  - H. Mosaffa
AU  - C. Makropoulos
AU  - A. Anghelea
PY  - 2024
KW  - Precipitation
KW  - Uncertainty
KW  - Accuracy
KW  - Machine learning algorithms
KW  - Fuses
KW  - Geoscience and remote sensing
KW  - Soil
KW  - Precipitation products
KW  - Precipitation data merging
KW  - Machine Learning
KW  - Explainable AI
KW  - predictive uncertainty
DO  - 10.1109/IGARSS53475.2024.10641931
JO  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 7-12 July 2024
AB  - Precipitation is a key variable of earth observation systems, involved in a wide spectrum of applications, from the optimal water resources management to early warning systems for flood/drought events. During the last decade, significant research effort has been given in the development of satellite-based precipitation products (SPPs), with improved accuracy and high spatio-temporal coverage. This work presents a methodological framework that couples machine learning and explainability techniques, along with uncertainty quantification methods, to develop a new SPP for the Mediterranean region. Specifically, the methodology couples: a) ensemble (tree-based) machine learning algorithms to merge multiple satellite-based observations (i.e., both precipitation and soil moisture-based products), b) global and local explainable AI techniques to provide explanations at the model and instance level, respectively, and c) a copula-based approach to quantify the predictive uncertainty of the final integrated product. The new product fuses data from two widely known low-latency SPPs, i.e., the SM2RAIN-ASCAT and GPM Late Run dataset, while its performance is assessed against state-of-art products, i.e., the EMO5, GPM Final Run, ERA5 and PERSIANN CDR.
ER  - 

TY  - CONF
TI  - Towards a data-driven assistance system for operating segment erectors in tunnel boring machines
T2  - 2021 14th International Symposium on Computational Intelligence and Design (ISCID)
SP  - 263
EP  - 267
AU  - H. A. Zhou
AU  - A. Gannouni
AU  - C. Gentz
AU  - J. Tröndle
AU  - S. Neumann
AU  - A. Abdelrazeq
AU  - G. Jacobs
AU  - F. Hees
PY  - 2021
KW  - Buildings
KW  - Data preprocessing
KW  - Machine learning
KW  - Tunneling
KW  - Sensors
KW  - Safety
KW  - Data mining
KW  - TBM
KW  - Segment Erector
KW  - Machine Learning
KW  - Assistance System
KW  - XAI
DO  - 10.1109/ISCID52796.2021.00068
JO  - 2021 14th International Symposium on Computational Intelligence and Design (ISCID)
IS  - 
SN  - 2473-3547
VO  - 
VL  - 
JA  - 2021 14th International Symposium on Computational Intelligence and Design (ISCID)
Y1  - 11-12 Dec. 2021
AB  - In tunnel construction, heavy machinery like tunnel boring machines are used to increase the safety and efficiency of tunneling projects. Despite the fact that these machines are equipped with a variety of different sensors, human workers still operate both the advance and ring building phases of the machine. Due to the high complexity of handling segment erectors, the ring building phase in particular has a lot of room for improvement. In this work, a concept of a data-driven assistance system to support operators in the ring building process is proposed. Three approaches to extract human expertise from operational data of tunnel boring machines using machine learning and explainable artificial intelligence are presented and discussed. Furthermore, preliminary data preprocessing results verify that our approach of determining ring building times is more accurate compared to on-site measurements. This contribution highlights the untouched potential of ring building optimization using machine learning techniques based on real-world data.
ER  - 

TY  - CONF
TI  - Exploring Accident Frequency Using Explainable AI: a Case Study of the Central Section of National Freeway No.1 in Taiwan
T2  - 2025 22nd International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)
SP  - 1
EP  - 6
AU  - M. -C. Yang
AU  - D. -J. Lin
AU  - H. -H. Liu
AU  - A. -C. Chou
AU  - W. -Y. Leu
AU  - Y. -C. Wu
AU  - C. -H. Ou
AU  - K. -C. Wu
PY  - 2025
KW  - Radio frequency
KW  - Solid modeling
KW  - Explainable AI
KW  - Computational modeling
KW  - Roads
KW  - Vehicle detection
KW  - Traffic control
KW  - Environmental factors
KW  - Random forests
KW  - Accidents
KW  - Artificial Intelligence
KW  - SHAP
KW  - Explainable AI (XAI)
KW  - Regression Model
KW  - Accident Frequency
DO  - 10.1109/ECTI-CON64996.2025.11101022
JO  - 2025 22nd International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)
IS  - 
SN  - 2837-6471
VO  - 
VL  - 
JA  - 2025 22nd International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)
Y1  - 20-23 May 2025
AB  - In recent years, with the widespread adoption of artificial intelligence (AI) technology, numerous studies have employed AI for traffic accident analysis and prediction, replacing traditional statistical methods. In Taiwan, traffic accidents occur frequently on a daily basis, making it crucial to investigate the factors contributing to their occurrence. This study utilizes SHapley Additive exPlanations (SHAP), an explainable AI (XAI) technique, in conjunction with two regression models—linear regression and random forest regression—to examine the impact of freeway environmental factors on accident frequency. The study focuses on the central section of National Freeway No. 1 in Taiwan, using road environment variables provided by the Taiwan Ministry of Transportation and Communications and open data on vehicle detection traffic flow from 2021 to 2024. A total of 12 variables, including curve radius and the number of lanes, were analyzed. The SHAP method was used to evaluate feature importance and generate bee swarm plots for visualization. The findings from both regression models indicate that traffic volume and radius of curvature (A-value) are among the top three most influential environmental factors affecting accident frequency. These findings offer valuable empirical insights for enhancing freeway safety management.
ER  - 

TY  - CONF
TI  - Composite Product Cybersecurity Certification Using Explainable AI Based Dynamic Risk Assessment
T2  - 2025 IEEE International Conference on Cyber Security and Resilience (CSR)
SP  - 476
EP  - 481
AU  - N. Basheer
AU  - S. Islam
AU  - S. Papastergiou
AU  - E. M. Kalogeraki
PY  - 2025
KW  - Explainable AI
KW  - Reinforcement learning
KW  - Transforms
KW  - Human in the loop
KW  - Real-time systems
KW  - Software
KW  - Risk management
KW  - Computer security
KW  - Certification
KW  - Resilience
KW  - dynamic risk assessment
KW  - composite TOE
KW  - explainable AI
KW  - human-in-the-loop
KW  - reinforcement learning
KW  - vulnerability
DO  - 10.1109/CSR64739.2025.11130134
JO  - 2025 IEEE International Conference on Cyber Security and Resilience (CSR)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 IEEE International Conference on Cyber Security and Resilience (CSR)
Y1  - 4-6 Aug. 2025
AB  - In the era of security certification, the Composite Target of Evaluation (TOE) plays a key role for composite product cybersecurity certification due to the interconnection among the different hardware and software components. Traditional certification is at odds with time-based sophisticated threats and evolving vulnerabilities. To address this issue, we propose a dynamic risk assessment using Human-in-the-Loop (HITL) reinforcement learning and explainable AI practice to support the composite product cybersecurity certifications. Our approach transforms a composite TOE into individual ones and links assets and vulnerabilities related to the individual TOE for an effective risk assessment through a Deep Q-Network model. The explainable AI practice justifies the model outcomes, while HITL experts validate and refine the decisions to ensure they align with real-world context. In this paper, we evaluated our method using the CVEFree dataset and results showed that the model enhances vulnerability prioritization in total rewards, improving from - 131 to 22,362. Explainable AI aids in the identification of influential features, making the process of issuing a cybersecurity certification more reliable.
ER  - 

TY  - CONF
TI  - Explainable Text Classification for Legal Document Review in Construction Delay Disputes
T2  - 2023 IEEE International Conference on Big Data (BigData)
SP  - 1928
EP  - 1933
AU  - N. Huber-Fliflet
AU  - J. Zhang
AU  - P. Gronvall
AU  - F. Wei
AU  - P. Spinelli
AU  - A. Dabrowski
AU  - J. Yang
PY  - 2023
KW  - Logistic regression
KW  - Law
KW  - Text categorization
KW  - Documentation
KW  - Predictive models
KW  - Boosting
KW  - Data models
KW  - Text classification
KW  - Explainable AI
KW  - Legal document review
KW  - E-Discovery
KW  - rationales
KW  - construction
DO  - 10.1109/BigData59044.2023.10386240
JO  - 2023 IEEE International Conference on Big Data (BigData)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Big Data (BigData)
Y1  - 15-18 Dec. 2023
AB  - The costs involved in manually reviewing documents in legal civil litigations have grown dramatically as more and more information is stored electronically. As a result, the document review process can require an extraordinary dedication of resources. In construction litigations, quickly finding supporting documentation in a delay dispute is critical to the success of a matter. Identifying relevant delay-related communications and supporting documentation has historically been expensive and time consuming. Using machine learning technologies, respondents can be more comprehensive in their assessment of the data requiring review to respond to the claim in time. Explainable machine learning is an active machine learning research area, and in an explainable machine learning system, predictions generated from a machine learning model are explainable and human understandable. In delay dispute ‘document review’ scenarios, a document can be identified as delay-related, as long as one or more of the text snippets in a document are deemed delay-related. In these scenarios, if these delay-related snippets can be located, then attorneys could easily evaluate the model’s decision. The authors of this paper propose an approach for accurately identifying rationales and an approach for boosting document classification accuracy using delay-related snippets and their applications in construction delay disputes. The authors conducted experiments using data from a few real world delay dispute matters and the results from these experiments show that the proposed approaches have the potential to significantly advance the application of text classification in document review in construction delay dispute matters.
ER  - 

TY  - CONF
TI  - Predicting Heart Disease Risk in Diabetic patients using a Pipeline of Ensemble Learning and XAI - Enhanced Approaches
T2  - 2024 1st International Conference on Cognitive, Green and Ubiquitous Computing (IC-CGU)
SP  - 1
EP  - 8
AU  - P. Ghadekar
AU  - U. Shaikh
AU  - R. Ner
AU  - S. Patil
AU  - R. Qazi
PY  - 2024
KW  - Heart
KW  - Explainable AI
KW  - Decision making
KW  - Medical services
KW  - Artificial neural networks
KW  - Diabetes
KW  - Risk management
KW  - Diabetic Patients
KW  - Ensemble Learning
KW  - Heart Disease Risk
KW  - Pipeline
KW  - Predicting
KW  - XAI-Enhanced Approaches
DO  - 10.1109/IC-CGU58078.2024.10530665
JO  - 2024 1st International Conference on Cognitive, Green and Ubiquitous Computing (IC-CGU)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 1st International Conference on Cognitive, Green and Ubiquitous Computing (IC-CGU)
Y1  - 1-2 March 2024
AB  - An innovative approach for examining the complex interaction between diabetes and heart disease is presented in “A Unified Approach to Diabetes and Heart Disease Analysis: An Ensemble of XAI-Enhanced Models”. For this project, explainable Artificial Intelligence (XAI) approaches are applied to strengthen a unified ensemble of AI models. The method encourages interpretability and openness in the examination of these common health concerns. By utilizing heterogenous data sources, like clinical notes, lifestyle information, and medical pictures, the ensemble model provides thorough insights. Critical variables and risk markers that underlie the co-occurrence of diabetes and heart disease are revealed by integrating machine learning models with XAI. Early risk assessment and tailored interventions are made possible by this strategy, which has the potential to completely transform the healthcare industry. Improving the models' interpretability seeks to close the trust and confidence gap that exists between sophisticated AI systems and real-world medical applications, benefiting both patients and healthcare providers. The aim is to improve early risk assessment and diagnosis through interpretable insights and improved predictive accuracy through the integration of multiple AI models and XAI methods. This will ultimately improve the quality of healthcare decision-making in these crucial domains amidst the various conditions and unknown stages of heart health.
ER  - 

TY  - CONF
TI  - Evaluating Project Manager Competencies Using Explainable Artificial Intelligence
T2  - 2025 International Conference Automatics, Robotics and Artificial Intelligence (ICARAI)
SP  - 1
EP  - 6
AU  - A. A. F. Nejad
AU  - F. Arabikhan
AU  - A. Gegov
AU  - A. Ichtev
PY  - 2025
KW  - Industries
KW  - Decision support systems
KW  - Ant colony optimization
KW  - Accuracy
KW  - Real-time systems
KW  - Data models
KW  - Telecommunications
KW  - Decision trees
KW  - Artificial intelligence
KW  - Context modeling
KW  - Project Manager
KW  - Competency Evaluation
KW  - PMBOK
KW  - Explainable Artificial Intelligence
KW  - IPMA
DO  - 10.1109/ICARAI67046.2025.11137895
JO  - 2025 International Conference Automatics, Robotics and Artificial Intelligence (ICARAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference Automatics, Robotics and Artificial Intelligence (ICARAI)
Y1  - 13-15 June 2025
AB  - Nowadays, in various industries, particularly those related to information technology and telecom, AI-driven projects have increased. The implementation of these projects poses fundamental challenges, and the presence of competent project managers can enhance the likelihood of their success. Therefore, the primary goal of this article is to develop a model for evaluating the competencies of AI-driven project managers. In this context, after reviewing conventional models in this field and conducting surveys with experts based on the IPMA model, evaluation indicators for AI-driven project managers were identified. Subsequently, using the decision tree algorithm and optimizing class weights with the Ant Colony Optimization algorithm, a decision support system was developed for continuous and real-time evaluation of AI-driven project managers. The article's findings indicate that indicators such as Change and Transformation, Resourcefulness, and Time are among the most important competencies for project managers. Moreover, the decision tree model optimized with the Ant Colony Optimization algorithm can accurately identify the competencies of project managers with a 94% accuracy rate, outperforming other algorithms. Additionally, a sensitivity analysis of various model features, conducted using the Shapley Additive exPlanations Algorithm, reveals the impact of each feature on the competencies of project managers.
ER  - 

TY  - CONF
TI  - Unlocking Transparency in Credit Scoring: Leveraging XGBoost with XAI for Informed Business Decision-Making
T2  - 2024 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)
SP  - 1
EP  - 6
AU  - M. Alblooshi
AU  - H. Alhajeri
AU  - M. Almatrooshi
AU  - M. Alaraj
PY  - 2024
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Decision making
KW  - Transforms
KW  - Predictive models
KW  - Parallel processing
KW  - Prediction algorithms
KW  - credit scoring
KW  - XGBoost
KW  - XAI
KW  - LIME
KW  - Blackbox
KW  - algorithm
DO  - 10.1109/ACDSA59508.2024.10467573
JO  - 2024 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)
Y1  - 1-2 Feb. 2024
AB  - Credit score analysis is vital to modern banking systems, allowing banks and other financial institutions to determine a borrower's creditworthiness. In such a situation, accurate and robust prediction models are vital because they allow lenders to make rational decisions regarding loan approvals and risk management. This paper provides an overview of using XGBoost, a sophisticated machine learning algorithm, to improve credit score evaluation, and the XAI model, LIME, to describe the black box machine learning algorithm. XGBoost, a gradient boosting-based ensemble learning algorithm, has gained prominence for its capacity to give improved predicted accuracy while handling vast and complicated datasets. Its algorithmic characteristics, including regularization, parallel processing, and decision tree optimisation, make it especially well-suited for credit scoring problems. Because of its complexity, implementing XAI is critical since it will help lenders grasp the reasons for the result of the XGBoost. The results show how the XAI model, LIME, helps simplify the complexity of these models. It is critical to integrate XAI models since they will improve lender decision-making. The fundamental goal of this research is to evaluate the XAI model, LIME, and determine how well the XAI model explains the findings of our experimental tests. Furthermore, it illustrates the possibility of incorporating LIME into credit score analysis, resulting in more efficient lending procedures, enhanced risk management, and better decision-making. Finally, this paper emphasizes the importance of using advanced machine learning techniques such as XGBoost in credit scoring analysis, which has the potential to transform the way banks and other financial institutions assess credit risk, as well as include LIME for a better understanding of the results.
ER  - 

TY  - CONF
TI  - The Geores Project: Geospatial Application in Support of Environmental Sustainability and Resilience to Climate Changes in Urban Areas
T2  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
SP  - 1384
EP  - 1387
AU  - R. Lafortezza
AU  - F. Giordano
AU  - D. Capolongo
AU  - A. Refice
AU  - F. P. Lovergine
AU  - M. Elia
AU  - N. Amoroso
AU  - R. Nutricato
AU  - D. O. Nitti
AU  - A. Parisi
AU  - A. Ursi
AU  - P. Sacco
AU  - M. Virelli
AU  - D. Tapete
PY  - 2024
KW  - Degradation
KW  - Wildfires
KW  - Explainable AI
KW  - Urban areas
KW  - Green products
KW  - Geospatial analysis
KW  - Sediments
KW  - Multi-risk, artificial intelligence, sediment connectivity, land displacement, flood, wildfire
DO  - 10.1109/IGARSS53475.2024.10642728
JO  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 7-12 July 2024
AB  - The GEORES project is funded by the Italian Space Agency (ASI) and aims to develop a geospatial application meant to improve environmental sustainability and resilience to climate changes in urban areas, based on the synergistic use of the most advanced Earth Observation (EO) technologies, Artificial Intelligence (AI) and eXplainable AI (XAI). GEORES is organized into four main modules to support management of the main risks associated with land degradation: (1) Sediment Connectivity; (2) Land Displacement; (3) Urban Floods; (4) Urban Wildfires. For each module, EO data, calculation models and algorithms are integrated to identify "hot-spots" of urban and peri-urban territory at high risk from the point of view of land degradation caused by phenomena of hydrogeological instability, sediment flow or vegetation fires. The extracted information is expressed with specific indicators ("geo-analytics") calculated dynamically and automatically. The demonstration is undertaken in the Metropolitan City of Bari and Gargano Promontory, Apulia Region, southern Italy, and foresees the engagement of final users (i.e. Regional Civil Protection and Municipality of Bari).
ER  - 

TY  - CONF
TI  - Emotional Well-being of a Human: A Case Study of Proposing a Digital Twin Solution Integrating IoT Sensors and Cloud Computing
T2  - 2025 International Conference on Ambient Intelligence in Health Care (ICAIHC)
SP  - 1
EP  - 7
AU  - A. Swain
AU  - A. Dalei
AU  - S. Sahoo
PY  - 2025
KW  - Temperature sensors
KW  - Heart rate
KW  - Cloud computing
KW  - Predictive models
KW  - Real-time systems
KW  - Data models
KW  - Digital twins
KW  - Internet of Things
KW  - Biomedical monitoring
KW  - Monitoring
KW  - Digital Twin
KW  - IoT Sensor
KW  - Explainable AI-SHAP
KW  - ML Model
KW  - Cloud Computing
KW  - Human Psychology
KW  - Well-Being
DO  - 10.1109/ICAIHC64101.2025.10956277
JO  - 2025 International Conference on Ambient Intelligence in Health Care (ICAIHC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Ambient Intelligence in Health Care (ICAIHC)
Y1  - 10-11 Jan. 2025
AB  - There are many different types of technology that can improve well-being and quality of life through monitoring, comprehension, and continuous feedback. One such technology is the digital twin. Consequently, employing a digital twin as a type of solution can improve an individual's emotional health and quality of life in a surrounding environment. But it will take some time to develop digital twin systems that can make it easier to create and use digital twins because there are many variables that can affect how a digital twin is designed, including the kind of sensor, complementary technologies, the context of the situation being assessed for the person, and the surrounding environment. In this paper, we describe a real-time hybrid sensor fusion strategy for environmental factors, such as temperature, humidity and brightness with respect to an individual's heart rate monitoring using machine learning models such as random forest, linear regression and ridge regression. Further, Explainable AI-SHAP is utilized to enhance the interpretability of the machine learning models, hence guaranteeing the credibility and understanding of the digital twin's predictions in this study. These methods put the idea of real- time feedback to use by introducing a digital twin for emotions of human into practice.
ER  - 

TY  - CONF
TI  - The Intersection of AI, ML, and Industry: A Review of Emerging Trends in Real-World Applications
T2  - 2024 4th International Conference on Advancement in Electronics & Communication Engineering (AECE)
SP  - 769
EP  - 773
AU  - A. Joshi
AU  - V. Kumar
AU  - N. Chauhan
AU  - A. Kumar
AU  - R. K. Singh
PY  - 2024
KW  - Industries
KW  - Productivity
KW  - Ethics
KW  - Reviews
KW  - Transportation
KW  - Medical services
KW  - Transforms
KW  - Market research
KW  - Manufacturing
KW  - Artificial intelligence
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Industry
KW  - Emerging Trends
KW  - Real-World Applications
KW  - Healthcare
KW  - Finance
KW  - Manufacturing
KW  - Retail
KW  - Transportation
DO  - 10.1109/AECE62803.2024.10911386
JO  - 2024 4th International Conference on Advancement in Electronics & Communication Engineering (AECE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Advancement in Electronics & Communication Engineering (AECE)
Y1  - 22-23 Nov. 2024
AB  - Machine learning (ML) and artificial intelligence (AI) are transforming a number of industries, spurring creativity, and improving productivity. This review article examines the new developments at the nexus of industry, artificial intelligence, and machine learning. It offers a thorough rundown of current developments and real-world uses. We demonstrate how artificial intelligence (AI) and machine learning (ML) are being incorporated into industrial processes to solve complex problems and enhance operational workflows by looking at important industries including healthcare, banking, manufacturing, retail, and transportation. We highlight the revolutionary power of these technologies, talk about the difficulties and moral issues, and suggest future paths for study and development through in-depth case studies and practical examples.
ER  - 

TY  - CONF
TI  - Shapley Additive Explanations for Machine Learning-based Decision Support Systems for Power Transformers’ Diagnostics
T2  - 2025 IEEE Ural-Siberian Conference on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)
SP  - 423
EP  - 426
AU  - A. I. Khalyasmaa
AU  - P. V. Matrenin
AU  - S. A. Eroshenko
PY  - 2025
KW  - Decision support systems
KW  - Additives
KW  - Accuracy
KW  - Power supplies
KW  - Oils
KW  - High-voltage techniques
KW  - Oil insulation
KW  - Boosting
KW  - Power transformers
KW  - Reliability
KW  - technical state
KW  - diagnostics
KW  - power transformers
KW  - explainable artificial intelligence
KW  - Shapley additive explanations
DO  - 10.1109/USBEREIT65494.2025.11054163
JO  - 2025 IEEE Ural-Siberian Conference on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)
IS  - 
SN  - 2769-3635
VO  - 
VL  - 
JA  - 2025 IEEE Ural-Siberian Conference on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)
Y1  - 12-13 May 2025
AB  - Technical diagnostics of high-voltage equipment is a relevant task to reduce the risks of accidents at power facilities and, consequently, to ensure reliable power supply to consumers. Currently, a lot of research is aimed at improving the accuracy of diagnostics of high-voltage equipment, in particular, power transformers using machine learning methods. However, the application of machine learning leads to the construction of diagnostic systems that are a "black box" for the user, if the methods of explainable artificial intelligence are applied. This paper proposes the application of Shapley additive explanations algorithm to increase the interpretability of ensemble machine learning models in assessing the technical state of a power transformer using the proposed modified Shapley additive explanations method. An open dataset containing the results of transformer oil sampling was used. The difference of the paper lies in the modification of Shapley additive explanations, aimed at increasing the informativeness of visualization of interpretations of the results of machine learning models. The proposed method makes it possible to increase the validity of decisions made in the technical diagnostics of high-voltage equipment using machine learning models, which will allow for more effective implementation of intelligent decision support systems in the power industry.
ER  - 

TY  - CONF
TI  - Adaptive Shapley: Using Explainable AI with Large Datasets to Quantify the Impact of Arbitrary Error Sources
T2  - 2024 9th International Conference on Big Data Analytics (ICBDA)
SP  - 305
EP  - 310
AU  - B. M. Magnussen
AU  - M. Jessulat
AU  - C. Stern
AU  - B. Sick
PY  - 2024
KW  - Measurement
KW  - Measurement errors
KW  - Current measurement
KW  - Measurement uncertainty
KW  - Production
KW  - Sensor phenomena and characterization
KW  - Predictive models
KW  - explainable ai
KW  - big data mining
KW  - shapley values
DO  - 10.1109/ICBDA61153.2024.10607312
JO  - 2024 9th International Conference on Big Data Analytics (ICBDA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 9th International Conference on Big Data Analytics (ICBDA)
Y1  - 16-18 March 2024
AB  - Almost all sensors suffer from some level of uncer-tainty introduced from production inaccuracies. When the sensor data is processed by machine learning, quantifying the impact of such production inaccuracies on the output of the machine learning model becomes difficult. Certain neural network architectures, such as continuous feature networks, allow individual features and data to be omitted while still being able to correctly predict the result without the need for retraining. Such features can, for example, be individual channels of a sensor. This article proposes a method to use the capability to omit arbitrary features or sensor channels to calculate Shapley values for each sensor channel. These Shapley values represent the contribution of each individual channel to the measurement. They are defined using an arbitrary function called the “value function”. If the value function is defined as the error of the current measurement, the Shapley values will represent the contribution of each sensor channel to the error of the measurement result. By calculating Shapley values like this for a large unlabelled dataset of measurements, it is possible to understand how much measurement error was introduced by which channel of which sensor in each measurement. Averaging the Shapley values for each sensor in the dataset will then result in a metric for each channel of that sensor, which represents a contribution to measurement errors. By comparing these values to any arbitrary quality metrics for the sensor channels obtained in a calibration process or similar step, it is possible to correlate and quantify which value in the quality metric will cause how much of a measurement error, or whether the quality metric is even relevant for the measurement accuracy. This article will show the efficacy and use case of the method on an example of the production and quality control of optical sensors based on multiple spatially resolved reflection spectroscopy.
ER  - 

TY  - CONF
TI  - Improving Mutual Fund Performance Analysis through the Fusion of CNN-LSTM and Explainable AI Techniques
T2  - 2024 Third International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT)
SP  - 1
EP  - 6
AU  - R. B P G
AU  - N. Francis
AU  - N.Venkatarathnam
AU  - K. Mahar
AU  - M. Kethan
AU  - I. I. Raj
PY  - 2024
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Mutual funds
KW  - Predictive models
KW  - Performance analysis
KW  - Convolutional neural networks
KW  - Stakeholders
KW  - Forecasting
KW  - Long short term memory
KW  - Mutual Fund Performance Analysis
KW  - LSTM Networks
KW  - Explainable AI Techniques
KW  - Financial Time Series Data
KW  - Predictive Accuracy
DO  - 10.1109/ICEEICT61591.2024.10718505
JO  - 2024 Third International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Third International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT)
Y1  - 24-26 July 2024
AB  - Analysis of mutual fund performance is crucial for fund managers and investors to make wise choices. Predictions made using traditional approaches are frequently not as accurate since they are unable to identify intricate patterns in financial data. Convolutional neural networks (CNN) and long short-term memory (LSTM) networks are two examples of deep learning approaches that provide promising ways to improve the predictive accuracy of financial forecasting jobs. Incorporating explainable AI techniques can also help with risk management and decision-making by offering insights into the fundamental causes influencing mutual fund performance. By utilizing the combined strength of explainable AI techniques and CNN-LSTM architecture, this work seeks to improve mutual fund performance analysis. The aim is to create a strong framework that can forecast mutual fund performance with accuracy and offer comprehensible explanations of the underlying elements. This work is interesting because it combines explainable AI methods which are particularly useful for analyzing mutual fund performance with CNN-LSTM architecture. In this work, the dual challenges of prediction accuracy and model transparency in financial forecasting are addressed by integrating deep learning with interpretability. The proposed framework for mutual fund performance analysis uses historical data, CNN-LSTM architecture, and explainable AI methods. The model outperforms traditional methods, achieving higher predictive accuracy and providing actionable insights into fund performance drivers. The model's interpretability enhances trustworthiness and utility for investors and fund managers, empowering stakeholders with better decision-making in dynamic financial markets.
ER  - 

TY  - CONF
TI  - State-of-the-Art Review of Life Insurtech: Machine learning for underwriting decisions and a Shift Toward Data-Driven, Society-oriented Environment
T2  - 2024 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)
SP  - 1
EP  - 12
AU  - A. Kharlamova
AU  - A. Kruglov
AU  - G. Succi
PY  - 2024
KW  - Surveys
KW  - Privacy
KW  - Systematics
KW  - Reviews
KW  - Transfer learning
KW  - Insurance
KW  - Risk management
KW  - artificial intelligence
KW  - machine learning
KW  - life insurance
KW  - XAI
KW  - privacy-preserving techniques
KW  - federated learning
KW  - transfer learning
KW  - sharpley
DO  - 10.1109/HORA61326.2024.10550565
JO  - 2024 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)
Y1  - 23-25 May 2024
AB  - Machine learning has been used by insurance companies for nearly a decade to identify potential risks and improve underwriting decisions. Nonetheless, there is a lack of systematic survey articles on state-of-the-art (SoTA) machine learning techniques, especially with respect to Society-oriented Environment models, developed to tackle these problems. This article begins by outlining the limitations of current systems used in this domain, including interpretability and explainability constraints, privacy issues, and the credibility of machine learning constructions along with their solutions. It then provides an extensive review of state-of-the-art machine learning algorithms such as Explainable AI, Privacy-Preserving Techniques, Federated and Transfer Learning, Sharpley, as well as others developed and applied for various Life Insurtech problems. The article also examines the existing challenges and future trends in developing machine-learning-based underwriting decision approaches. We believe that this survey can offer practical guidance for building next-generation insurance risk management systems.
ER  - 

TY  - JOUR
TI  - Explainable Artificial Intelligence for Smart Grid Intrusion Detection Systems
T2  - IT Professional
SP  - 18
EP  - 24
AU  - A. Yayla
AU  - L. Haghnegahdar
AU  - E. Dincelli
PY  - 2022
KW  - Industries
KW  - Privacy
KW  - Machine learning algorithms
KW  - Closed box
KW  - Intrusion detection
KW  - Control systems
KW  - Turning
DO  - 10.1109/MITP.2022.3163731
JO  - IT Professional
IS  - 5
SN  - 1941-045X
VO  - 24
VL  - 24
JA  - IT Professional
Y1  - 1 Sept.-Oct. 2022
AB  - A popular approach to overcome the complexity of cybersecurity and sophistication of cyber attacks is implementing artificial intelligence (AI)-based security controls that integrate machine learning (ML) algorithms into security controls, such as intrusion and malware detection. These AI-based security controls are considered more effective than traditional signature-based and heuristics-based controls. However, the growing adoption of advanced ML algorithms is turning these AI-based security controls into black-box systems. We postulate that these black-box AI methods would make risk management and informed decision-making challenging. Using smart grid intrusion detection as our context, we illustrate our arguments by outlining a risk assessment plan to discuss the transparency and interpretability of an AI-based security control. We contribute to the literature by changing the focus from performance to explainability of algorithms, highlighting critical steps in explainability for integrating into risk assessment planning, and outlining the implications of explainability in AI-based intrusion detection systems.
ER  - 

TY  - CONF
TI  - AI-Powered CNC Digital Twin for Predictive Maintenance
T2  - 2025 Fourth International Conference on Power, Control and Computing Technologies (ICPC2T)
SP  - 1
EP  - 6
AU  - S. Singh
AU  - S. Sethi
AU  - R. Sharma
AU  - D. Vaibhavi
AU  - A. Tiwari
PY  - 2025
KW  - Productivity
KW  - Schedules
KW  - Power control
KW  - Real-time systems
KW  - Maintenance
KW  - Digital twins
KW  - Internet of Things
KW  - Computer numerical control
KW  - Predictive maintenance
KW  - Smart manufacturing
KW  - CNC
KW  - AI
KW  - IoT
KW  - XGBoost
KW  - MQTT
KW  - HTTP/2
KW  - SSL/TLS
KW  - OPC-UA
KW  - NVIDIA
KW  - TLS
KW  - ERP
KW  - MES
KW  - TFX
KW  - CI/CD
DO  - 10.1109/ICPC2T63847.2025.10958573
JO  - 2025 Fourth International Conference on Power, Control and Computing Technologies (ICPC2T)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 Fourth International Conference on Power, Control and Computing Technologies (ICPC2T)
Y1  - 20-22 Jan. 2025
AB  - In today’s manufacturing landscape, maintaining the operational efficiency of CNC (Computer Numerical Control) machines is critical for minimizing downtime and maximizing productivity. This paper presents a novel framework that integrates artificial intelligence (AI) with digital twin technology for predictive maintenance in CNC operations. By leveraging IoT (Internet of Things) sensors, real-time data is collected from CNC machines, enabling the creation of a dynamic digital twin that accurately represents machine performance and behavior. Hereby, employing advanced machine learning techniques, specifically XGBoost, to analyze historical and real-time data, facilitating the early detection of potential failures and optimizing maintenance schedules. It not only enhances the accuracy of maintenance predictions but also significantly reduces the maintenance cost and unplanned machine downtimes. This research contributes to the ongoing development of smart manufacturing systems and underscores the importance of AI-driven solutions in enhancing predictive maintenance strategies within the industrial sector.
ER  - 

TY  - CONF
TI  - Does Dataset Complexity Matters for Model Explainers?
T2  - 2021 IEEE International Conference on Big Data (Big Data)
SP  - 5257
EP  - 5265
AU  - J. Ribeiro
AU  - R. Silva
AU  - L. Cardoso
AU  - R. Alves
PY  - 2021
KW  - Analytical models
KW  - Computational modeling
KW  - Conferences
KW  - Big Data
KW  - Predictive models
KW  - Benchmark testing
KW  - Data models
KW  - Explainable Artificial Intelligence - XAI
KW  - Black box model
KW  - Dataset complexity
DO  - 10.1109/BigData52589.2021.9671630
JO  - 2021 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Big Data (Big Data)
Y1  - 15-18 Dec. 2021
AB  - Strategies based on Explainable Artificial Intelligence - XAI have emerged in computing to promote a better understanding of predictions made by black box models. Most XAI measures used today explain these types of models, generating attribute rankings aimed at explaining the model, that is, the analysis of Attribute Importance of Model. There is no consensus on which XAI measure generates an overall explainability rank. For this reason, several proposals for tools have emerged (Ciu, Dalex, Eli5, Lofo, Shap and Skater). An experimental benchmark of explainable AI techniques capable of producing global explainability ranks based on tabular data related to different problems and ensemble models are presented herein. Seeking to answer questions such as "Are the explanations generated by the different measures the same, similar or different?" and "How does data complexity play along model explainability?" The results from the construction of 82 computational models and 592 ranks shed some light on the other side of the problem of explainability: dataset complexity!
ER  - 

TY  - CONF
TI  - Enhancing Supplier Selection through Explainable AI: A Transparent and Interpretable Approach
T2  - 2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)
SP  - 1
EP  - 6
AU  - K. P
AU  - V. K. S
PY  - 2023
KW  - Surveys
KW  - Productivity
KW  - Procurement
KW  - Biological system modeling
KW  - Computational modeling
KW  - Predictive models
KW  - Numerical models
KW  - Supplier selection
KW  - Explainable AI (XAI)
KW  - Transparency
KW  - Interpretability
KW  - Machine learning models
KW  - Risk mitigation
DO  - 10.1109/RMKMATE59243.2023.10369820
JO  - 2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)
Y1  - 1-2 Nov. 2023
AB  - Supplier Selection under Procurement remains a relatively unexplored issue in Supply Chain Management. It is a key factor that overlooks the enforcement of good buying decisions, satisfying stakeholders, minimizing maverick spending and hitting performance targets. Through provider determination, an association looks at, surveys, and draws in providers. The Provider Choice cycle uses an excessive measure of an organisation's resources and is habitually crucial to the productivity of any firm. So far, the extent of sending man-made intelligence-supported provider determination models has been restricted to the acquisition of C-type items. This is because of the presence of undetected predispositions in the framework, the powerlessness of computer-based intelligence models to consider subjective boundaries, the absence of significant datasets, and equivocalness in regards to the choice made by the calculation. The development of Explainable AI has given us a collection of tools and frameworks that make it simpler to comprehend and interpret the predictions provided by machine learning models, enabling us to create AI that is more inclusive and understandable. In addition, a number of recent studies have demonstrated a strong correlation between weightable statistics and qualitative parameters like a supplier's market reputation. Subsequently, in this task, a blend of numerical, measurable, and simulated intelligence approaches are utilised to check the exhibition and reasonableness of a provider in light of goal and emotional characteristics. A post-hoc Explainable AI structure is then added to help find bias, drift, and other data gaps and close them. It also increases end-user trust by making information more transparent and providing explanations that can be understood by humans.
ER  - 

TY  - CONF
TI  - Autonomous Pollination System for Tomato Plants in Greenhouses: Integrating Deep Learning and Robotic Hardware Manipulation on Edge Device
T2  - 2024 International Conference on Innovations in Science, Engineering and Technology (ICISET)
SP  - 1
EP  - 6
AU  - M. J. Karim
PY  - 2024
KW  - Deep learning
KW  - Visualization
KW  - Explainable AI
KW  - Pollination (plants)
KW  - Greenhouses
KW  - Transformers
KW  - Ventilation
KW  - Hardware
KW  - Robots
KW  - Graphical user interfaces
KW  - C3 Transformer Module
KW  - Lightweight model
KW  - Deep Learning
KW  - Robotic Hardware
KW  - YOLOv8 Nano
KW  - Edge device
KW  - BoT-SORT Algorithm
KW  - Explainable AI
DO  - 10.1109/ICISET62123.2024.10939425
JO  - 2024 International Conference on Innovations in Science, Engineering and Technology (ICISET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Innovations in Science, Engineering and Technology (ICISET)
Y1  - 26-27 Oct. 2024
AB  - Air pollination is important for plants like tomatoes as it plays a vital role in the reproduction and productivity of crops. Tomatoes, being predominantly selfpollinated, depend on airflow to transfer pollen from the anther to the stigma within the flower. In isolated environments like greenhouses, ensuring effective air pollination presents a significant challenge. Due to limited natural airflow, greenhouses often rely on mechanical ventilation systems which cannot adequately replicate the natural pollination process. In this work, a low-cost but efficient autonomous pollination system is developed for tomato plants in greenhouses that utilize both deep learning capability as well as robotic hardware manipulation. The deep learning model is based on the YOLOv8 Nano architecture that is customized with a transformer module which achieved an mAP@50 score of $\mathbf{9 4. 6}$ percent for mature flower detection. Explainable AI (XAI) like EigenCAM and XGrad-CAM approach showed correct visualization over the flower area with minimum noise. The developed model was implemented on the Nvidia Jetson Nano where the Bot-SORT tracking algorithm was used to avoid over-pollination of the same flower. For real-time application, a Graphical User Interface (GUI) app was developed which functioned together with the hardware system using closed-loop feedback control on a pan-tilt mechanism for targeting each mature tomato flower. This technique ensured consistent pollination throughout the day and offers efficiency, precision, and cost-effectiveness by increasing yield.
ER  - 

TY  - JOUR
TI  - Advancing Manufacturing Through Artificial Intelligence: Current Landscape, Perspectives, Best Practices, Challenges, and Future Direction
T2  - IEEE Access
SP  - 131621
EP  - 131637
AU  - R. Rakholia
AU  - A. L. Suárez-Cetrulo
AU  - M. Singh
AU  - R. Simón Carbajo
PY  - 2024
KW  - Artificial intelligence
KW  - Manufacturing
KW  - Industries
KW  - Machine learning algorithms
KW  - Reviews
KW  - Automation
KW  - Production
KW  - Smart manufacturing
KW  - Fourth Industrial Revolution
KW  - Quality control
KW  - Internet of Things
KW  - Best practices
KW  - Smart manufacturing
KW  - Industry 4.0
KW  - artificial intelligence
KW  - automation
KW  - machine learning
KW  - quality control
KW  - Internet of Things
KW  - robotics
DO  - 10.1109/ACCESS.2024.3458830
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - The industrial sector is currently undergoing a transformative era of intelligent automation driven by Artificial Intelligence (AI) capabilities. This synergy greatly enhances efficiency and seamlessly enables data-driven decision-making processes. These advantages enable more efficient resource allocation and enhance production planning precision. This paper aims to provide state-of-the-art and ongoing developments in the AI landscape within the manufacturing industry. In addition, the review explores the key areas where AI is being applied in manufacturing, such as predictive maintenance, quality control, process optimization, supply chain management, robotics and automation, and intelligent decision support systems. The review also encompasses an exploration of the challenges encountered by the manufacturing sector, alongside an investigation into the potential of AI to mitigate these challenges. Furthermore, this work thoroughly reviews recent AI advancements, including explainable AI, human-robot collaboration, edge computing, and the Internet of Things (IoT) integration. The review concludes by providing recommendations, highlighting best practices, and providing insights into potential collaborative opportunities.
ER  - 

TY  - CONF
TI  - Towards Explainable AI Validation in Industry 4.0: A Fuzzy Cognitive Map-based Evaluation Framework for Assessing Business Value
T2  - 2023 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)
SP  - 1
EP  - 9
AU  - Z. Dikopoulou
AU  - E. Lavasa
AU  - S. Perez-Castanos
AU  - D. Monzo
AU  - S. Moustakidis
PY  - 2023
KW  - Measurement
KW  - Manufacturing industries
KW  - Technological innovation
KW  - Simulation
KW  - Scalability
KW  - Transforms
KW  - Fourth Industrial Revolution
KW  - explainability
KW  - explainable artificial intelligence
KW  - fuzzy cognitive maps
KW  - key performance indicators
KW  - business perspectives
KW  - industry 4.0
DO  - 10.1109/ICE/ITMC58018.2023.10332301
JO  - 2023 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)
IS  - 
SN  - 2693-8855
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)
Y1  - 19-22 June 2023
AB  - The development of Artificial Intelligence (AI) systems in Industry 4.0 has gained momentum due to their potential for increasing efficiency and productivity. However, AI systems can be just as complex and opaque, leading to concerns about their reliability, trustworthiness, and accountability. To address these issues, this paper proposes a validation framework for Explainable AI (XAI) in Industry 4.0 based on Fuzzy Cognitive Maps (FCMs). The proposed framework aims to evaluate Key Performance Indicators (KPIs) based on a set of AI metrics and XAI metrics. The FCM-based approach enables the representation of causal-effect relationships between the different concepts of the system using expert knowledge. The presented validation framework provides a theoretical background for evaluating and optimizing the business values of AI systems based on multiple criteria in the manufacturing industry, demonstrating its effectiveness. The main contributions of this paper are: i) the development of an FCM-based validation framework for XAI in Industry 4.0; ii) the identification of relevant AI and XAI metrics for the evaluation of the KPIs of the theoretical graph model; and iii) the demonstration of the effectiveness of the proposed framework through a case study. The results of this study provide valuable insights into the importance of considering not only accuracy but also efficiency and transparency when developing AI pipelines that generate higher business value. Overall, this paper offers a theoretical foundation and practical insights for organizations seeking to evaluate the business values of their AI systems in Industry 4.0. It emphasizes the importance of explainability and the integration of AI and XAI metrics in achieving transparent and accountable AI solutions that deliver optimal results for the manufacturing industry and beyond.
ER  - 

TY  - CONF
TI  - Early-Stage Diabetes Risk Prediction Utilizing Machine Learning with Explainable AI from Polynomial and Binning Feature Generation
T2  - 2024 2nd International Conference on Information and Communication Technology (ICICT)
SP  - 26
EP  - 30
AU  - M. Mamun
AU  - S. H. Chowdhury
AU  - M. I. Hussain
AU  - M. S. Iqbal
PY  - 2024
KW  - Accuracy
KW  - Explainable AI
KW  - Medical services
KW  - Predictive models
KW  - Polynomials
KW  - Diabetes
KW  - Risk management
KW  - Reliability
KW  - Insulin
KW  - Diseases
KW  - Early-Stage Diabetes Risk
KW  - Machine Learning
KW  - Feature Generation
KW  - Explainable Artificial Intelligence
DO  - 10.1109/ICICT64387.2024.10839710
JO  - 2024 2nd International Conference on Information and Communication Technology (ICICT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd International Conference on Information and Communication Technology (ICICT)
Y1  - 21-22 Oct. 2024
AB  - Diabetes is a chronic disease that affects a significant portion of the global population. It occurs when the body cannot produce enough insulin or effectively use the insulin it produces, leading to elevated blood glucose levels. Diabetes is a major contributor to various severe health conditions, including heart disease, stroke, kidney failure, and nerve damage. Early detection of diabetes is crucial in mitigating these associated health risks and improving patient outcomes. In response to the increasing prevalence of diabetes, we have developed an automated system for Early-Stage Diabetes Risk Prediction (ESDRP). This study utilizes a dataset consisting of 16 features from 520 instances. We applied multiple Machine Learning (ML) models, including XGBoost (XGB), Bootstrap Aggregating (BAG), Adaptive Boosting (AdaBoost), Light Gradient Boosted Machine (LGBM), and Gradient Boosting Decision Trees (GBDT), both with and without feature generation techniques. Specifically, we explored polynomial and binning feature generation methods. Our findings indicate that the polynomial feature generation technique combined with XGB yielded the highest performance, achieving an accuracy of 99.22%, precision of 100%, recall of 98.15%, specificity of 99.06%, and F1-score of 100%. Additionally, all the ML models were evaluated using confusion matrices (CM) and ROC curves, with the average performance across 10-fold cross-validation demonstrating robust predictive capabilities. Furthermore, to establish trust in our model's predictions, we incorporated two explainable AI (XAI) methods: LIME and SHAP. These techniques helped us understand feature importance, the decision-making process of the models, and enhanced the reliability of our results. Our automated system aims to assist individuals of all ages and healthcare systems in identifying ESDRP, thereby supporting informed decision-making and improving global health outcomes.
ER  - 

TY  - CONF
TI  - An Approach Towards Identifying Bangladeshi Leaf Diseases through Transfer Learning and XAI
T2  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
SP  - 1744
EP  - 1749
AU  - F. F. Preotee
AU  - S. Sarker
AU  - S. R. Refat
AU  - T. Muhammad
AU  - S. Islam
PY  - 2024
KW  - Deep learning
KW  - Productivity
KW  - Accuracy
KW  - Explainable AI
KW  - Plants (biology)
KW  - Transfer learning
KW  - Livelihood
KW  - Predictive models
KW  - Protection
KW  - Diseases
KW  - Leaf Disease Detection
KW  - Deep Learning
KW  - Transfer Learning
KW  - Explainable AI
KW  - Gradient based Explainable AI
DO  - 10.1109/ICCIT64611.2024.11022335
JO  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 2474-9656
VO  - 
VL  - 
JA  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
Y1  - 20-22 Dec. 2024
AB  - Leaf diseases are harmful conditions that affect the health, appearance and productivity of plants, leading to significant plant loss and negatively impacting farmers’ livelihoods. These diseases cause visible symptoms such as lesions, color changes, and texture variations, making it difficult for farmers to manage plant health, especially in large or remote farms where expert knowledge is limited. The main motivation of this study is to provide an efficient and accessible solution for identifying plant leaf diseases in Bangladesh, where agriculture plays a critical role in food security. The objective of our research is to classify 21 distinct leaf diseases across six plants using deep learning models, improving disease detection accuracy while reducing the need for expert involvement. Deep Learning (DL) techniques, including CNN and Transfer Learning (TL) models like VGG16, VGG19, MobileNetV2, InceptionV3, ResNet50V2 and Xception are used. VGG19 and Xception achieve the highest accuracies, with 98.90% and 98.66% respectively. Additionally, Explainable AI (XAI) techniques such as GradCAM, GradCAM++, LayerCAM, ScoreCAM and FasterScoreCAM are used to enhance transparency by highlighting the regions of the models focused on during disease classification. This transparency ensures that farmers can understand the model’s predictions and take necessary action. This approach not only improves disease management but also supports farmers in making informed decisions, leading to better plant protection and increased agricultural productivity.
ER  - 

TY  - CONF
TI  - Comparative Study of Object Recognition Utilizing Machine Learning Techniques
T2  - 2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)
SP  - 726
EP  - 731
AU  - T. Sarkar
AU  - M. Rakhra
AU  - V. Sharma
AU  - S. Takkar
AU  - K. Jairath
PY  - 2024
KW  - Productivity
KW  - Explainable AI
KW  - Profitability
KW  - Decision making
KW  - Transfer learning
KW  - Machine learning
KW  - Complexity theory
KW  - Object Detection
KW  - object identification
KW  - asset recognition
KW  - Machine Learning
KW  - Artificial Intelligence
KW  - identification systems
KW  - device identification
DO  - 10.1109/IC3SE62002.2024.10593475
JO  - 2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)
Y1  - 9-11 May 2024
AB  - Machine learning is an essential discipline in artificial intelligence & image processing because it affects item/object or asset recognition or identification processes. It employs datasets, evaluation measures, and both traditional and deep learning techniques. Applications demonstrate the impact of machine learning in a variety of sectors, including medical image processing and driver-less cars. There is a scarcity of rigorous assessments of all the latest technologies utilized in the fields of Artificial Intelligence & Machine Learning. This study demonstrates the variety of object detection algorithms available in this field. This publication will be useful for scholars researching in this area because it provides comprehensive information regarding future directions.
ER  - 

TY  - CONF
TI  - Analysis of Decrease in Accuracy of Two-tier Trees without Using Feature Selection
T2  - 2021 14th International Conference Management of large-scale system development (MLSD)
SP  - 1
EP  - 5
AU  - S. Saltykov
PY  - 2021
KW  - Correlation
KW  - Conference management
KW  - Feature extraction
KW  - Large-scale systems
KW  - Decision trees
KW  - explainable artificial intelligence
KW  - CART
KW  - complementary features
KW  - feature selection
DO  - 10.1109/MLSD52249.2021.9600173
JO  - 2021 14th International Conference Management of large-scale system development (MLSD)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 14th International Conference Management of large-scale system development (MLSD)
Y1  - 27-29 Sept. 2021
AB  - It has been shown in previous papers that traditional approaches to feature selection can lead to learning a two-tier decision tree, which significantly loses in accuracy to a enumerate of all non-empty subsets of features containing no more than three features. In this paper, on a real, public dataset, we calculated how often such an accuracy loss can occur. It is shown that there is a statistically significant correlation between the coefficient of determination of a two-tier decision tree, constructed without any feature selection, and the magnitude of such a loss in accuracy. The characteristic values for the loss of accuracy in the construction of a two-tier tree without any feature selection, compared to a full enumeration of all nonempty subsets of features containing no more than three features, for a real dataset in some domain are calculated.
ER  - 

TY  - CONF
TI  - AgriUXE: Integrating Explainable AI and Multimodal Data for Smart Agriculture
T2  - 2024 International Symposium on Sensing and Instrumentation in 5G and IoT Era (ISSI)
SP  - 1
EP  - 6
AU  - R. P. Porfírio
AU  - R. N. Madeira
AU  - P. A. Santos
PY  - 2024
KW  - Smart agriculture
KW  - Explainable AI
KW  - Instruments
KW  - Decision making
KW  - Mixed reality
KW  - Digital twins
KW  - Stakeholders
KW  - Remote sensing
KW  - Intelligent sensors
KW  - Farming
KW  - Artificial intelligence
KW  - Explainable AI
KW  - Smart agriculture
KW  - Multimodal sensors
KW  - Human computer interaction
DO  - 10.1109/ISSI63632.2024.10720487
JO  - 2024 International Symposium on Sensing and Instrumentation in 5G and IoT Era (ISSI)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2024 International Symposium on Sensing and Instrumentation in 5G and IoT Era (ISSI)
Y1  - 29-30 Aug. 2024
AB  - The increasing demand for sustainable and efficient agricultural practices has driven the adoption of digital technologies in the sector. Despite the persistent lack of transparency in machine learning models, farmers often perceive smart agriculture solutions as complex and of limited value. To address this challenge, we present AgriUXE (Agricultural eXperience Enhanced through eXplainability), a digital platform that combines multimodal data with explainable artifical intelligence (XAI) techniques to provide user-adaptive explanations to drive decision-making processes in smart farming solutions. Given data collected from multimodal sources, such as Internet-of-Things (IoT) sensors and remote sensing, the platform intends to generate comprehensible insights and predictions tailored to the needs of different farm stakeholders. AgriUXE has the potential to integrate with various applications, including mixed reality and digital twin solutions, showcasing its ability to improve the overall farming experience through improved transparency, and decision-making. Additionally, this paper details the functionalities of AgriDash, a practical application for enhanced viticulture data collection, prediction, and visualization.
ER  - 

TY  - CONF
TI  - Importance Of AI In Acoustic Emission Testing and Enhancement Provided Through Compound AI
T2  - 2025 7th International Congress on Human-Computer Interaction, Optimization and Robotic Applications (ICHORA)
SP  - 1
EP  - 6
AU  - Z. Uddin
AU  - S. Dindar
PY  - 2025
KW  - Adaptation models
KW  - Accuracy
KW  - Computational modeling
KW  - Acoustic emission
KW  - Rail transportation
KW  - Real-time systems
KW  - Compounds
KW  - Artificial intelligence
KW  - Monitoring
KW  - Testing
KW  - transportation
KW  - railway engineering
KW  - artificial intelligence
KW  - acoustic emission testing
DO  - 10.1109/ICHORA65333.2025.11017232
JO  - 2025 7th International Congress on Human-Computer Interaction, Optimization and Robotic Applications (ICHORA)
IS  - 
SN  - 2996-4393
VO  - 
VL  - 
JA  - 2025 7th International Congress on Human-Computer Interaction, Optimization and Robotic Applications (ICHORA)
Y1  - 23-24 May 2025
AB  - Acoustic emission has evolved as one of the essential non-destructive testing methods for railway track monitoring. The growing complexity of railway infrastructure, artificial intelligence (AI) is transforming AE testing by the automation of defect detection and classification. This paper presents a extensive review of literature on the use of AI techniques for AE testing, assessing their advantages, limitations, and the promise of compound AI models. Standalone methods like Artificial Neural ANN (Artificial Neural Networks), SVM (Support Vector Machines), CNN (Convolutional Neural Networks) offer an efficient feature extraction and classification but suffers away from data dependency, computation costs, and temporal analysis. Compound models—like CNN with Combine LSTM or Random Forest with CNN—improve accuracy by taking advantage of complementary strengths. The AI for AE testing futures research will focus on integrating for better transparency and decision making — such as Explainable AI (XAI). AI-driven AE Real time monitoring, reduced manual intervention, and better defect prediction are provided by testing. enhancing safety and efficiency within rail networks.
ER  - 

TY  - CONF
TI  - Explainable AI for CPS-Based Manufacturing Workcell
T2  - 2023 International Conference on System Science and Engineering (ICSSE)
SP  - 332
EP  - 337
AU  - R. -J. Soon
AU  - D. V. Sang
AU  - C. -B. Chng
AU  - C. -K. Chui
PY  - 2023
KW  - Deep learning
KW  - Process control
KW  - Quality control
KW  - Cyber-physical systems
KW  - Mathematical models
KW  - Manufacturing
KW  - Reliability
KW  - explainable AI
KW  - cyber-physical system
KW  - manufacturing
DO  - 10.1109/ICSSE58758.2023.10227195
JO  - 2023 International Conference on System Science and Engineering (ICSSE)
IS  - 
SN  - 2325-0925
VO  - 
VL  - 
JA  - 2023 International Conference on System Science and Engineering (ICSSE)
Y1  - 27-28 July 2023
AB  - This paper discusses the application of Explainable AI (XAI) for a Cyber-Physical System (CPS)-based manufacturing workcell. XAI involves providing interpretable outputs from deep learning models. CPSs are systems that integrate interacting analogue, physical and human inputs, and have applications in control optimisation, automation and process intelligence. Current state of the art and gaps in XAI and CPSs are introduced. The need for CPSs with in-built logic to correct and reduce dependency on conventional mathematical modelling of physical systems is identified, together with how the in-built logic must be explainable and understandable to build trust in the logic. This is especially true for applications where manufacturing accuracy and precision are critical. Whilst there have been advances in both XAI and CPSs in recent years, little progress has been made in integrating the two. We show that XAI can increase the efficiency and reliability of CPS-based manufacturing. XAI in CPS can ensure that manufacturing validation inputs made by deep learning networks within the CPS are understandable and correct. It thus ensures that the CPS outputs have actions that are explainable and logic dependent, with safeguards against malicious intent. This contrasts against conventional CPS which may have learning-based systems with ‘black box’ outputs that are assumed to be correct. Approaches towards XAI and how they can be integrated into a learning-based machine vision CPS manufacturing workcell are described. An analysis of the effectiveness of these approaches and their feasibility of implementation in such a CPS is performed. A recommendation of what is the most promising approach towards implementing XAI in the CPS manufacturing of a mass-market, specialised and personalised product, is made as a conclusion.
ER  - 

TY  - CONF
TI  - Enhancing Property and Casualty Insurance Security with Cloud-Based AI Solutions
T2  - 2025 4th International Conference on Computational Modelling, Simulation and Optimization (ICCMSO)
SP  - 160
EP  - 167
AU  - S. R. Adavelli
AU  - R. T. Madhala
AU  - N. Rahul
PY  - 2025
KW  - Accuracy
KW  - Quantum computing
KW  - Explainable AI
KW  - Computational modeling
KW  - Insurance
KW  - Real-time systems
KW  - Fraud
KW  - Risk management
KW  - Security
KW  - Reliability
KW  - Property
KW  - Casualty
KW  - Insurance
KW  - Security
KW  - Cloud
KW  - AI
KW  - FL
KW  - QNN
KW  - XAI
DO  - 10.1109/ICCMSO67468.2025.00037
JO  - 2025 4th International Conference on Computational Modelling, Simulation and Optimization (ICCMSO)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 4th International Conference on Computational Modelling, Simulation and Optimization (ICCMSO)
Y1  - 20-22 June 2025
AB  - The Property and Casualty (P&C) insurance industry faces challenges such as fraudulent claims, slow processing times, and inaccurate risk assessments. Traditional Artificial Intelligence (AI) models improve efficiency but struggle with adaptability in handling complex claims and evolving risks. To address these issues, we propose a Cloud-Integrated Hybrid AI Framework (CI-HAF) that combines Federated Learning (FL), Quantum Neural Networks (QNNs), Explainable AI (XAI), and cloud services for intelligent risk assessment and automated claims processing. FL enables decentralized learning across insurers while maintaining data privacy, QNNs enhance predictive accuracy by leveraging quantum-inspired computing for complex data patterns, and XAI improves transparency and regulatory compliance. The cloud infrastructure provides scalable storage, real-time data processing, and seamless model deployment, ensuring faster and more efficient insurance operations. The proposed CI-HAF model is evaluated using real-world insurance datasets and benchmarked against conventional AI approaches. Performance metrics include claim processing time, fraud detection accuracy, precision-recall trade-off, and computational efficiency. Experimental results indicate that CI-HAF improves fraud detection accuracy by 15 %, reduces claim processing time by 45 %, and enhances risk prediction reliability. By integrating AI, cloud services, and quantum-driven learning, this study establishes a transformative approach for $\mathrm{P} & \mathrm{C}$ insurance, ensuring secure, scalable, and highly efficient claims processing and risk assessment.
ER  - 

TY  - CONF
TI  - Credit Risk Assessment using Ensemble Models and Explainable AI
T2  - 2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)
SP  - 1505
EP  - 1511
AU  - R. Srikanteswara
AU  - K. Naghera
AU  - S. B. Kukkaje
AU  - A. Kumar
AU  - S. P
PY  - 2025
KW  - Explainable AI
KW  - Computational modeling
KW  - Scalability
KW  - Predictive models
KW  - Data models
KW  - Risk management
KW  - Ensemble learning
KW  - Stakeholders
KW  - Streams
KW  - Synthetic data
KW  - Credit Risk and Machine Learning Algorithms
KW  - Ensemble Learning and Risk Interpretation in Financial Institutions
KW  - Explainable AI and Credit Risk Management
KW  - Risk Models and Credit Risk
DO  - 10.1109/IDCIOT64235.2025.10914916
JO  - 2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)
Y1  - 5-7 Feb. 2025
AB  - New challenges are still there in the credit risk management in this context of the financial system, primarily because of the lack of good models for prognosis as well as low interpretability. To address this issue, this research brings together CatBoost, XGBoost, and LightGBM to improve the prospect of identifying high-risk borrowers. A minimal level of Explainable AI (XAI) is utilized to make confirmations that the model follows them and to build confidence in the outcome. Moreover, based on a difference to basic models, the obtained ROC-AUC score equals 0.9693, proving the feasibility of the proposed ensemble approach in questions of impact developing default risks. The end product is therefore a robust method with ensemble modeling and compliant with regulatory requirements in operational financial environments.
ER  - 

TY  - CONF
TI  - Leveraging Artificial Intelligence for Improved Financial Decision-Making in the Indian Mutual Fund Sector
T2  - 2025 First International Conference on Advances in Computer Science, Electrical, Electronics, and Communication Technologies (CE2CT)
SP  - 970
EP  - 975
AU  - T. K. Vadapalli
AU  - S. Kanchi
AU  - S. Joglekar
AU  - V. Phalke
AU  - J. Hareesh
AU  - A. Magar
PY  - 2025
KW  - Economics
KW  - Surveys
KW  - Decision making
KW  - Finance
KW  - Mutual funds
KW  - Predictive models
KW  - Artificial intelligence
KW  - Long short term memory
KW  - Random forests
KW  - Investment
KW  - AI
KW  - Mutual fund
KW  - Financial decision making
DO  - 10.1109/CE2CT64011.2025.10939360
JO  - 2025 First International Conference on Advances in Computer Science, Electrical, Electronics, and Communication Technologies (CE2CT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 First International Conference on Advances in Computer Science, Electrical, Electronics, and Communication Technologies (CE2CT)
Y1  - 21-22 Feb. 2025
AB  - The mutual fund industry in India has grown at an exponential rate, making sophisticated analytical methods necessary for better financial decision-making. This research investigates how machine learning (ML) and artificial intelligence (AI) systems can forecast important mutual fund efficiency indicators, such as returns, volatility, Sharpe ratios, and net asset value (NAV). We use metrics like R2 score, Mean Absolute Error (MAE), and Mean Squared Error (MSE) to assess several systems, including XGBoost, Random Forest, Decision Trees, Linear Regression, and Long Short-Term Memory (LSTM) networks. With the lowest prediction error and a test R2 of 0.93, the results show that deep learning models-in particular, LSTM-perform better than conventional methods. Feature importance analyses also identifies historical returns, AUM, and NAV as important predictors. The results show how AI can improve investment methods, choose funds more wisely, and reduce risks. In India's changing investment environment, this study adds to the expanding area of AI-driven financial decision-making.
ER  - 

TY  - CONF
TI  - Explainable AI-Based ECG Heartbeat Classification Using Deep Learning Models
T2  - 2024 4th International Conference on Artificial Intelligence and Signal Processing (AISP)
SP  - 1
EP  - 5
AU  - R. Moningi
AU  - S. Mahakur
AU  - S. Mundada
AU  - A. Kumar Tripathy
PY  - 2024
KW  - Deep learning
KW  - Accuracy
KW  - Additives
KW  - Heart beat
KW  - Signal processing algorithms
KW  - Electrocardiography
KW  - Signal processing
KW  - Reliability
KW  - Medical diagnosis
KW  - Artificial intelligence
KW  - ECG
KW  - Heartbeat classification
KW  - CNN
KW  - LSTM
KW  - Explainable AI
DO  - 10.1109/AISP61711.2024.10870845
JO  - 2024 4th International Conference on Artificial Intelligence and Signal Processing (AISP)
IS  - 
SN  - 2640-5768
VO  - 
VL  - 
JA  - 2024 4th International Conference on Artificial Intelligence and Signal Processing (AISP)
Y1  - 26-28 Oct. 2024
AB  - The ECG is fundamental in detecting cardiovascular ailments, and its thorough analysis is imperative. The analysis of conventional ECG depends on interpretation based on the expertise of the professional, which is slow and prone to mistakes. Deep learning models, which include CNNs, LSTMs, and Attention techniques introduced recently, have demonstrated promising results in the automation of ECG classification with high accuracy. However, a major drawback regarding these models is the ‘black box’ nature, which goes against clinical usage and comparison. This study adopts SHapley Additive exPlanations (SHAP) values and Local Interpretable Model-agnostic Explanations (LIME) techniques under deep learning algorithms for identifying ECG heartbeats. The model is trained and tested on the MIT-BIH Arrhythmia Database, and the performance seems to be accurate with an overall accuracy of 98.25%. Local Interpretable Model-Agnostic Explanations involving the construction of a local model, such as LIME and Shapely Additive explanations, or SHAP, help to provide more explanation regarding what the model is doing, thus increasing its reliability for clinical use. In this paper, the focus is presented on the experiment's method and outcomes, with a discussion of the interpretability of the final model as a crucial factor in its application in medical diagnostics.
ER  - 

TY  - CONF
TI  - Explainable AI for the Metaverse: A Short Survey
T2  - 2023 International Conference on Intelligent Metaverse Technologies & Applications (iMETA)
SP  - 1
EP  - 6
AU  - C. S. G
AU  - G. Yenduri
AU  - G. Srivastava
AU  - R. M
AU  - D. R. K
AU  - M. Uzair
AU  - T. R. Gadekallu
PY  - 2023
KW  - Surveys
KW  - Ethics
KW  - Solid modeling
KW  - Privacy
KW  - Metaverse
KW  - Navigation
KW  - Shape
KW  - Explainable Artificial Intelligence
KW  - Virtual Reality
KW  - Augmented Reality
KW  - Virtual world
KW  - immersive environment
DO  - 10.1109/iMETA59369.2023.10294907
JO  - 2023 International Conference on Intelligent Metaverse Technologies & Applications (iMETA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Intelligent Metaverse Technologies & Applications (iMETA)
Y1  - 18-20 Sept. 2023
AB  - Virtual reality, augmented reality, and immersive technologies have advanced rapidly, giving rise to the concept of the metaverse. As users delve into these virtual environments, it becomes crucial to understand the decision-making processes of intelligent systems within the metaverse. Explainable AI (XAI) provides a framework for interpreting and understanding the outcomes of artificial intelligence, making it an essential component for ensuring transparency, trust, and user engagement within the metaverse. This paper aims to explore the fusion of XAI in the context of the metaverse, including key enabling technologies, the impact of XAI on metaverse applications, integration challenges, and future directions.
ER  - 

TY  - CONF
TI  - An Empirical Evaluation of Machine Learning Models for Stroke Risk Assessment with Explainable AI
T2  - 2025 International Conference on Artificial Intelligence and Data Engineering (AIDE)
SP  - 150
EP  - 157
AU  - J. V. N. Yashwanth Reddy
AU  - G. Sai Saketh Ram
AU  - V. Varshini
AU  - M. Srinivas
PY  - 2025
KW  - Adaptation models
KW  - Accuracy
KW  - Explainable AI
KW  - Medical services
KW  - Predictive models
KW  - Data models
KW  - Risk management
KW  - Reliability
KW  - Socioeconomics
KW  - Random forests
KW  - Stroke
KW  - Machine Learning
KW  - Explainable AI
DO  - 10.1109/AIDE64228.2025.10987490
JO  - 2025 International Conference on Artificial Intelligence and Data Engineering (AIDE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Artificial Intelligence and Data Engineering (AIDE)
Y1  - 6-7 Feb. 2025
AB  - Stroke has significant socioeconomic effects, particularly in low- and middle-income nations, and is a major cause of mortality and disability worldwide. This study applies machine learning models to predict stroke, leveraging clinical and demographic data to highlight those who may be at risk. We applied a range of models, including Random Forest, XGBoost, Gradient Boosting, and Neural Networks, after comprehensive data preprocessing. Among these, Random Forest achieved the highest accuracy (98.87%) and ROC AUC (1.0), making it highly effective for stroke risk prediction. We improved transparency in healthcare applications by emphasizing the relevance of individual risk factors in forecasts using LIME for model interpretability.. These findings support the use of ensemble models and explainable AI for reliable, interpretable stroke prediction, paving the way for preventive healthcare.
ER  - 

TY  - JOUR
TI  - Human-Centered AI in Smart Farming: Toward Agriculture 5.0
T2  - IEEE Access
SP  - 62199
EP  - 62214
AU  - A. Holzinger
AU  - I. Fister
AU  - I. Fister
AU  - H. -P. Kaul
AU  - S. Asseng
PY  - 2024
KW  - Artificial intelligence
KW  - Meteorology
KW  - Smart agriculture
KW  - Task analysis
KW  - Fifth Industrial Revolution
KW  - Life sciences
KW  - Ethics
KW  - Human factors
KW  - Farming
KW  - Digital systems
KW  - Human-centered AI
KW  - smart farming
KW  - agriculture 5.0
KW  - digital transformation
KW  - artificial intelligence
DO  - 10.1109/ACCESS.2024.3395532
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - This paper delineates the contemporary landscape, challenges, and prospective developments in human-centred artificial intelligence (AI) within the ambit of smart farming, a pivotal element of the emergent Agriculture 5.0, supplanting Agriculture 4.0. Analogous to Industry 4.0, agriculture has witnessed a trend towards comprehensive automation, often marginalizing human involvement. However, this approach has encountered limitations in agricultural contexts for various reasons. While AI’s capacity to assume human tasks is acknowledged, the inclusion of human expertise and experiential knowledge (human-in-the-loop) often proves indispensable, corroborated by the Moravec’s Paradox: tasks simple for humans are complex for AI. Furthermore, social, ethical, and legal imperatives necessitate human oversight of AI, a stance strongly reflected in the European Union’s regulatory framework. Consequently, this paper explores the advancements in human-centred AI focusing on their application in agricultural processes. These technological strides aim to enhance crop yields, minimize labor and resource wastage, and optimize the farm-to-consumer supply chain. The potential of AI to augment human decision-making, thereby fostering a sustainable, efficient, and resilient agri-food sector, is a focal point of this discussion - motivated by the current worldwide extreme weather events. Finally, a framework for Agriculture 5.0 is presented, which balances technological prowess with the needs, capabilities, and contexts of human stakeholders. Such an approach, emphasizing accessible, intuitive AI systems that meaningfully complement human activities, is crucial for the successful realization of future Agriculture 5.0.
ER  - 

TY  - CHAP
TI  - Innovations in Distributed Computing for Enhanced Risk Management in Finance
T2  - Meta-Heuristic Algorithms for Advanced Distributed Systems
SP  - 341
EP  - 357
AU  - Venkateswararao Podile
AU  - Syed M. Faisal
AU  - Gangu N. Mandala
AU  - Shaik Altaf
AU  - Nayana Harshitha
AU  - Cheedella A.S. Lakshmi
AU  - Chunduru R. Chandan
PY  - 2024
KW  - Distributed computing
KW  - Risk management
KW  - Finance
KW  - Distributed databases
KW  - Scalability
KW  - Data processing
KW  - Technological innovation
DO  - 10.1002/9781394188093.ch20
PB  - Wiley
SN  - 9781394188079
UR  - http://ieeexplore.ieee.org/document/10473770
AB  - This chapter explores the use of distributed computing technology for enhancing risk management in finance. The performance of traditional risk management systems is compared to that of distributed computing&#x2010;based risk management systems, and the objectives of the chapter are established. The theoretical framework of distributed computing is discussed, including an overview of its types and technologies for finance risk management. A comparison of distributed computing approaches for finance risk management is presented, and the innovations in distributed computing for enhanced risk management in finance are highlighted. These include advancements in distributed computing architectures and frameworks for finance risk management, new techniques, and emerging trends in distributed computing for finance risk management. Challenges and limitations of distributed computing for finance risk management are identified, including technical challenges in implementing distributed computing and limitations of the technology. Finally, best practices and recommendations for implementing distributed computing in finance risk management are presented. This includes a framework for selecting the right distributed computing approach for finance risk management as well as guidelines for implementation. This chapter provides a comprehensive overview of the use of distributed computing technology in finance risk management, highlighting its benefits, limitations, and best practices.
ER  - 

TY  - JOUR
TI  - Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems
T2  - IEEE Transactions on Wireless Communications
SP  - 1
EP  - 1
AU  - N. Khan
AU  - A. Abdallah
AU  - A. Celik
AU  - A. M. Eltawil
AU  - S. Coleri
PY  - 2025
KW  - Robustness
KW  - Millimeter wave communication
KW  - Training
KW  - Data models
KW  - Explainable AI
KW  - Systematics
KW  - Feature extraction
KW  - Discrete Fourier transforms
KW  - Communication system security
KW  - Accuracy
KW  - Digital twins
KW  - explainable AI
KW  - robustness
KW  - beam alignment
KW  - millimeter-wave (mmWave) communications
KW  - multiple-input multiple-output (MIMO)
DO  - 10.1109/TWC.2025.3596804
JO  - IEEE Transactions on Wireless Communications
IS  - 
SN  - 1558-2248
VO  - 
VL  - 
JA  - IEEE Transactions on Wireless Communications
Y1  - 
AB  - In line with the AI-native 6G vision, explainability and robustness are crucial for building trust and ensuring reliable performance in millimeter-wave (mmWave) systems. Efficient beam alignment is essential for initial access, but deep learning (DL) solutions face challenges, including high data collection overhead, hardware constraints, lack of explainability, and susceptibility to adversarial attacks. This paper proposes a robust and explainable DL-based beam alignment engine (BAE) for mmWave multiple-input multiple-output (MIMO) systems. The BAE uses received signal strength indicator (RSSI) measurements from wide beams to predict the best narrow beam, reducing the overhead of exhaustive beam sweeping. To overcome the challenge of real-world data collection, this work leverages a site-specific digital twin (DT) to generate synthetic channel data closely resembling real-world environments. A model refinement via transfer learning is proposed to fine-tune the pre-trained model residing in the DT with minimal real-world data, effectively bridging mismatches between the digital replica and real-world environments. To reduce beam training overhead and enhance transparency, the framework uses deep Shapley additive explanations (SHAP) to rank input features by importance, prioritizing key spatial directions and minimizing beam sweeping. It also incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a credibility metric for detecting out-of-distribution inputs and ensuring robust, transparent decision-making. Experimental results show that the proposed framework reduces real-world data needs by 70%, beam training overhead by 62%, and improves outlier detection robustness by up to 8.5×, achieving near-optimal spectral efficiency and transparent decision making compared to traditional softmax based DL models.
ER  - 

TY  - JOUR
TI  - Explainable AI for Spectral Analysis of Electromagnetic Fields
T2  - IEEE Access
SP  - 113407
EP  - 113427
AU  - D. Kalatzis
AU  - A. Ploussi
AU  - E. Spyratou
AU  - T. Panagiotakopoulos
AU  - E. P. Efstathopoulos
AU  - Y. Kiouvrekis
PY  - 2025
KW  - Accuracy
KW  - Telephony
KW  - Electromagnetic fields
KW  - Deep learning
KW  - Base stations
KW  - Wireless communication
KW  - Predictive models
KW  - Estimation
KW  - Random forests
KW  - Frequency measurement
KW  - Explainable artificial intelligence (XAI)
KW  - AI
KW  - machine learning
KW  - electromagnetic propagation
DO  - 10.1109/ACCESS.2025.3583944
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 13
VL  - 13
JA  - IEEE Access
Y1  - 2025
AB  - This study explores the application of Explainable Artificial Intelligence (XAI) techniques to the spectral analysis of electromagnetic field (EMF) measurements conducted across various frequency bands in Cyprus. Using an extensive data set obtained from mobile telephony, radio broadcasting, and terrestrial digital television sources, this work aims to capture real-world EMF exposure levels spanning frequencies from 30 MHz to 6 GHz. The frequency bands analyzed include FM radio (87.5-108 MHz), VHF, UHF television, and various mobile telephony zones (700 MHz, 800 MHz, 900 MHz, 1800 MHz, 2100 MHz, 2600 MHz, and 3600 MHz), representing both traditional and emerging wireless technologies. A comparative evaluation of six machine learning algorithms was conducted: XGBoost, LightGBM, Random Forests, k-Nearest Neighbors, Neural Networks and Decision Trees to assess prediction performance across each frequency band. Furthermore, SHAP (SHapley Additive exPlanations) was employed to elucidate the contribution of spatial and demographic characteristics to the intensity of the EMF. The results show that ensemble tree-based methods, particularly Random Forests and LightGBM, consistently outperformed simpler models in accuracy and interpretability. The integration of SHAP enabled transparent feature attribution, revealing distinctive exposure patterns linked to urban topology, population density, and built environment metrics. This approach enables data-driven EMF exposure mapping, aiding urban infrastructure planning, compliance monitoring, and public health risk evaluation.
ER  - 

TY  - CONF
TI  - Comparative Study of Two Feature Selection-Based Risk Assessment Models for Industrial Software
T2  - 2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)
SP  - 1275
EP  - 1279
AU  - Y. Gong
AU  - Y. Jiang
PY  - 2025
KW  - Support vector machines
KW  - Adaptation models
KW  - Automation
KW  - Explainable AI
KW  - Supply chains
KW  - Refining
KW  - Software
KW  - Real-time systems
KW  - Topology
KW  - Risk management
KW  - Risk Assessment
KW  - Industrial Software
KW  - AHP-FCE
KW  - Support Vector Machine (SVM)
DO  - 10.1109/AIITA65135.2025.11048119
JO  - 2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)
Y1  - 28-30 March 2025
AB  - China dominates global manufacturing but lags in industrial software, exposing critical vulnerabilities amid rising risks like supply chain attacks and geopolitical disruptions. This study constructs a three-tier risk system with 20 metrics, including open-source shutdowns and data leaks. A novel SVM model is proposed, using hybrid kernels to capture nonlinear interactions and supply chain topologies, optimized via Bayesian methods. Evaluated on industrial datasets, SVM-ISRA outperforms AHP-FCE. While excelling in automation and dynamic adaptation, SVM-ISRA's “black-box” nature necessitates hybrid frameworks integrating expert knowledge for high-stakes compliance. The work demonstrates SVM's superiority in complex risk environments while advocating explainable AI extensions to bridge data-driven insights with hierarchical risk structures.
ER  - 

TY  - CONF
TI  - Exploring Artificial Intelligence & Machine Learning Applications in Structural Engineering Trends and Challenges
T2  - 2025 International Conference on Next Generation Information System Engineering (NGISE)
SP  - 1
EP  - 6
AU  - R. K. Varshney
AU  - N. K. Pandey
AU  - A. K. Pandey
AU  - A. N. Singh
PY  - 2025
KW  - Manuals
KW  - Predictive models
KW  - Market research
KW  - Data models
KW  - Structural engineering
KW  - Iterative methods
KW  - Artificial intelligence
KW  - Monitoring
KW  - Next generation networking
KW  - Lenses
KW  - structural engineering
KW  - Artificial Intelligence
KW  - AI applications
KW  - predictive modeling
KW  - health monitoring
KW  - challenges
DO  - 10.1109/NGISE64126.2025.11085231
JO  - 2025 International Conference on Next Generation Information System Engineering (NGISE)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2025 International Conference on Next Generation Information System Engineering (NGISE)
Y1  - 28-29 March 2025
AB  - The primary focus in structural engineering is to create designs that are more efficient and secure while also advancing sustainable solutions. It is essential to adopt innovative approaches in design, analysis, and management as constructions become increasingly complex and projects expand in scale. Consequently, structural engineering today is progressively recognizing the potential for AI to bring about significant changes. The dominant approaches to design have largely been iterative, with certain cases involving the application of manual calculations. Nonetheless, it is essential to adopt a more analytical perspective in light of contemporary frameworks and transformations across different sectors. Artificial intelligence and its four types are widely utilized today, encompassing machine learning, data analytics, and cognitive computing. The various aspects of structural engineering can be enhanced through the analysis of computational data, the identification of patterns within extensive data sets, and the formulation of unique decisions. Consequently, this paper examines the various applications of structural engineering through the lens of AI. Fields including predictive modeling, health monitoring, seismic analysis, construction, and various other domains are encompassed within the scope of the examination. Nonetheless, certain drawbacks must be acknowledged, such as data limitations, restricted model interpretability, and resource constraints. Nonetheless, suggestions for tackling the issues have been presented, including explainable AI, data synthesis, and integrated educational models. The study aims to deliver comprehensive insights into the influence of AI on structural engineering, emphasizing its role in fostering the creation of safer and environmentally friendly structures.
ER  - 

TY  - CONF
TI  - SHAP-Based Explainable Machine Learning Framework for Interpreting SHCC Mix Design Parameters
T2  - 2025 5th International Conference on Emerging Smart Technologies and Applications (eSmarTA)
SP  - 1
EP  - 7
AU  - M. A. Al-Hammadi
AU  - Z. Al-Huda
AU  - R. N. Ali Algburi
AU  - M. A. Al-antari
PY  - 2025
KW  - Knowledge engineering
KW  - Fly ash
KW  - Machine learning algorithms
KW  - Cement industry
KW  - Machine learning
KW  - Transforms
KW  - Prediction algorithms
KW  - Concrete
KW  - Design optimization
KW  - Biomedical imaging
KW  - COVID-19
KW  - Artificial Intelligence
KW  - Medical Image Classification
KW  - Diversity Medical Dataset
KW  - Generalization Smart Solution
DO  - 10.1109/eSmarTA66764.2025.11131699
JO  - 2025 5th International Conference on Emerging Smart Technologies and Applications (eSmarTA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 5th International Conference on Emerging Smart Technologies and Applications (eSmarTA)
Y1  - 5-6 Aug. 2025
AB  - Artificial intelligence advancements have generated significant potential for concrete mix design optimization; however, translating machine learning predictions into practical engineering knowledge remains challenging. This investigation presents an explainable machine learning framework that addresses this limitation through application to strain-hardening cementitious composites (SHCC). Utilizing Shapley Additive exPlanations (SHAP) methodology, an analysis of 266 experimental samples incorporating various mineral admixtures and fiber types was conducted. The CatBoost algorithm demonstrated superior predictive capability (R2=0.957) while facilitating transparent interpretation of parametric relationships. SHAP analysis identified cement content as the primary determinant of compressive strength development, with water-to-binder ratio and fly ash content constituting secondary influential factors. Experimental results indicated optimal compressive strength was achieved with water-to-binder ratios not exceeding 0.3, fly ash content approximately 600 kg/m³, and fiber volume fractions between 0.2-0.3%. Additionally, the investigation revealed that shorter fibers with larger diameters provided enhanced contribution to compressive strength development. These findings extend the fundamental understanding of SHCC behavior while establishing quantitative guidelines for sustainable mix design optimization. The methodological framework developed herein effectively transforms complex algorithmic outputs into practical engineering knowledge, providing valuable analytical capabilities for both academic researchers and industry practitioners in cementitious materials development.
ER  - 

TY  - CONF
TI  - RA3: A Human-in-the-loop Framework for Interpreting and Improving Image Captioning with Relation-Aware Attribution Analysis
T2  - 2024 IEEE 40th International Conference on Data Engineering (ICDE)
SP  - 330
EP  - 341
AU  - L. Chai
AU  - L. Qi
AU  - H. Sun
AU  - J. Li
PY  - 2024
KW  - Analytical models
KW  - Annotations
KW  - Computational modeling
KW  - Semantics
KW  - Optimization methods
KW  - Debugging
KW  - Human in the loop
KW  - Crowdsourcing
KW  - human-in-the-loop
KW  - Human-centered explainable AI (HCXAI)
KW  - Explanation-guided learning
KW  - Image captioning
DO  - 10.1109/ICDE60146.2024.00032
JO  - 2024 IEEE 40th International Conference on Data Engineering (ICDE)
IS  - 
SN  - 2375-026X
VO  - 
VL  - 
JA  - 2024 IEEE 40th International Conference on Data Engineering (ICDE)
Y1  - 13-16 May 2024
AB  - Interpreting model behavior is crucial for model evaluation and optimization. Recent research demonstrates that incorporating human intelligence into the learning process effectively improve the interpretability and performance of the machine learning models, especially for simple classification tasks. However, the image captioning task has not received much attention. Such complex sequential tasks generally contain semantic relationships between different concepts, which pose challenges for interpreting model behavior and developing optimization methods. In this paper, we present RA 3 (Relation-Aware Attribution Analysis), a human-in-the-loop framework, for improving the interpretability, and further boosting the performance of the image captioning model. Specifically, we first engage human participants in two types of annotation tasks to identify what the model actually focuses on (model attribution) and what it should focus on (human rationale) at the conceptual level, supported by machine learning interpretability methods. Then, we identify and filter hard instances based on relation-aware model attribution for both validating the quality of the explanation and eliminating low-quality captions (this process is also considered as a kind of data debugging). We subsequently designed an explanation loss that penalizes the difference between model attribution and human rationale to optimize the model's behavior for improving caption quality. Through extensive experiments on crowdsourced annotations and MSCOCO, the experiment results indicate that the explanations produced by RA3 can accurately describe the model's behavior, effectively identify difficult instances, and significantly improve the caption quality.
ER  - 

TY  - JOUR
TI  - Explainable AI for Industry 5.0: Vision, Architecture, and Potential Directions
T2  - IEEE Open Journal of Industry Applications
SP  - 177
EP  - 208
AU  - C. Trivedi
AU  - P. Bhattacharya
AU  - V. K. Prasad
AU  - V. Patel
AU  - A. Singh
AU  - S. Tanwar
AU  - R. Sharma
AU  - S. Aluvala
AU  - G. Pau
AU  - G. Sharma
PY  - 2024
KW  - Fifth Industrial Revolution
KW  - Artificial intelligence
KW  - Surveys
KW  - Fourth Industrial Revolution
KW  - Data models
KW  - Predictive models
KW  - Industries
KW  - Automation
KW  - cobots
KW  - cyber-physical systems (CPSs)
KW  - digital twins (DTs)
KW  - explainable artificial intelligence (EXAI)
KW  - Industry 5.0
DO  - 10.1109/OJIA.2024.3399057
JO  - IEEE Open Journal of Industry Applications
IS  - 
SN  - 2644-1241
VO  - 5
VL  - 5
JA  - IEEE Open Journal of Industry Applications
Y1  - 2024
AB  - The Industrial Revolution has shifted toward Industry 5.0, reinventing the Industry 4.0 operational process by introducing human elements into critical decision processes. Industry 5.0 would present massive customization via transformative technologies, such as cyber-physical systems (CPSs), artificial intelligence (AI), and big data analytics. In Industry 5.0, the AI models must be transparent, valid, and interpretable. AI models employ machine learning and deep learning mechanisms to make the industrial process autonomous, reduce downtime, and improve operational and maintenance costs. However, the models require explainability in the learning process. Thus, explainable AI (EXAI) adds interpretability and improves the diagnosis of critical industrial processes, which augments the machine-to-human explanations and vice versa. Recent surveys of EXAI in industrial applications are mostly oriented toward EXAI models, the underlying assumptions. Still, fewer studies are conducted toward a holistic integration of EXAI with human-centric processes that drives the Industry 5.0 applicative verticals. Thus, to address the gap, we propose a first-of-its-kind survey that systematically untangles EXAI integration and its potential in Industry 5.0 applications. First, we present the background of EXAI in Industry 5.0 and CPSs and a reference EXAI-based Industry 5.0 architecture with insights into large language models. Then, based on the research questions, a solution taxonomy of EXAI in Industry 5.0 is presented, which is ably supported by applicative use cases (cloud, digital twins, smart grids, augmented reality, and unmanned aerial vehicles). Finally, a case study of EXAI in manufacturing cost assessment is discussed, followed by open issues and future directions. The survey is designed to extend novel prototypes and designs to realize EXAI-based real-time Industry 5.0 applications.
ER  - 

TY  - CONF
TI  - Enhancing Occupancy Detection with Explainable AI: Leveraging SHAP and Large Language Models
T2  - 2025 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)
SP  - 1
EP  - 9
AU  - D. Kampelopoulos
AU  - K. Papadopoulos
AU  - A. Tsanousa
AU  - G. Meditskos
AU  - S. Vrochidis
AU  - I. Kompatsiaris
PY  - 2025
KW  - Training
KW  - Meters
KW  - Technological innovation
KW  - Explainable AI
KW  - Large language models
KW  - Natural languages
KW  - Employment
KW  - Predictive models
KW  - Cognition
KW  - Construction industry
KW  - machine learning
KW  - occupancy detection
KW  - model interpretation
KW  - SHAP
KW  - Large Language Models
DO  - 10.1109/ICE/ITMC65658.2025.11106581
JO  - 2025 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)
IS  - 
SN  - 2693-8855
VO  - 
VL  - 
JA  - 2025 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)
Y1  - 16-19 June 2025
AB  - The rapid growth of the field of Artificial Intelligence has increased the utilization of predictive machine learning models in the construction industry. While their capabilities are significant, they often perform in a black-box manner, raising the need for interpretability of these models. This work focuses on the problem of occupancy detection and proposes a generalized explainability approach for common model configurations trained on public occupancy datasets. The data are collected from buildings with multiple sensor modalities, including power meters and environmental sensors. The proposed methodology involves the interpretation of the model predictions via the SHapley Additive exPlanations (SHAP) method and the employment of a Large Language Model (LLM) to provide the reasoning behind the model predictions and the feature contribution in natural language. This work provides generalized prompt instructions along with example responses on multiple scenarios, as well as a discussion on the limitations, common issues that were overcome with minimal prompt-tuning, and the potential for future development of the proposed method.
ER  - 

TY  - CONF
TI  - Next-Gen Healthcare: Explainable AI for Precise Gestational Diabetes Risk Assessment
T2  - 2024 IEEE Globecom Workshops (GC Wkshps)
SP  - 1
EP  - 6
AU  - K. Patel
AU  - K. Patel
AU  - A. Rathod
AU  - A. Nair
AU  - T. Vyas
AU  - S. Desai
PY  - 2024
KW  - Pediatrics
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Real-time systems
KW  - Gestational diabetes
KW  - Risk management
KW  - Stakeholders
KW  - Random forests
KW  - Diseases
KW  - Explainable AI
KW  - gestational diabetes mellitus
KW  - random forest classifier
KW  - SHAP
KW  - LIME
DO  - 10.1109/GCWkshp64532.2024.11100520
JO  - 2024 IEEE Globecom Workshops (GC Wkshps)
IS  - 
SN  - 2166-0077
VO  - 
VL  - 
JA  - 2024 IEEE Globecom Workshops (GC Wkshps)
Y1  - 8-12 Dec. 2024
AB  - Pregnant women might face a silent and dangerous risk known as gestational diabetes mellitus (GDM). It is a disease that presents itself with high blood sugar levels and could carry devastating consequences for both the mother and the baby. Unfortunately, it often goes undetected until it’s too late, resulting in potentially life-threatening complications such as preterm birth and congenital disabilities. After thoroughly comparing and analyzing machine learning (ML) models, the Random Forest Classifier was selected with a high accuracy of 91%. Previous research works in this domain lack interpretability by incorporating explainable artificial intelligence (XAI) techniques like Shapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), transparency and trust among clients and stakeholders increases by showcasing the reasoning behind each decision. This approach allows healthcare professionals to perform more accurate risk assessments.
ER  - 

TY  - CONF
TI  - Explainable AI-Enhanced Deep Learning for Pumpkin Leaf Disease Detection: A Comparative Analysis of CNN Architectures
T2  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
SP  - 2428
EP  - 2433
AU  - M. A. Alam Khandaker
AU  - Z. S. Raha
AU  - S. Islam
AU  - T. Muhammad
PY  - 2024
KW  - Deep learning
KW  - Training
KW  - Productivity
KW  - Visualization
KW  - Accuracy
KW  - Explainable AI
KW  - Computer architecture
KW  - Predictive models
KW  - Diseases
KW  - Residual neural networks
KW  - Pumpkin leaf detection
KW  - Deep Learning
KW  - CNN Architecture
KW  - Explainable AI
DO  - 10.1109/ICCIT64611.2024.11021957
JO  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 2474-9656
VO  - 
VL  - 
JA  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
Y1  - 20-22 Dec. 2024
AB  - Pumpkin leaf diseases are significant threats to agricultural productivity, requiring a timely and precise diagnosis for effective management. Traditional identification methods are laborious and susceptible to human error, emphasizing the necessity for automated solutions. This study employs on the "Pumpkin Leaf Disease Dataset", that comprises of 2,000 high-resolution images separated into five categories. Downy mildew, powdery mildew, mosaic disease, bacterial leaf spot, and healthy leaves. The dataset was rigorously assembled from several agricultural fields to ensure a strong representation for model training. We explored many proficient deep learning architectures, including DenseNet201, DenseNet121, DenseNet169, Xception, ResNet50, ResNet101 and InceptionResNetV2, and observed that ResNet50 performed most effectively, with an accuracy of 90.5% and comparable precision, recall, and F1-Score. We used Explainable AI (XAI) approaches like Grad-CAM, Grad-CAM++, Score-CAM, and Layer-CAM to provide meaningful representations of model decision-making processes, which improved understanding and trust in automated disease diagnostics. These findings demonstrate ResNet50’s potential to revolutionize pumpkin leaf disease detection, allowing for earlier and more accurate treatments.
ER  - 

TY  - CONF
TI  - A Deep Learning Toolbox for Analog Integrated Circuit Placement
T2  - SMACD / PRIME 2021; International Conference on SMACD and 16th Conference on PRIME
SP  - 1
EP  - 4
AU  - A. Gusmao
AU  - A. Canelas
AU  - N. Horta
AU  - N. Lourenco
AU  - R. Martins
PY  - 2021
DO  - 
JO  - SMACD / PRIME 2021; International Conference on SMACD and 16th Conference on PRIME
IS  - 
SN  - 
VO  - 
VL  - 
JA  - SMACD / PRIME 2021; International Conference on SMACD and 16th Conference on PRIME
Y1  - 19-22 July 2021
AB  - This paper presents a deep learning toolbox, DEEPPLACER, to assist designers during the layout design of analog integrated circuits. DEEPPLACER relies on a simple pair-wise device interaction circuit description, i.e., the circuits’ topological constraints, to propose valid floorplan solutions for block-level structures, including topologies and deep technology nodes not used for its training, at push-button speed. Despite its automatic functionalities, the toolbox is focused on explainable artificial intelligence, involving the designer in the synthesis flow via filtering and editing options over the candidate floorplan solutions. This constant state of human-machine feedback environment turns the designer aware of the impact of each device’s position change and inherent tradeoffs while suggesting subsequent moves, ultimately increasing the designers’ productivity in this time-consuming and iterative task. Finally, DEEPPLACER is shown to instantly generate a floorplan with 61% better constraint fulfilment than a human designed solution.
ER  - 

TY  - CONF
TI  - Enhancing PHM with XAI: Review and Dataset for System Health Indicator Construction
T2  - 2024 8th International Conference on System Reliability and Safety (ICSRS)
SP  - 511
EP  - 517
AU  - D. A. Nguyen
AU  - K. T. P. Nguyen
AU  - K. Medjaher
PY  - 2024
KW  - Degradation
KW  - Explainable AI
KW  - Reviews
KW  - Benchmark testing
KW  - Propulsion
KW  - Predictive models
KW  - Complexity theory
KW  - Safety
KW  - Prognostics and health management
KW  - Usability
KW  - Explainable AI
KW  - Prognostics and Health Management
KW  - Health Indicator
KW  - Predictive Maintenance
KW  - Tennessee Eastman process
DO  - 10.1109/ICSRS63046.2024.10927485
JO  - 2024 8th International Conference on System Reliability and Safety (ICSRS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 8th International Conference on System Reliability and Safety (ICSRS)
Y1  - 20-22 Nov. 2024
AB  - The integration of Explainable Artificial Intelligence (XAI) into Prognostics and Health Management (PHM) holds the promise of transforming maintenance strategies across various industrial sectors by enhancing the interpretability and accuracy of predictive models. Despite progress, integrating XAI into PHM encounters hurdles, such as the absence of consistent methodologies and a unified framework, complicating the integration of these AI techniques into PHM systems. Additionally, the development of interpretable and robust health indicators (HIs) for systems with multiple components is still largely unexplored. It is hindered by the lack of benchmark data that fully represents system complexities, including component interdependencies and the variability of operational conditions. Therefore, this paper aims at addressing these challenges by offering two main contributions: a focused review of XAI in PHM and the introduction of a novel run-to-fail (RTF) dataset from the Tennessee Eastman Process (TEP). This dataset is a leap forward, enabling the detailed monitoring of component degradation and the exploration of component interactions under various environmental conditions. It allows enhancing the development of explainable HIs and prognostics algorithms at the system level to better reflect the challenges faced in industrial settings.
ER  - 

TY  - JOUR
TI  - A Framework to Overcome the Dark Side of Generative Artificial Intelligence (GAI) Like ChatGPT in Social Media and Education
T2  - IEEE Transactions on Computational Social Systems
SP  - 5266
EP  - 5274
AU  - P. W. Kim
PY  - 2024
KW  - Chatbots
KW  - Artificial intelligence
KW  - Taxonomy
KW  - Social networking (online)
KW  - Data models
KW  - Knowledge engineering
KW  - Generative AI
KW  - Algorithm
KW  - ChatGPT
KW  - data
KW  - information
KW  - knowledge
KW  - and work (DIKW) hierarchy
KW  - education
KW  - explainable artificial intelligence (XAI)
KW  - GAI-assisted learning (GAIAL)
KW  - GAI-assisted tasks (GAIATs)
KW  - generative artificial intelligence (GAI)
KW  - social media
DO  - 10.1109/TCSS.2023.3315237
JO  - IEEE Transactions on Computational Social Systems
IS  - 4
SN  - 2329-924X
VO  - 11
VL  - 11
JA  - IEEE Transactions on Computational Social Systems
Y1  - Aug. 2024
AB  - As the performance of generative artificial intelligence (GAI), such as ChatGPT, improves, content created by GAI will be distributed in the social media space, and knowledge and writings from unknown sources will be disseminated and reproduced. Now that GAI is becoming widespread, it is necessary to distinguish GAI from human intelligence, which constitutes knowledge. The data, information, knowledge, and work (DIKW) hierarchy is a useful framework for teaching and for checking metacognitive and explainable artificial intelligence (XAI) literacy. There are two types of collaboration between GAI and human intelligence: a combined intelligence model and a parallel intelligence model. The combined intelligence model is a method of using GAI for creating works by collecting data, organizing information, and deriving knowledge from information. This model is suitable for GAI-assisted tasks (GAIATs). The parallel intelligence model is suitable for GAI-assisted learning (GAIAL); it is a method in which a person develops abilities by analyzing and comparing tasks created by GAI after going through the data-information-knowledge-work process. The zone of proximal development (ZPD) created by educational scaffolding is a quantitative framework that is appropriate for evaluating the effects of GAI. The ZPD generated by GAI that corresponds to scaffolding should be managed so as not to favor or disadvantage specific individuals.
ER  - 

TY  - CONF
TI  - Ensembled Deep Learning Technique for Cardiovascular Disease Detection: A CNN-GRU Approach
T2  - 2024 IEEE 3rd World Conference on Applied Intelligence and Computing (AIC)
SP  - 484
EP  - 488
AU  - T. Soni
AU  - D. Dupta
AU  - M. Uppal
PY  - 2024
KW  - Deep learning
KW  - Pathology
KW  - Accuracy
KW  - Computational modeling
KW  - Multilayer perceptrons
KW  - Feature extraction
KW  - Robustness
KW  - Cardiovascular diseases
KW  - Risk management
KW  - Random forests
KW  - Heart disease
KW  - Deep learning
KW  - Artificial Intelligence
DO  - 10.1109/AIC61668.2024.10730954
JO  - 2024 IEEE 3rd World Conference on Applied Intelligence and Computing (AIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 3rd World Conference on Applied Intelligence and Computing (AIC)
Y1  - 27-28 July 2024
AB  - Innovative early diagnosis and intervention methods are needed to combat cardiovascular disease (CVD), the primary cause of death globally. In recent years, Deep Learning (DL) algorithms have become strong medical image analysis and clinical decision support tools. This presentation covers DL advances and applications in cardiovascular disease detection. Deep learning algorithms especially CNNs did well in medical imaging modalities like computed tomography, angiography, and cardiac magnetic resonance imaging. These algorithms extract features well, allowing accurate heart structure segmentation, abnormality identification, and function evaluation. DL-based methods may also identify subtle anatomical and pathological changes, aiding early diagnosis and risk assessment. Data quality, interpretability, and regulatory compliance must be addressed when using DL models in clinical practice. To assure the dependability and ethics of DL-based cardiovascular care solutions, explainable Artificial Intelligence (AI), federated learning and regulatory frameworks must improve model robustness, transparency and validation.
ER  - 

TY  - CONF
TI  - Symbolic AI for XAI: Evaluating LFIT Inductive Programming for Fair and Explainable Automatic Recruitment
T2  - 2021 IEEE Winter Conference on Applications of Computer Vision Workshops (WACVW)
SP  - 78
EP  - 87
AU  - A. Ortega
AU  - J. Fierrez
AU  - A. Morales
AU  - Z. Wang
AU  - T. Ribeiro
PY  - 2021
KW  - Training
KW  - Machine learning algorithms
KW  - Biometrics (access control)
KW  - Conferences
KW  - Resumes
KW  - Neural networks
KW  - Tools
DO  - 10.1109/WACVW52041.2021.00013
JO  - 2021 IEEE Winter Conference on Applications of Computer Vision Workshops (WACVW)
IS  - 
SN  - 2690-621X
VO  - 
VL  - 
JA  - 2021 IEEE Winter Conference on Applications of Computer Vision Workshops (WACVW)
Y1  - 5-9 Jan. 2021
AB  - Machine learning methods are growing in relevance for biometrics and personal information processing in domains such as forensics, e-health, recruitment, and e-learning. In these domains, white-box (human-readable) explanations of systems built on machine learning methods can become crucial. Inductive Logic Programming (ILP) is a subfield of symbolic AI aimed to automatically learn declarative theories about the process of data. Learning from Interpretation Transition (LFIT) is an ILP technique that can learn a propositional logic theory equivalent to a given blackbox system (under certain conditions). The present work takes a first step to a general methodology to incorporate accurate declarative explanations to classic machine learning by checking the viability of LFIT in a specific AI application scenario: fair recruitment based on an automatic tool generated with machine learning methods for ranking Curricula Vitae that incorporates soft biometric information (gender and ethnicity). We show the expressiveness of LFIT for this specific problem and propose a scheme that can be applicable to other domains.
ER  - 

TY  - JOUR
TI  - Optimal Channel Selection for FY-4B GIIRS Explainable Machine Learning Cloud Detection Algorithm
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 12
AU  - H. Yang
AU  - L. Guan
PY  - 2025
KW  - Clouds
KW  - Satellites
KW  - Atmospheric modeling
KW  - Spatial resolution
KW  - Computational modeling
KW  - Accuracy
KW  - Instruments
KW  - Hyperspectral imaging
KW  - Data models
KW  - Boosting
KW  - Cloud detection
KW  - cloud mask algorithm
KW  - Fengyun 4B (FY-4B)
KW  - Geostationary Interferometric Infrared Sounder (GIIRS)
KW  - machine learning (ML)
KW  - SHapley additive exPlanation (SHAP)
DO  - 10.1109/TGRS.2025.3597266
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 63
VL  - 63
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2025
AB  - Cloud detection is a crucial preliminary step for assimilating meteorological satellite observation and retrieving other atmospheric parameters. This article presents an explainable machine learning (ML) algorithm for cloud detection using observations from the FY-4B Geostationary Interferometric Infrared Sounder (GIIRS). Four ML models—random forest (RF), light gradient boosting machine (LightGBM), categorical boosting (CatBoost), and extreme gradient boosting (XGBoost)—were evaluated first for their effectiveness in cloud detection. The top 250 channels were selected as model inputs after feature importance analysis, which optimizes both computational efficiency and detection accuracy. Among the evaluated models, XGBoost demonstrated superior performance with a detection accuracy of 83.5%. An advanced channel selection strategy based on the SHapley Additive exPlanation (SHAP) analysis is proposed. The recognition accuracy using a subset of fewer 74 channels according to SHAP analysis is comparable with 250. FY-4B GIIRS real case applications have shown that this algorithm can be used operationally to retrieve GIIRS cloud mask products with fast speed and high accuracy. It takes no more than 1 s to do cloud mask for the entire China region. The results demonstrate a strong alignment with the Advanced Geosynchronous Radiation Imager (AGRI) L2 operational cloud mask product and visible channel albedo observations with high spatial resolution. Additionally, the algorithm maintains high detection accuracy even in regions with thin cirrus clouds. Due to the lower spatial resolution of GIIRS, the XGBoost model may classify probably cloud and probably clear areas as clear sky and clear sky areas with some cloud cover as partly cloudy covered. To evaluate its robustness and generalizability, the model was successfully applied to a similar instrument FY-3E/HIRAS-II uploaded on the polar satellite platform. It also demonstrates strong potential for operational application. Furthermore, the model robustness is tested during different seasons. The results revealed that the model was trained using combined seasonal training data (namely, increasing the representativeness of the samples) would enhance cross-seasonal cloud detection performance.
ER  - 

TY  - CONF
TI  - Interpretable Machine Learning for Undeclared Work Prediction
T2  - 2023 14th International Conference on Information, Intelligence, Systems & Applications (IISA)
SP  - 1
EP  - 8
AU  - E. Alogogianni
AU  - M. Virvou
PY  - 2023
KW  - Adaptation models
KW  - Biological system modeling
KW  - Decision making
KW  - Machine learning
KW  - Inspection
KW  - Predictive models
KW  - Data models
KW  - data mining
KW  - interpretable machine learning
KW  - lazy associative classification
KW  - predictive modelling
KW  - class imbalance
KW  - class overlap
KW  - targeting
KW  - risk assessment
KW  - undeclared work
KW  - labour inspectorate
KW  - informal economy
KW  - tax evasion
DO  - 10.1109/IISA59645.2023.10345943
JO  - 2023 14th International Conference on Information, Intelligence, Systems & Applications (IISA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 14th International Conference on Information, Intelligence, Systems & Applications (IISA)
Y1  - 10-12 July 2023
AB  - Machine learning models have vastly proved their contribution to decision-making in many aspects of daily life. Yet, their acceptance is confined, especially in public authorities, due to the lack of trust and reliability in their results, often credited to the shortage of “black box” models' explainability and interpretability. The present study illustrates an application of interpretable machine learning to predict undeclared work and other labour law infringements, using real-life data of past inspections and employment declarations obtained from a Labour Inspectorate. Undeclared work is a severe socioeconomic issue causing significant losses in taxes and social security contributions and undermining competitiveness and people and state welfare, thus rendering its detection and tackling a critical goal for the enforcement authorities in charge. We employ Lazy Associative Classification and encompass data sampling methods to deal with class overlapping and imbalance problems, achieving increased prediction performance, tripling the inspection yields with regard to undeclared work detection, and, in parallel, transparency, offering precise explanations for each prediction, as well as general insights in the prevalence and feature patterns of undeclared work. This research also discusses the multiple benefits of embodying this machine learning approach in an inspection recommendation system for responsible enforcement authorities, with the most significant being simplicity, flexibility, broadened adaptability to the users' diverse and ever-changing needs, and enhanced acceptance and confidence of the users in the system inspection suggestions.
ER  - 

TY  - JOUR
TI  - Actionable Analytics: Stop Telling Me What It Is; Please Tell Me What To Do
T2  - IEEE Software
SP  - 115
EP  - 120
AU  - C. Tantithamthavorn
AU  - J. Jiarpakdee
AU  - J. Grundy
PY  - 2021
KW  - Project management
KW  - Software development management
KW  - Decision making
KW  - Analytical models
DO  - 10.1109/MS.2021.3072088
JO  - IEEE Software
IS  - 4
SN  - 1937-4194
VO  - 38
VL  - 38
JA  - IEEE Software
Y1  - July-Aug. 2021
AB  - The success of software projects depends on complex decision making (e.g., which tasks should a developer do first, who should perform this task, is the software of high quality, is a software system reliable and resilient enough to deploy, etc.). Bad decisions cost money (and reputation) so we need better tools for making better decisions. This article discusses the "why" and "how" of explainable and actionable software analytics. For the task of reducing the risk of software defects, we show initial results from a successful case study that offers more actionable software analytics. Also, we present an interactive tutorial on the subject of Explainable AI tools for SE in our Software Analytics Cookbook (https://xai4se.github.io/book/), and we discuss some open questions that need to be addressed.
ER  - 

TY  - JOUR
TI  - A Deep Learning Approach Considering Image Background for Pneumonia Identification Using Explainable AI (XAI)
T2  - IEEE/ACM Transactions on Computational Biology and Bioinformatics
SP  - 857
EP  - 868
AU  - Y. Yang
AU  - G. Mei
AU  - F. Piccialli
PY  - 2024
KW  - Pulmonary diseases
KW  - Deep learning
KW  - X-ray imaging
KW  - COVID-19
KW  - Analytical models
KW  - Image recognition
KW  - Solid modeling
KW  - Deep learning
KW  - explainable AI
KW  - image background
KW  - pneumonia
KW  - x-ray
DO  - 10.1109/TCBB.2022.3190265
JO  - IEEE/ACM Transactions on Computational Biology and Bioinformatics
IS  - 4
SN  - 1557-9964
VO  - 21
VL  - 21
JA  - IEEE/ACM Transactions on Computational Biology and Bioinformatics
Y1  - July-Aug. 2024
AB  - Pneumonia mainly refers to lung infections caused by pathogens, such as bacteria and viruses. Currently, deep learning methods have been applied to identify pneumonia. However, the traditional deep learning methods for pneumonia identification take less account of the influence of the lung X-ray image background on the model's testing effect, which limits the improvement of the model's accuracy. In this paper, we propose a deep learning method that considers image background factors and analyzes the proposed method with explainable deep learning for explainability. The essential idea is to remove the image background, improve the pneumonia recognition accuracy, and apply the Grad-CAM method to obtain an explainable deep learning model for pneumonia identification. In the proposed approach, (1) preliminary deep learning models for pneumonia X-ray image identification without considering the background are built; (2) deep learning models for pneumonia X-ray image identification with background consideration are built to improve the accuracy of pneumonia identification; (3) Grad-CAM method is employed to analyze the explainability. The proposed approach improves the accuracy of pneumonia identification, and the highest accuracy of VGG16 reaches 95.6%. The proposed approach can be applied to real pneumonia identification for early detection and treatment.
ER  - 

TY  - CONF
TI  - Explainable Feature Learning for Predicting Neonatal Intensive Care Unit (NICU) Admissions
T2  - 2021 IEEE International Conference on Biomedical Engineering, Computer and Information Technology for Health (BECITHCON)
SP  - 69
EP  - 74
AU  - G. Marvin
AU  - M. G. R. Alam
PY  - 2021
KW  - Representation learning
KW  - Support vector machines
KW  - Pediatrics
KW  - Costs
KW  - Predictive models
KW  - Robustness
KW  - Resource management
KW  - Explainable AI (XAI)
KW  - Predictive Medicine
KW  - Perinatal Quality Management
KW  - Maternal-Fetal and Neonatal Medicine
KW  - Feature Learning
KW  - Neonatal Emergencies
KW  - NICU Facility Management
DO  - 10.1109/BECITHCON54710.2021.9893719
JO  - 2021 IEEE International Conference on Biomedical Engineering, Computer and Information Technology for Health (BECITHCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Biomedical Engineering, Computer and Information Technology for Health (BECITHCON)
Y1  - 4-5 Dec. 2021
AB  - Neonatal Intensive Care Units (NICU) service costs are rapidly growing due to the higher resource utilization intensity. This in turn increases the healthcare costs for NICU patients besides the inaccessibility and unpreparedness of both NICU service providers and patient caretakers hence an increase in neonatal mortality and morbidity. There a lot of contributors to NICU admissions but the exiting methods consider very limited features to precisely predict NICU admissions. In this paper, we present a robust Explainable Artificial Intelligence approach that allows machines to interpretably learn from a pool of possible contributing features in order to predict an NICU admission. Our machine learning approach interpretably illustrates the thought process of admission prediction to the physician and patient. This provides transparent and trustable insights for the precise, proactive, personalized and participatory NICU medical diagnostics and treatment plans for the patient. We statistically and visually present Random Forest and Logistic Regression prediction explanations using SHAP, LIME and ELI5 techniques. This predictive technological approach can preventively increase success of maternal and neonatal monitoring and treatment plans. It can also enhance proactive management of NICU facilities (resources) by the responsible facility administrators most especially in resource constrained settings.
ER  - 

TY  - CONF
TI  - Plant Leaf Disease Classification and Identification Using Deep Convolution Neural Network
T2  - 2025 International Conference on Computational, Communication and Information Technology (ICCCIT)
SP  - 312
EP  - 317
AU  - A. Jain
AU  - K. Harshita
AU  - A. Karar
AU  - D. D. S
PY  - 2025
KW  - Productivity
KW  - Precision agriculture
KW  - Deep learning
KW  - Plant diseases
KW  - Temperature distribution
KW  - Accuracy
KW  - Transfer learning
KW  - Crops
KW  - Real-time systems
KW  - Optimization
KW  - CNN
KW  - Dataset
KW  - Plant Village
DO  - 10.1109/ICCCIT62592.2025.10927927
JO  - 2025 International Conference on Computational, Communication and Information Technology (ICCCIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Computational, Communication and Information Technology (ICCCIT)
Y1  - 7-8 Feb. 2025
AB  - With the advancement of deep learning, and CNNs in particular, comes the ability to correctly classify plant diseases automatically with increased accuracy. This work provides a robust system by adopting CNN architectures and employing the application of transfer learning toward leaf disease classification in plants using diverse datasets, with proper preprocessing and augmentation. Deployed as a user-friendly application, the system demonstrates significant potential in enhancing crop productivity and minimizing losses through early disease detection.
ER  - 

TY  - CONF
TI  - Comparative Analysis of LIME and Shape Analysis Techniques in Prostate Cancer MRI Interpretation
T2  - 2024 International Conference on Engineering and Emerging Technologies (ICEET)
SP  - 1
EP  - 6
AU  - M. R. Hassan
AU  - M. M. Hassan
AU  - M. A. Rahman
PY  - 2024
KW  - Deep learning
KW  - Analytical models
KW  - Shape
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Decision making
KW  - Predictive models
KW  - Reliability
KW  - Artificial intelligence
KW  - Prostate cancer
KW  - Prostate cancer
KW  - MRI
KW  - machine learning
KW  - LIME
KW  - shape analysis
KW  - diagnostic accuracy
DO  - 10.1109/ICEET65156.2024.10913801
JO  - 2024 International Conference on Engineering and Emerging Technologies (ICEET)
IS  - 
SN  - 2831-3682
VO  - 
VL  - 
JA  - 2024 International Conference on Engineering and Emerging Technologies (ICEET)
Y1  - 27-28 Dec. 2024
AB  - Advancements in artificial intelligence (AI) are enabling the integration of explainable AI (XAI) models in Magnetic Resonance Imaging (MRI) to improve diagnostic accuracy and personalize prostate cancer treatment. This paper explores the effectiveness of two XAI techniques, Local Interpretable Model-agnostic Explanations (LIME) and shape analysis, in interpreting prostate cancer MRI datasets. A central focus is the necessary validation to ensure these models provide reliable and actionable diagnostic insights. However, the persistent “black-box problem,” which refers to the lack of transparency and interpretability in AI decision-making, has led to clinical skepticism regarding deep learning applications. Since most AI models produce results without clearly explaining their internal processes, radiologists may need help understanding the rationale behind predictions, limiting AI's role as a decision-support tool. In this study, we stress the need for validation to ensure that XAI-generated explanations are reliable and clinically useful. Their findings are intended to help integrate XAI into clinical practice and aid in transforming AI from an opaquer to a more transparent and crucial aid to radiologists in diagnosing and managing prostate cancer.
ER  - 

TY  - CONF
TI  - Explainable AI Based framework for Banana Disease Detection
T2  - 2024 5th International Conference on Innovative Trends in Information Technology (ICITIIT)
SP  - 1
EP  - 6
AU  - B. Ashoka S
AU  - M. Pramodha
AU  - A. Y. Muaad
AU  - R. Nyange
AU  - A. Anusha
AU  - N. Shilpa G
AU  - C. Chola
PY  - 2024
KW  - Economics
KW  - Productivity
KW  - Accuracy
KW  - Explainable AI
KW  - Surveillance
KW  - Market research
KW  - Real-time systems
KW  - Banana leaf
KW  - Machine Learning
KW  - Explainable-AI
DO  - 10.1109/ICITIIT61487.2024.10580364
JO  - 2024 5th International Conference on Innovative Trends in Information Technology (ICITIIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 5th International Conference on Innovative Trends in Information Technology (ICITIIT)
Y1  - 15-16 March 2024
AB  - Due to widespread usage of banana as a staple food crop and susceptibility to numerous illnesses. Bananas require sophisticated detection techniques to support sustainable agricultural practices. Bananas are particularly susceptible to various stem and leaf spot diseases, resulting in significant economic losses within the banana cultivation sector. In this paper, a new XAI framework for banana disease detection and classification is introduced. Our framework uses state-of-the-art AI methods to analyze photos of banana plants. With great precision, it can detect a variety of illnesses like Cordana, Black Sigatoka, Pestalotiopsis, and fusarium wilt. The outcome shows that the framework performs better than current techniques in precisely identifying and categorizing banana diseases. The research employed a Convolutional Neural Networks (CNNs) to detect diseases in banana plants using RGB images of banana leaves. We used pre-trained model called EfficientnetB0 model to evaluate using two datasets BLSD and BDT. For BLSD, the model achieved an accuracy of 99.22%. Next for BDT, on the other hand, demonstrated improved performance with an accuracy of 99.63%.
ER  - 

TY  - CONF
TI  - A Type-2 Fuzzy Logic Approach to Explainable AI for regulatory compliance, fair customer outcomes and market stability in the Global Financial Sector
T2  - 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
SP  - 1
EP  - 8
AU  - J. Adams
AU  - H. Hagras
PY  - 2020
KW  - Artificial intelligence
KW  - Fuzzy logic
KW  - Industries
KW  - Ethics
KW  - Data models
KW  - Uncertainty
KW  - Government
KW  - Regulatory Compliance
KW  - Accountability and Explainability
KW  - Type-2 Fuzzy Logic
KW  - Neural Networks
DO  - 10.1109/FUZZ48607.2020.9177542
JO  - 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
IS  - 
SN  - 1558-4739
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
Y1  - 19-24 July 2020
AB  - The field of Artificial Intelligence (AI) is enjoying unprecedented success and is dramatically transforming the landscape of the financial services industry. However, there is a strong need to develop an accountability and explainability framework for AI in financial services, based on a risk-based assessment of appropriate explainability levels and techniques by use case and domain. This paper proposes a risk management framework for the implementation of AI in banking with consideration of explainability and outlines the implementation requirements to enable AI to achieve positive outcomes for financial institutions and the customers, markets and societies they serve. The work presents the evaluation of three algorithmic approaches (Neural Networks, Logistic Regression and Type 2 Fuzzy Logic with evolutionary optimisation) for nine banking use cases. We review the emerging regulatory and industry guidance on ethical and safe adoption of AI from key markets worldwide and compare leading AI explainability techniques. We will show that the Type-2 Fuzzy Logic models deliver very good performance which is comparable to or lagging marginally behind the Neural Network models in terms of accuracy, but outperform all models for explainability, thus they are recommended as a suitable machine learning approach for use cases in financial services from an explainability perspective. This research is important for several reasons: (i) there is limited knowledge and understanding of the potential for Type-2 Fuzzy Logic as a highly adaptable, high performing, explainable AI technique; (ii) there is limited cross discipline understanding between financial services and AI expertise and this work aims to bridge that gap; (iii) regulatory thinking is evolving with limited guidance worldwide and this work aims to support that thinking; (iv) it is important that banks retain customer trust and maintain market stability as adoption of AI increases.
ER  - 

TY  - CONF
TI  - Towards Precise Brain Tumor Classification: Employing Deep Learning and Explainable AI for Improved Diagnosis
T2  - 2025 International Conference on Electronics and Renewable Systems (ICEARS)
SP  - 1505
EP  - 1510
AU  - Yashu
AU  - V. Kukreja
PY  - 2025
KW  - Support vector machines
KW  - Accuracy
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Merging
KW  - Medical services
KW  - Predictive models
KW  - Brain modeling
KW  - Convolutional neural networks
KW  - Medical diagnostic imaging
KW  - Brain Tumor
KW  - CNN
KW  - Classification
KW  - Human Brain
KW  - Deep Learning
KW  - SVM
KW  - Analysis
KW  - Sustainable
KW  - Productivity
DO  - 10.1109/ICEARS64219.2025.10940901
JO  - 2025 International Conference on Electronics and Renewable Systems (ICEARS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Electronics and Renewable Systems (ICEARS)
Y1  - 11-13 Feb. 2025
AB  - A major health concern are brain tumours; successful treatment depends on early, precise diagnosis. Radiologists are mostly relied upon in conventional diagnostic techniques, which could be time-consuming and prone to subjective interpretation. With Explainable AI (XAI) approaches, especially Grad-CAM, this study suggests a hybrid model combining Convolutional Neural Networks (CNNs) and Support Vector Machines (SVMs) to increase the accuracy and interpretability of brain tumour identification from MRI images. Leveraging their respective strengths in processing complicated data and differentiating high-dimensional classes, the strategy uses CNNs for feature extraction and SVMs for classification. By emphasising the most relevant portions of the MRI scans that support the predictions of the model, the GradCAM approach increases transparency and hence builds clinician confidence in AI-driven diagnoses. The model was trained on a balanced dataset of MRI images, enhanced to enhance generalisation, preprocessed for homogeneity. Whereas the SVM classifier exhibited great classification accuracy across four tumour types—glioma, pituitary tumour, meningioma, and medulloblastoma—the CNN design effectively captured hierarchical patterns in the data. The corresponding F1 scores of more than 90% indicated that the model achieved an overall accuracy of 95% across all tumour types and particularly high performances for meningioma and glioma classification. This multiclass technique offers a possibility of a powerful method to improve the diagnostics of brain tumours by achieving both high performance and interpretability of the results, which is the basis for improved collaboration between the artificial intelligence systems and medical practitioners.
ER  - 

TY  - CONF
TI  - Exploring Transparency and AI Assessment in LLM-Assisted Research Applications
T2  - SoutheastCon 2025
SP  - 346
EP  - 351
AU  - G. Bacon
AU  - V. Menon
PY  - 2025
KW  - Measurement
KW  - Technological innovation
KW  - Large language models
KW  - Semantics
KW  - Natural languages
KW  - Learning (artificial intelligence)
KW  - Large scale integration
KW  - Human in the loop
KW  - Indexing
KW  - Business
KW  - large language model (LLM)
KW  - artificial intelligence/machine learning (AI/ML)
KW  - ethical/explainable AI
KW  - Latent Semantic Indexing (LSI)
KW  - Small Business Innovation Research (SBIR)
DO  - 10.1109/SoutheastCon56624.2025.10971523
JO  - SoutheastCon 2025
IS  - 
SN  - 1558-058X
VO  - 
VL  - 
JA  - SoutheastCon 2025
Y1  - 22-30 March 2025
AB  - In this work, we further investigate the utility of using a large language model as a research assistant to identify research grant funding opportunities that are best suited for a user-defined natural language set of capabilities. The use case is a United States Department of Defense Small Business Innovation Research Broad Agency Announcement. To explore principles of responsible/ethical artificial intelligence and accountability in the context of large language model-driven applications, we perform clustering on embeddings and apply a suite of metrics to compare cluster quality against Latent Semantic Indexing. Further, we use visualization techniques to depict the contents of funding opportunities that lie at the intersection of multiple capabilities. Finally, we show the importance of maintaining the human in the loop for vetted data quality.
ER  - 

TY  - CONF
TI  - Depicting Decision-Making: A Type-2 Fuzzy Logic Based Explainable Artificial Intelligence System for Goal-Driven Simulation in the Workforce Allocation Domain
T2  - 2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
SP  - 1
EP  - 6
AU  - E. Ferreyra
AU  - H. Hagras
AU  - M. Kern
AU  - G. Owusu
PY  - 2019
KW  - Artificial intelligence
KW  - Fuzzy logic
KW  - Resource management
KW  - Fuzzy sets
KW  - Uncertainty
KW  - Computational modeling
KW  - Explainable AI
KW  - interval type-2 fuzzy logic systems
KW  - goal-driven simulation
KW  - big bang-big crunch
DO  - 10.1109/FUZZ-IEEE.2019.8858933
JO  - 2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
IS  - 
SN  - 1558-4739
VO  - 
VL  - 
JA  - 2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
Y1  - 23-26 June 2019
AB  - The recent years have witnessed a growing anticipation for the positive transformation of industries which adopt Artificial Intelligence (AI) for the core areas of their business activities. However, the effectiveness and reliability of such AI systems must comprise the ability to explain their data acquisition, the underlying algorithms operations and the final decisions to stakeholders, including regulators, risk managers, supervisors and end-users among others. There are plenty of areas where Explainable AI (XAI) holds the promise to be a major disruptor. Particularly, in Telecommunication Service Providers (TSPs) which is a core business activity relating to the workforce allocation domain, which, involves costly and time-consuming scheduling processes. This paper focuses on the construction of an XAI framework to assist workforce allocation based on a big bang- big crunch interval type-2 fuzzy logic system (BB-BC IT2FLS) for modelling and scaling goal-driven simulation (GDS) problems, specifically within the telecommunications industry. The obtained results reported the proposed XAI system produces similar results to opaque box models like Neural Networks (NNs) and LSTM Recurrent NNs while being able to explain the decision and operation of the employed system.
ER  - 

TY  - CONF
TI  - Predicting Liquidity Ratio of Mutual Funds via Ensemble Learning
T2  - 2020 IEEE International Conference on Big Data (Big Data)
SP  - 5441
EP  - 5450
AU  - K. Kong
AU  - R. Liu
AU  - Y. Zhang
AU  - Y. Chen
PY  - 2020
KW  - Regulators
KW  - Mutual funds
KW  - Machine learning
KW  - Predictive models
KW  - Big Data
KW  - Tools
KW  - Robustness
KW  - XAI
KW  - liquidity ratio
KW  - mutual funds
KW  - Ensemble Learning
KW  - fund position
DO  - 10.1109/BigData50022.2020.9378486
JO  - 2020 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Big Data (Big Data)
Y1  - 10-13 Dec. 2020
AB  - We are entering a new era of AI, in which the core technology is machine learning. However, many machine learning models are opaque and not intuitive enough, making it difficult for users to understand how AI systems make decisions. In some scenarios, especially in the fields of finance, healthcare, automatic driving, users have a strong demand for the interpretability of the model. Although AI systems provide a lot of benefits, if the decision and behavior cannot be explained to users or regulators, the effectiveness and development of the systems will be restricted. To gain the trust of users, Explainable AI is necessary.Daily prediction of mutual fund holdings can be a very useful tool. If we can predict the daily holding positions of large mutual funds, we can gain insights into the sentiments of institutional investors which shed lights into the outlook of the market. How to get daily holding from the delayed disclosure information? We leverage on another source of key information - the price of a mutual fund is updated daily, often released a few hours after the market closes. Therefore, we can utilize the daily price fluctuation, combined with quarterly revealed holdings, to make daily predictions of mutual fund holdings. In this paper, we proposed an Ensemble Learning model to predict liquidity ratio of mutual funds. The model has strong interpretability, which is beneficial to users, developers and regulators and all parties involved. Compared with the real fund position data only disclosed once a quarter, our model effectuates timely and efficient high-frequency calculation. In the process of modeling, we creatively apply the framework of Ensemble Learning to portfolio decomposition for the first time. The Ensemble Learning model leverages the diversity of base learners to improve the overall prediction performance. Extensive empirical results on China A-Shares market show that our model can achieve superior accuracy, robustness, and generalization ability.
ER  - 

TY  - JOUR
TI  - UAV Application With XAI and Soft Computing Methods to Protect Privacy of Power-Data in Smart Grids
T2  - IEEE Transactions on Consumer Electronics
SP  - 3921
EP  - 3929
AU  - Y. Gan
AU  - Z. Xiao
AU  - T. Ke
AU  - Z. Liang
PY  - 2025
KW  - Data privacy
KW  - Protection
KW  - Data security
KW  - Autonomous aerial vehicles
KW  - Privacy
KW  - Explainable AI
KW  - Data models
KW  - Smart grids
KW  - Risk management
KW  - Computational modeling
KW  - XAI
KW  - chaotic sequence
KW  - power-data
KW  - UAV networks
KW  - consumer electronics
KW  - privacy protection model
DO  - 10.1109/TCE.2025.3551519
JO  - IEEE Transactions on Consumer Electronics
IS  - 2
SN  - 1558-4127
VO  - 71
VL  - 71
JA  - IEEE Transactions on Consumer Electronics
Y1  - May 2025
AB  - Power grids, energy generation plants, and energy distribution centres are vulnerable to cyberattacks, which can cause economic damage, disrupt power supply, and can also compromise community safety. In order to avoid the privacy leakage and data loss of multi-located power data, a privacy protection model using UAV networks, explainable AI (XAI) and soft computing is proposed in this article. UAVs are utilized to gather and monitor data from energy infrastructure. This data is very sensitive in nature and the proposed two-level authentication mechanism ensures protection of data from unauthorized access. XAI enhances the data security by analyzing the anomalies or unusual patterns in power data. An encryption method based on chaotic sequence is developed for preventing data leakage. The least squares method is used to assign weights to evaluate the privacy risk of multi-scenario power data. Then a composite chaotic sequence is generated using one and two dimensional logistic chaotic systems for safety of the data. The chaotic sequence encryption process is analyzed, and the differential privacy technology is integrated to devise the privacy protection of power data located at multiple locations. Together, these advanced technologies present a framework which is highly secure and responsive for managing power-data. The experimental results show that the proposed method has strong anti-attack and security policy to avoid the privacy leakage and data loss of multi-scenario power-data. By utilizing advanced technologies such as UAV networks, XAI and multi-level authentication with Chaotic sequence, this research work has offered a secure, simple, and resilient framework to manage and protect energy data from unauthorized access.
ER  - 

TY  - JOUR
TI  - Explainable Transfer Learning for Modeling and Assessing Risks in Tunnel Construction
T2  - IEEE Transactions on Engineering Management
SP  - 8339
EP  - 8355
AU  - H. Luo
AU  - J. Chen
AU  - P. E. D. Love
AU  - W. Fang
PY  - 2024
KW  - Predictive models
KW  - Engineering management
KW  - Monitoring
KW  - Risk management
KW  - Transfer learning
KW  - Support vector machines
KW  - Safety
KW  - Explainable artificial intelligence (XAI)
KW  - risk
KW  - transfer learning
KW  - tunneling
DO  - 10.1109/TEM.2024.3369231
JO  - IEEE Transactions on Engineering Management
IS  - 
SN  - 1558-0040
VO  - 71
VL  - 71
JA  - IEEE Transactions on Engineering Management
Y1  - 2024
AB  - Deep learning models are black boxes. Thus, determining the source domain data contributing to transfer learning for ground settlement prediction is impossible. The research presented in this article aims to determine the source domain data (i.e., the dataset or domain used for model pretraining) that contributes most to transfer learning for risk prediction in tunnel construction and quantify its contribution to improving prediction accuracy. We propose a novel explainable transfer learning approach to quantify the selection of degraded knowledge from source and subsource domains. Our approach comprises feature selection and space point clustering, construction of a similarity metric between the target domain and each subsource domain, and construction of a stacked deep neural network model with selective transfer learning. We apply our model to a real-life tunnel project to demonstrate its feasibility and effectiveness. The results indicate that our proposed explainable transfer learning approach outperforms other transparent and opaque analysis models on risk prediction with R2 above 0.5 by adjusting the clustering, transferring, and freezing strategy, and the optimal number of freezing layers should be less than half of the total number of layers, and the best number of freezing layers is 1. We show that explaining transfer learning enables transparency in training and understanding the source domain data, contributing to ground settlement prediction.
ER  - 

TY  - CONF
TI  - An Interactive Graphical Visualization Approach to CNNs and RNNs
T2  - 2020 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
SP  - 1
EP  - 7
AU  - A. Vyas
AU  - P. Calyam
PY  - 2020
KW  - Deep learning
KW  - Productivity
KW  - Technological innovation
KW  - Recurrent neural networks
KW  - Image recognition
KW  - Data visualization
KW  - Network architecture
KW  - CNN Visualization
KW  - RNN Visualization
KW  - Explainable AI
KW  - Deep Learning Visualization
DO  - 10.1109/AIPR50011.2020.9425299
JO  - 2020 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
IS  - 
SN  - 2332-5615
VO  - 
VL  - 
JA  - 2020 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
Y1  - 13-15 Oct. 2020
AB  - Deep Learning models such as Convolutional Neural Nets (CNNs) and Recurrent Neural Nets (RNNs) are routinely used due to their utility and benefits in various practical applications such as e.g., natural language processing and image recognition. However, despite such adoption, these techniques typically present opaqueness of their internal workings to users. The current black box approach to deep learning makes models difficult to understand and fine-tune, and the related lack of information directly influences productivity and hinders innovations in reliable and consistent model development. In this paper, we present a novel web-based, graphical approach to visualizing CNNs and RNNs to address the above adoption challenges. Our approach features an interactive graphical user interface, where the user can view the overarching network architecture and data flow, the weights and corresponding input processing at each layer, and some interpretable aspects of the model as a whole. We show the effectiveness of our visualization techniques on the MNIST dataset corresponding to an image recognition application. Our work contributes to the effective graphical visualization approaches for complex neural networks and thus makes it easier to manage, manipulate, and increase the performance of these networks.
ER  - 

TY  - CONF
TI  - Predictive Modeling for Concrete Compressive Strength: ML-Regressors, Custom Neural Networks, and Explainable AI Approaches
T2  - 2024 4th Asian Conference on Innovation in Technology (ASIANCON)
SP  - 1
EP  - 7
AU  - B. M
AU  - G. S
AU  - A. Kodipalli
AU  - T. Rao
PY  - 2024
KW  - Technological innovation
KW  - Fly ash
KW  - Explainable AI
KW  - Input variables
KW  - Furnaces
KW  - Neural networks
KW  - Predictive models
KW  - Slag
KW  - Concrete
KW  - Random forests
KW  - Concrete Compressive Strength
KW  - ANN
KW  - Machine Learning
KW  - Regressors
KW  - Explainable AI
KW  - LIME
KW  - CCS
DO  - 10.1109/ASIANCON62057.2024.10838137
JO  - 2024 4th Asian Conference on Innovation in Technology (ASIANCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th Asian Conference on Innovation in Technology (ASIANCON)
Y1  - 23-25 Aug. 2024
AB  - Concrete compressive strength is crucial for optimizing concrete mix designs in construction. This study employs traditional machine learning regressors, custom ANNs, and Explainable AI (XAI) techniques using a dataset from Kaggle with 1030 instances and 9 attributes. Among the models tested, Random Forest Regression exhibited the best performance with the lowest MSE and highest R-squared (R2) values. Custom ANNs, particularly a five-layer architecture with SeLU and ReLU activations optimized using the Adam optimizer, achieved superior results with an MSE of 22.2 and an R2 of 0.92. To ensure model transparency, we used the LIME technique, revealing the contributions of individual input variables. Our findings highlight the effectiveness of advanced ML models and XAI techniques in predicting concrete compressive strength, offering a robust framework for optimizing concrete mix designs and enhancing construction practices. Future research can expand on these results by exploring larger datasets, additional features, and more advanced XAI methods.
ER  - 

TY  - CONF
TI  - Towards the Development of eXplainable Digital Twins for Precision Agriculture
T2  - 2024 First International Conference on Pioneering Developments in Computer Science & Digital Technologies (IC2SDT)
SP  - 64
EP  - 69
AU  - P. K. Gupta
AU  - B. D. Mazumdar
AU  - S. N. Pillai
AU  - R. S. Komaragiri
PY  - 2024
KW  - Precision agriculture
KW  - Training
KW  - Satellites
KW  - Explainable AI
KW  - Crops
KW  - Prediction algorithms
KW  - Data models
KW  - Real-time systems
KW  - Digital twins
KW  - Stakeholders
KW  - Deep Learning
KW  - Digital Twins
KW  - Enhanced eXplainable Digital Twins
KW  - eXplainable AI
KW  - eXplainable Digital Twins
DO  - 10.1109/IC2SDT62152.2024.10696477
JO  - 2024 First International Conference on Pioneering Developments in Computer Science & Digital Technologies (IC2SDT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 First International Conference on Pioneering Developments in Computer Science & Digital Technologies (IC2SDT)
Y1  - 2-4 Aug. 2024
AB  - Precision agriculture practices involve deploying the sensors in agricultural farms, to collect data about various factors responsible for crop growth and development. Recently this sensor data is used for generating a virtual replica of the corresponding agricultural physical asset (crop, farm, etc.) called the digital twin (DT). The DT primarily collects data about the current state of a physical asset through sensors. A Deep Learning (DL) model underlying a DT processes this data and generates recommendations. The DL algorithms are, however, often perceived as ‘black-boxes’, which has led to the development of eXplainable artificial intelligence (XAI) approaches. Also, the DL algorithms are limited to generating predictions based on the past data, which can be easily countered by the real-time change tracking capability of the DTs. Therefore, in this work we propose a novel framework of eXplainable Digital Twins (XDTs), which leverages the strengths of both XAI for DL algorithms and the real-time responsiveness of the DTs. Further, we feel that DL algorithms requires huge volumes of data for training and model development. In agricultural domain, where a number of domain experts (like, a farmer has lots of experience about crops, etc.) are involved, we feel this data hungriness limitation of the DL models can be removed by complementing the sensor data with qualitative inputs of the domain experts. Therefore, we also propose (a possible framework) Enhanced XDTs (EXDTs), which use the domain experts' qualitative data, satellite & meteorological data as well as the generated explanations from the XDTs to generate (better) explanations and also suggest necessary state changes to the corresponding agricultural physical asset. We feel our proposed framework is useful for the involved stakeholders in precision agriculture and can be used in other domains as well.
ER  - 

TY  - JOUR
TI  - An Approach for Crop Prediction in Agriculture: Integrating Genetic Algorithms and Machine Learning
T2  - IEEE Access
SP  - 173583
EP  - 173598
AU  - T. Mahmud
AU  - N. Datta
AU  - R. Chakma
AU  - U. Kanti Das
AU  - M. Tarek Aziz
AU  - M. Islam
AU  - A. Hasnat Muhammed Salimullah
AU  - M. Shahadat Hossain
AU  - K. Andersson
PY  - 2024
KW  - Crops
KW  - Accuracy
KW  - Soil measurement
KW  - Predictive models
KW  - Meteorology
KW  - Random forests
KW  - Agriculture
KW  - Rain
KW  - Prediction algorithms
KW  - Genetic algorithms
KW  - Explainable AI
KW  - Machine learning
KW  - Agriculture
KW  - crop prediction
KW  - genetic algorithm
KW  - fitness function
KW  - random forest
KW  - explainable AI (XAI)
DO  - 10.1109/ACCESS.2024.3478739
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Objectives: The agricultural sector in many South Asian countries, including Bangladesh and India, plays a pivotal role in the economy, with a significant portion of the population dependent on it for livelihood. However, farmers often encounter challenges such as unpredictable weather conditions, soil variability, and natural disasters like floods and erosion, leading to substantial crop losses and financial strain. Despite government subsidies, many farmers struggle to sustain their livelihoods, resulting in a decline in interest in agriculture. Our focus lies on predicting the classification of various crops, including rice, jute, maize, and others, based on a combination of soil and weather features. Soil features, including Nitrogen, Phosphorus, Potassium, and pH levels, along with weather variables such as Temperature, Humidity, and Rainfall, are utilized as inputs for the predictive model. Methods: In this study, we address the critical issue of crop prediction by leveraging advanced machine-learning techniques and integrating genetic algorithms into the predictive model. Our proposed approach employs a hybrid methodology, where a Genetic Algorithm is utilized to optimize the hyperparameters of the model, enhancing its performance and robustness. Specifically, we employ a Random Forest classifier, a powerful ensemble learning technique, to classify the class labels associated with 22 different types of crops. Findings: The model’s accuracy is evaluated extensively, demonstrating a remarkable accuracy rate of 99.3%. Additionally, we utilized Local Interpretable Model-agnostic Explaination(LIME) and SHapley Additive exPlanations(SHAP) Explainable AI (XAI) methods to interpret and validate the model’s predictions. Novelty: The study presents a unique method for crop prediction that combines machine learning (ML) with genetic algorithms (GAs). The goal of this integration is to improve crop forecast models’ interpretability and accuracy. Due to the nature of local approximation LIME may yield contradictory answers. On the other hand, for sophisticated models and extensive datasets, SHAP can be computationally costly. By improving feature selection and model parameters, the integration of GAs with ML models overcomes these drawbacks and produces predictions that are more reliable and accurate. The high accuracy achieved by our system underscores its potential to mitigate crop losses and enhance agricultural productivity, thereby contributing to the sustainability and prosperity of the agricultural sector in any country.
ER  - 

TY  - CONF
TI  - Business Management Transformation Through the Influence of Artificial Intelligence
T2  - 2024 IEEE 22nd Jubilee International Symposium on Intelligent Systems and Informatics (SISY)
SP  - 000107
EP  - 000112
AU  - M. Prorok
AU  - I. Takács
PY  - 2024
KW  - Productivity
KW  - Decision making
KW  - Neural networks
KW  - Medical services
KW  - Strategic planning
KW  - Natural language processing
KW  - Reliability
KW  - Problem-solving
KW  - Artificial intelligence
KW  - Business
KW  - artificial intelligence
KW  - business management
KW  - automation
KW  - decision-making
KW  - data analytics
DO  - 10.1109/SISY62279.2024.10737615
JO  - 2024 IEEE 22nd Jubilee International Symposium on Intelligent Systems and Informatics (SISY)
IS  - 
SN  - 1949-0488
VO  - 
VL  - 
JA  - 2024 IEEE 22nd Jubilee International Symposium on Intelligent Systems and Informatics (SISY)
Y1  - 19-21 Sept. 2024
AB  - The paper examines the profound influence of Artificial Intelligence (AI) on business management. Artificial intelligence imitates human cognitive abilities, including observation, problem-solving, learning, and decision-making. This allows machines, algorithms to carry out activities with intelligence that resembles that of humans. The research emphasizes the capacity of AI to evaluate vast information, detect patterns, and autonomously make judgments, hence improving productivity, and efficiency utilizing secondary research from relevant publications to the topic by international researchers. AI approaches such as machine learning, Natural Language Processing (NLP), deep learning, and neural networks are widely used in sectors like manufacturing, finance, and health care. AI in business management streamlines repetitive operations, enhances operational effectiveness, diminishes expenses, and mitigates human fallibility. AI-based analytics offer essential insights for strategic planning, while AI-driven customization improves client relations. Nevertheless, it is imperative to solve difficulties such as ensuring the accuracy and reliability of data, enhancing the openness of models, considering ethical implications, and seamlessly integrating AI systems into current processes. In this article, we discuss ways to enable companies to make the most of AI's business management potential. The result of the research demonstrated that AI automates commonplace duties, minimizing human error and manual labor requirements-significant cost savings and enhanced operational efficacy result from this automation. Furthermore, data analytics powered by artificial intelligence reveal patterns and trends within enormous datasets, whereby furnishing crucial insights that guide strategic planning and decision-making.
ER  - 

TY  - CONF
TI  - Data Acquisition Approaches for AI-supported Metal Processing
T2  - 2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)
SP  - 1027
EP  - 1030
AU  - B. Girsule
AU  - G. Rottermanner
AU  - C. Jandl
AU  - M. Kreiger
AU  - T. Moser
AU  - P. Fuchs
PY  - 2020
KW  - Conferences
KW  - Data acquisition
KW  - Metals
KW  - Training data
KW  - Production
KW  - Machine learning
KW  - Generators
KW  - Human Machine Interfaces
KW  - Automation systems
KW  - Predictive Manufacturing Systems (PMS)
DO  - 10.1109/ETFA46521.2020.9211935
JO  - 2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)
IS  - 
SN  - 1946-0759
VO  - 1
VL  - 1
JA  - 2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)
Y1  - 8-11 Sept. 2020
AB  - Due to increasing digitalisation, it is possible to digitally map the production of sheet metal profiles from configuration to production. In our research project we design, implement and evaluate a knowledge- and rule-based system in cooperation with a sheet metal profile manufacturer using a modern production machine. One big challenge hereby is the data acquisition to perform a producibility assessment. In many cases only the positive (producible) production data is stored and negative data is discarded during the production process. In this paper, we present approaches to generate and collect negative training data for a machine learning approach using a data generator and feedback from manufacturing experts to perform a predictive manufacturing assessment.
ER  - 

TY  - CONF
TI  - Dynamic Financial Portfolio Optimization Using Temporal Convolutional Networks and Real-Time Data Analysis
T2  - 2025 International Conference on Emerging Technologies in Computing and Communication (ETCC)
SP  - 1
EP  - 5
AU  - A. E
AU  - G. Ramasamy
AU  - S. KN
PY  - 2025
KW  - Technological innovation
KW  - Data analysis
KW  - Estimation
KW  - Real-time systems
KW  - Eigenvalues and eigenfunctions
KW  - Convolutional neural networks
KW  - Clamps
KW  - Optimization
KW  - Portfolios
KW  - Numerical stability
KW  - Portfolio optimization
KW  - temporal convolutional networks
KW  - real-time data analysis
KW  - risk management
KW  - algorithmic trading
DO  - 10.1109/ETCC65847.2025.11108346
JO  - 2025 International Conference on Emerging Technologies in Computing and Communication (ETCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Emerging Technologies in Computing and Communication (ETCC)
Y1  - 26-27 June 2025
AB  - This paper presents an integrated framework for AI-driven portfolio optimization combining temporal convolutional networks (TCNs) with conditional value-at-risk (CVaR) minimization. Our system processes real-time market data through an automated pipeline implementing volatility-adjusted feature engineering and walk-forward validation. The architec- ture employs dilated causal convolutions for temporal pattern extraction combined with Ledoit-Wolf shrinkage covariance es- timation for robust portfolio optimization. Experimental results demonstrate an 18.7% annualized return with 22.3% volatility, outperforming traditional mean-variance optimization by 14.2% in risk-adjusted returns. The implementation addresses key challenges in numerical stability and computational efficiency through eigenvalue clamping and gradient checkpointing.
ER  - 

TY  - CONF
TI  - On the Comparative Analysis of Trends in Cybersecurity Risk Assessment, Governance, and Compliance Frameworks
T2  - 2024 International Jordanian Cybersecurity Conference (IJCC)
SP  - 136
EP  - 142
AU  - S. J. Aljarrah
AU  - S. Cherbal
AU  - A. Mashaleh
AU  - J. A. Karaki
AU  - A. Gawanmeh
PY  - 2024
KW  - Measurement
KW  - Privacy
KW  - Ethics
KW  - Technological innovation
KW  - Federated learning
KW  - Market research
KW  - Risk management
KW  - Computer security
KW  - Standards
KW  - Systematic literature review
KW  - Cybersecurity Risk Assessment
KW  - Adaptive Governance
KW  - Privacy-Preserving Frameworks
KW  - Regulatory Compliance
DO  - 10.1109/IJCC64742.2024.10847280
JO  - 2024 International Jordanian Cybersecurity Conference (IJCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Jordanian Cybersecurity Conference (IJCC)
Y1  - 17-18 Dec. 2024
AB  - This paper proposes an evaluation framework and systematic review of recent trends for cybersecurity risk assessment governance and compliance. The proposed framework incorporates several metrics for assessing model effectiveness. The findings highlight research opportunities in scalable privacy-preservation techniques, cross-domain validation, and standardized performance benchmarks. Federated learning models achieve the highest privacy rating while maintaining strong performance in precision and automation, suggesting distributed learning architectures as a promising direction for future governance, risk, and compliance framework development. Based on these findings, we recommend for future frameworks supported by regulatory considerations that balance privacy, performance, and ethical requirements, and combine quantum-resistant architectures with privacypreserving features.
ER  - 

TY  - CONF
TI  - Federated Multi-Task AI for Fire Detection and Risk Assessment with Edge Deployment and Explainability
T2  - 2025 7th Global Power, Energy and Communication Conference (GPECOM)
SP  - 985
EP  - 990
AU  - F. Chettiar
PY  - 2025
KW  - Scene classification
KW  - Accuracy
KW  - Image edge detection
KW  - Disasters
KW  - Estimation
KW  - Multitasking
KW  - Real-time systems
KW  - Cognition
KW  - Risk management
KW  - Convolutional neural networks
KW  - Federated Learning
KW  - Edge AI
KW  - Fire Detection
KW  - Multi-Task Learning
KW  - Risk Assessment
KW  - Scene Classification
KW  - Explainable AI
KW  - Grad-CAM
KW  - Privacy-Preserving Learning
KW  - Non-IID Data
DO  - 10.1109/GPECOM65896.2025.11061994
JO  - 2025 7th Global Power, Energy and Communication Conference (GPECOM)
IS  - 
SN  - 2832-7675
VO  - 
VL  - 
JA  - 2025 7th Global Power, Energy and Communication Conference (GPECOM)
Y1  - 11-13 June 2025
AB  - This paper introduces a federated learning–based, multi-task artificial intelligence (AI) approach intended for real-time fire detection and contextual risk evaluation in edge computing scenarios. The developed model utilizes a convolutional neural network (CNN) that can conduct binary fire detection, scene classification (indoor, outdoor, urban, forest), and fire-associated risk level prediction (low, medium, high). Joint training is accomplished on geo-clustered, privacy-respecting edge clients with heterogeneous, non-independent and identically distributed (non-IID) data. The resulting global model is saved in a TorchScript-optimized format to enable low-latency inference on the edge devices. Model interpretability is incorporated through Gradient-weighted Class Activation Mapping (Grad-CAM), with interpretable visual feedback. Empirical tests over five federated rounds demonstrate stable classification accuracy (up to 96%), high recall and precision, and robust generalization over heterogeneous clients. The system integrates federated multitask learning, edge deployment, contextual reasoning, and explainability into an effective solution for smart disaster response.
ER  - 

TY  - CONF
TI  - Revisiting Fidelity in Explainable AI: Unpacking Cognitive Biases and Deceptive Transparency in Model Interpretations
T2  - 2025 5th Intelligent Cybersecurity Conference (ICSC)
SP  - 298
EP  - 302
AU  - A. Patel
PY  - 2025
KW  - Measurement
KW  - Ethics
KW  - Accuracy
KW  - Systematics
KW  - Decision making
KW  - Psychology
KW  - Distortion
KW  - Reliability
KW  - Artificial intelligence
KW  - Usability
DO  - 10.1109/ICSC65596.2025.11139857
JO  - 2025 5th Intelligent Cybersecurity Conference (ICSC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 5th Intelligent Cybersecurity Conference (ICSC)
Y1  - 19-22 May 2025
AB  - The evolution of explainability in artificial intelligence continues in response to rising demands for transparency, yet a persistent challenge remains: ensuring alignment between generated insights and underlying operations. The intricate balance between clarity and precision is examined, with obstacles emerging due to inherent distortions in perception influencing both construction and reception. Additionally, the possibility of misleading presentations is considered, where seemingly persuasive outputs may be mistakenly accepted as accurate. Various approaches—causal, statistical, and visual—are explored in relation to their potential to either mitigate or amplify associated risks. Ethical dimensions are highlighted, with frameworks proposed to enhance reliability and openness while emphasizing both comprehension and authenticity. The necessity of thoughtful refinement is underscored to prevent unintended distortion, misrepresentation, or adverse consequences. Preliminary user studies indicate that aligning the framing of explanations with objective fidelity metrics can significantly reduce misinterpretations among non-expert users. These results underscore the need for clear, empirically validated guidelines to counteract the risks of deceptive transparency. This study aims to develop a framework for evaluating explanation fidelity and to propose best practices for mitigating cognitive bias in AI interpretability.
ER  - 

