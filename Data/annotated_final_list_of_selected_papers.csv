reference_type,author,year,title,publisher,volume,issue,pages,abstract,url,field,subcategory,keywords,journal
Journal Article,", Jirousek Afdhal, O., Palar, P. S., Falta, J., Dwianto, Y. B.",2023,Design exploration of additively manufactured chiral auxetic structure using explainable machine learning,Elsevier Ltd,232,,,"A design exploration of hexachiral structures using explainable machine learning (ML) is performed in this work. The hexachiral structures are fabricated using resin via vat photopolymerization (VPP). The ML model is used to build the function that explains the association between Poisson's ratio and the hexachiral design parameters. The data set for ML model construction is first collected by using the Halton sequence and simulated using the finite element method (FEM). To validate the data set, the results obtained from the FEM simulation are compared with those obtained from the compression test. The Gaussian Process Regression (GPR) models for Poisson's ratio and porosity are constructed to extract important design insight. A Global Sensitivity Analysis (GSA) and Shapley Additive Explanations (SHAP) are used to analyze the sensitivity of the porosity and Poisson's ratio to the hexachiral design parameters. GSA result shows that the strut's thickness is the most decisive parameter that affects the Poisson's ratio. The application of SHAP also reveals that the relationship between the strut thickness and Poisson's ratio is nonlinear. Finally, the minimum Poisson's ratio value is achieved by design with minimum strut thickness, minimum node radius, and maximum strut length.(c) 2023 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",,CE,Materials,Chiral auxetic structure; Machine learning; Vat photopolymerization; Finite element method; Gaussian process regression,Materials & Design
Journal Article,"N., Assadi Ahmed, M., Zhang, Q.",2023,Investigating the impact of borehole field data's input parameters on the forecasting accuracy of multivariate hybrid deep learning models for heating and cooling,Elsevier Ltd,301,,,"Heat pump systems coupled with Borehole heat exchanger (BHEx) are crucial for the efficient and cost-effective heating and cooling of buildings. Accurate performance forecasting of BHEx can help to reduce energy consumption, optimize system design, improve system reliability and ultimately contribute to the sustainability of the built environment. The performance of deep neural networks, including hybrid deep learning algorithms, is dependent on the number and true identification of input parameters used to train the model. While research on the accuracy and selection of individual deep neural networks is well-investigated, the impact of selected input parameters on hybrid algorithm selection and its subsequent influence on multi-step forecasting performance has not been quantified specifically for geothermal heating/cooling systems. The focus of the current work is to investigate and quantify this impact on multivariate hybrid deep learning algorithms for forecasting borehole heat exchanger outlet temperature. In the current work the developed hybrid AI models (LSTM-CNN, GRU-CNN, CNN-LSTM, CNN-GRU) have been trained, validated, and tested using the measured borehole-field data with variable number of input parameters (3,4,5 and 6). The validated AI models are used to forecast 24 h ahead performance of borehole heat exchanger to meet the heating and cooling loads. The most essential input parameters for the data-driven AI models are determined via an importance-level analysis by using Explainable AI techniques of Shapley Additive Explanations (SHAP). The findings show that increasing the number of input parameters has the potential to enhance the multi-step forecasting accuracy of the examined deep learning models. However, it is crucial to note that there is a point where including additional input parameters may not lead to further improvements. Therefore, it is essential to identify the optimal number of input parameters that can maximize the accuracy of the hybrid deep learning models. Moreover, the analysis highlights that depending on the architecture of specific model layers, it is possible to eliminate redundant weather sensors associated with time variables without compromising the accuracy of the hybrid deep learning models. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175633374&doi=10.1016%2Fj.enbuild.2023.113706&partnerID=40&md5=f3d44155a266739ecf03ded48154d589,CE,Energy,borehole coupled heat pump; heating and cooling; ground source heat pump; prediction model; hybrid forecasting models; data driven modeling,Energy and Buildings
Journal Article,"Y., Aslansefat Akhlaghi, K., Zhao, X., Sadati, S., Badiei, A., Xiao, X., Shittu, S., Fan, Y., Ma, X.",2021,Hourly performance forecast of a dew point cooler using explainable Artificial Intelligence and evolutionary optimisations by 2050,Elsevier Ltd,281,,,"The empirical success of the Artificial Intelligence (AI), has enhanced importance of the transparency in black box Machine Learning (ML) models. This study pioneers in developing an explainable and interpretable Deep Neural Network (DNN) model for a Guideless Irregular Dew Point Cooler (GIDPC). The game theory based SHapley Additive exPlanations (SHAP) method is used to interpret contribution of the operating conditions on performance parameters. Furthermore, in a response to the endeavours in developing more efficient metaheuristic optimisation algorithms for the energy systems, two Evolutionary Optimisation (EO) algorithms including a novel bio-inspired algorithm i.e., Slime Mould Algorithm (SMA), and Particle Swarm Optimization (PSO), are employed to simultaneously maximise the cooling efficiency and minimise the construction cost of the GIDPC. Additionally, performance of the optimised GIDPCs are compared in both statistical and deterministic way. The comparisons are carried out in diverse climates in 2020 and 2050 in which the hourly future weather data are projected using a high-emission scenario defined by Intergovernmental Panel for Climate Change (IPCC). The results revealed that the hourly COP of the optimised systems outperform the base design. Although power consumption of all systems increases from 2020 to 2050, owing to more operating hours as a result of global warming, but power savings of up to 72%, 69.49%, 63.24%, and 69.21% in hot summer continental, Arid, tropical rainforest and Mediterranean hot summer climates respectively, can be achieved when the systems run optimally. © 2020 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094604171&doi=10.1016%2Fj.apenergy.2020.116062&partnerID=40&md5=4b90da6259da2d19c08d7573fbdaae35,CE,Energy,dew point cooler; multi objective evolutionary optimization; particle swarm optimization; slime mould algorithm; artificial intelligence,Applied Energy
Journal Article,"M. A., Burton Aladsani, H., Abdullah, S. A., Wallace, J. W.",2022,Explainable Machine Learning Model for Predicting Drift Capacity of Reinforced Concrete Walls,American Concrete Institute,119,3,191–204,"The ability to predict the drift capacity of reinforced concrete structural walls is critical to the seismic design process. The accuracy of such predictions has implications for construction costs, seismic safety, and reliability. However, the inability of an empirical model to capture any nonlinearity that exists between the drift capacity and different influencing variables can negatively impact the predictive performance. This study proposes a drift capacity prediction model for special structural walls based on the extreme gradient boosting machine-learning algorithm and a data set of 164 special boundary element wall tests. The efficiency of the proposed model is evaluated using a nested cross-validation approach, and the results reveal its superior predictive capabilities relative to the empirical equation adopted in ACI 318-19. To overcome the lack of interpretability of the model, SHapley Additive exPlanations are used to examine the relative individual and interactive effects of the different input variables on the drift capacity. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131300644&doi=10.14359%2F51734484&partnerID=40&md5=3d3b77e82458bc6efaaf6d7eb96a3cec,CE,Structural,artificial intelligence; drift capacity; extreme gradient boosting; machine learning; reinforced concrete walls; special boundary elements,ACI Structural Journal
Journal Article,"M. G., Tripathi Alam, V., Bhatt, C. M., Mohanty, M. P.",2025,A novel framework embedding Bayesian-optimized ensemble machine learning and explainable artificial intelligence (XAI) to improve flood prediction in complex watersheds,Elsevier Inc.,27,,,"Floods are among the most common and destructive natural hazards, particularly in areas characterized by data scarcity and intricate geomorphological features. In such regions, accurate and interpretable flood susceptibility mapping is essential for effective risk reduction and informed urban planning. This study proposes a novel ensemble machine learning (ML) framework integrated with geomorphology-based flood conditioning factors (FCFs) to identify flood-prone areas over Kosi Megafan in India-a large watershed with significant anabranching characteristics. A comprehensive set of 21 FCFs, including topographic, hydrologic, and stream-related indicators-was derived from the FABDEM, satellite imagery, and ancillary datasets. After multicollinearity analysis and information gain ratio filtering, 15 key FCFs were selected for model training. Four base ML models, namely, Random Forest, XGBoost, CatBoost, and Long Short-Term Memory, were optimized using Bayesian techniques and combined into a stacked ensemble classifier. The model was trained using historical flood extents from the Global Surface Water dataset and validated against Sentinel-1 SAR imagery. Results indicate that the ensemble model outperformed individual classifiers, achieving the highest accuracy (90 %) and Cohen's Kappa (0.79). SHapley Additive exPlanations (SHAP) were used to enhance interpretability, highlighting elevation, rainfall, curve number, and drainage density as the most influential predictors. The final flood susceptibility map shows that 35.18 % of the megafan is very highly susceptible, aligning well with observed flood events. This interpretable and scalable framework holds strong potential for enhancing flood risk management in complex, datascarce catchments, while also supporting global initiatives such as the Sendai Framework and Sustainable Development Goals (SDGs) 11 and 13.",,CE,Water,Bayesian approach; flood susceptibility; Kosi megafan; machine learning; SHAP,Environmental and Sustainability Indicators
Journal Article,"S. F. M., Ahmed Alazawy, M. A., Raheem, S. H., Imran, H., Bernardo, L. F. A., Pinto, H. A. S.",2025,Explainable Machine Learning to Predict the Construction Cost of Power Plant Based on Random Forest and Shapley Method,Multidisciplinary Digital Publishing Institute (MDPI),6,2,,"This study aims to develop a reliable method for predicting power plant construction costs during the early planning stages using ensemble machine learning techniques. Accurate cost predictions are essential for project feasibility, and this research highlights the strength of ensemble methods in improving prediction accuracy by combining the advantages of multiple models, offering a significant improvement over traditional approaches. This investigation employed the Random Forest (RF) algorithm to estimate the overall construction cost of a power plant. The RF algorithm was contrasted with single-learner machine learning models: Support Vector Regression (SVR) and k-Nearest Neighbors (KNN). Performance measures, comprising the coefficient of determination ((Formula presented.)), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE), were used to evaluate and contrast the performance of the implemented models. Statistical measures demonstrated that the RF approach surpassed alternative models, demonstrating the highest coefficient of determination for testing ((Formula presented.)) and the lowest Root Mean Square Error (RMSE = 29.27) for the testing dataset. The Shapley Additive Explanation (SHAP) technique was implemented to explain the significance and impact of predictor factors affecting power plant construction costs. The outcomes of this investigation provide crucial information for project decision-makers, allowing them to reduce discrepancies in projected costs and make informed decisions at the beginning of the construction phase. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009281150&doi=10.3390%2Fcivileng6020021&partnerID=40&md5=ccccac53e66471f3831fb94559fa7919,CEM,Cost Estimation / Bidding,random forest; construction cost; shapley value; power plant,CivilEng
Journal Article,"M., Mallick Alkahtani, J., Alqadhi, S., Sarif, M. N., Ahmed, M. F. M., Abdo, H. G.",2024,Interpretation of Bayesian-optimized deep learning models for enhancing soil erosion susceptibility prediction and management: a case study of Eastern India,Taylor & Francis,39,1,,"Soil erosion poses a significant threat to sustainable land management and agricultural productivity. Addressing this issue requires advanced predictive models that can accurately identify areas at risk and inform soil conservation strategies. This study focuses on the development and interpretation of well-optimized deep learning (DL) models to predict soil erosion probability, aiming to enhance decision-making in land management. Utilizing the Revised Universal Soil Loss Equation (RUSLE) in conjunction with ground-truthing, we identified critical erosion-prone areas. To predict soil erosion probability, we employed Bayesian optimization to fine-tune Deep Neural Network (DNN), Convolutional Neural Network (CNN), Fully Connected Neural Network (FCNN), and DNN-CNN hybrid models. These DL models were verified using a set of metrics. SHAP value analysis as a means of explainable artificial intelligence (XAI) was used to interpret these DL models for better decision-making. RUSLE estimations and ground truthing highlight that Soil erosion rates in the northeastern and northwestern regions are nearing the highest observed at 25 tonnes per hectare annually, largely due to steep slopes and limited vegetation. In contrast, the southern and southeastern areas have lower erosion rates, due to denser vegetation and gentler slopes. Deep learning models, optimized using Bayesian methods, demonstrate high performance in spatially modeling soil erosion probability. The DNN model achieved an accuracy of 0.93, a precision of 0.92, and an F1-score of 0.94, identifying 222.73 sq. km as highly susceptible to erosion, which indicates its strong ability to detect true erosion events. The CNN model identified 49.68% of the study area (503.30 sq. km) as high-risk, with an accuracy of 0.90 and a precision of 0.91. The FCNN model showed a balanced risk distribution, indicating 37.04% of the land (375.25 sq. km) as very low risk and 36.43% (369.11 sq. km) as very high risk, with an accuracy of 0.91. The DNN-CNN hybrid model highlighted 41.58% of the area (421.20 sq. km) as high risk, demonstrating its effectiveness in capturing spatial patterns of erosion susceptibility. SHAP value analysis indicates that land use and soil type (LULC and K-factor) are crucial in erosion predictions, with LULC having a significant predictive influence in the DNN model. These insights facilitate the prioritization of soil conservation measures, enabling decision-makers to focus on the most impactful factors for mitigating soil erosion.",,CE,Environmental,Soil erosion prediction; deep learning; Bayesian Optimization; SHAP values; sustainable land management,Geocarto International
Journal Article,"D., Mallick Alqahtani, J., Alqahtani, A. M., Talukdar, S.",2024,Optimizing Residential Construction Site Selection in Mountainous Regions Using Geospatial Data and eXplainable AI,Multidisciplinary Digital Publishing Institute (MDPI),16,10,,"The rapid urbanization of Abha and its surrounding cities in Saudi Arabia’s mountainous regions poses challenges for sustainable and secure development. This study aimed to identify suitable sites for eco-friendly and safe building complexes amidst complex geophysical, geoecological, and socio-economic factors, integrating natural hazards assessment and risk management. Employing the Fuzzy Analytic Hierarchy Process (Fuzzy-AHP), the study constructed a suitability model incorporating sixteen parameters. Additionally, a Deep Neural Network (DNN) based on eXplainable Artificial Intelligence (XAI) conducted sensitivity analyses to assess the parameters’ influence on optimal location decision making. The results reveal slope as the most crucial parameter (22.90%), followed by altitude and land use/land cover (13.24%), emphasizing topography and environmental considerations. Drainage density (11.36%) and rainfall patterns (9.15%) are also significant for flood defense and water management. Only 12.21% of the study area is deemed “highly suitable”, with “no-build zones” designated for safety and environmental protection. DNN-based XAI demonstrates the positive impact of variables like the NDVI and municipal solid waste generation on site selection, informing waste management and ecological preservation strategies. This integrated methodology provides actionable insights for sustainable and safe residential development in Abha, aiding informed decision making and balancing urban expansion with environmental conservation and hazard risk reduction. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194271311&doi=10.3390%2Fsu16104235&partnerID=40&md5=86b34995ba3b61c606a7cb037273234b,CE,GIS / Remote Sensing,sustainable urbanization; gis-based site selection; risk assessment; artificial intelligence; mountainous terrain; decision-making framework,Sustainability
Journal Article,"M., Nassar Alyami, R. U. D., Khan, M., Hammad, A. W. A., Alabduljabbar, H., Nawaz, R., Fawad, M., Gamil, Y.",2024,Estimating compressive strength of concrete containing rice husk ash using interpretable machine learning-based models,Elsevier Ltd,20,,,"The construction sector is a major contributor to global greenhouse gas emissions. Using recycled and waste materials in concrete is a practical solution to address environmental challenges. Currently, agricultural waste is widely used as a substitute for cement in the production of ecofriendly concrete. However, traditional methods for assessing the strength of such materials are both expensive and time-consuming. Therefore, this study uses machine learning techniques to develop prediction models for the compressive strength (CS) of rice husk ash (RHA) concrete. The ML techniques used in the present study include random forest (RF), light gradient boosting machine (LightGBM), ridge regression, and extreme gradient boosting (XGBoost). A total of 348 values of CS were collected from the experimental studies, and five characteristics of RHA concrete were taken as input variables. For the performance assessment of the models, multiple statistical metrics were used. During the training phase, the correlation coefficients (R) obtained for ridge regression, RF, XGBoost, and LightGBM were 0.943, 0.981, 0.985, and 0.996, respectively. In the testing set, the developed models demonstrated even higher performance, with correlation coefficients of 0.971, 0.993, 0.992, and 0.998 for ridge regression, RF, XGBoost, and LightGBM, respectively. The statistical analysis revealed that the LightGBM model outperformed other models, whereas the ridge regression model exhibited comparatively lower accuracy. SHapley Additive exPlanation (SHAP) method was employed for the interpretability of the developed model. The SHAP analysis revealed that water-to-cement is a controlling parameter in estimating the CS of RHA concrete. In conclusion, this study provides valuable guidance for builders and researchers to estimate the CS of RHA concrete. However, it is suggested that more",,CE,Materials,rice husk ash; machine learning; compressive strength; shap analysis; prediction modeling,Case Studies in Construction Materials
Journal Article,"X., Luo An, H., Zheng, F., Jiao, Y., Qi, J., Zhang, Y.",2024,Explainable deep learning-based dynamic prediction of surface settlement considering temporal characteristics during deep excavation,Elsevier B.V,167,,,"Accurate prediction of surface settlement (SS) induced by deep foundation pit (DFP) excavation is challenging considering the complicated soil distributions and uncertain construction situations. Therefore, a Recurrent Gated Unit (GRU) neural network model incorporated with Variation Mode Decomposition (VMD) was developed for SS prediction. By data denoising with VMD and hyperparameter tuning using Bayesian Optimization (BO), the proposed GRU model achieved dynamic and accurate prediction of SS caused by DFP excavation. Shapley Additive exPlanations (SHAP) analysis was then performed to enhance the interpretability of the GRU model. The presented GRU model was validated with a case study of Wuhan Metro Line 12 Shiqiao Station. The results indicate that: (1) The GRU models with VMD data denoising technique achieved accurate prediction with the average RMSE of 0.0244, MAE of 0.0189, MAPE of 0.0017, R2 of 0.9894, and VAF of 99.1206. (2) The GRU model outperformed the other five state-of-the-art models in predictive performance with robustness improvement of 13.5 %, 12.0 %, 50.0 %, 29.2 %, and 27.3 %, separately, compared to the other five models. (3) The historical values of the SS make the most significant contributions to the outcomes of the GRU models. In short, this study enhances the accurate and dynamic prediction of SS caused by DFP excavation, contributing to the safe execution of DFP projects. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204989994&doi=10.1016%2Fj.asoc.2024.112273&partnerID=40&md5=c4273c9797fcc353c5f017d15aec9c92,CE,Geotechnical,surface settlement prediction; variation mode decomposition; gated recurrent unit neural network; bayesian optimization; deep learning model explanation,Applied Soft Computing
Journal Article,"X., Zheng An, F., Jiao, Y., Li, Z., Zhang, Y., He, L.",2024,Optimized machine learning models for predicting crown convergence of plateau mountain tunnels,Elsevier Ltd,46,,,"The precise prediction of tunnel convergence holds significant importance in ensuring the safety and efficiency of tunnel construction. This study involves the construction of seven machine learning (ML) models that are deemed reliable for predicting crown convergence (CC) of plateau mountain tunnels. These models include K-nearest neighbour (KNN), support vector regression (SVR), decision tree (DT), random forest (RF), extreme gradient boosting (XGBoost), categorical boosting (CatBoost), and automated machine learning model (Auto-ML). The Bayesian optimization (BO) technique and the Tree-based pipeline optimization tool (TPOT) were employed to optimize the hyperparameters of non-automated machine learning models and the automated machine learning model, separately. A total of 2,734 samples of crown convergence monitoring data were collected for training and testing the models. The evaluation metrics employed to assess the performance of the models’ predictions included the determination coefficient (R2), mean absolute percentage error (MAPE), mean absolute error (MAE), and root mean square error (RMSE). The XGBoost model demonstrated the best prediction performance with the R2, MAPE, MAE, and RMSE of 0.9887, 0.1271, 0.7759, and 1.1845 on test set. The XGBoost, CatBoost, and Auto-ML models were utilized to predict the crown convergence monitoring curves of four sections of Huzhubeishan Tunnel in Qinghai Province, China. Notably, the predicted curves exhibited a significant level of concordance with the actual monitoring curves. The results of Shapley Additive Explanations (SHAP) suggested that time, tunnel depth ratio and groundwater table were more important than other input variables. The XGBoost model showed the best robustness of prediction performance across varying dataset sizes for seven distinct machine learning models. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190891329&doi=10.1016%2Fj.trgeo.2024.101254&partnerID=40&md5=a2b3a32352b35b51bd82f4a89fce28bc,CE,Geotechnical,tunnel convergence; machine learning; bayesian optimization; automated machine learning; explainable artificial intelligence,Transportation Geotechnics
Journal Article,"H., Brzozowski Anysz, L., Kretowicz, W., Narloch, P.",2020,Feature Importance of Stabilised Rammed Earth Components Affecting the Compressive Strength Calculated with Explainable Artificial Intelligence Tools,Multidisciplinary Digital Publishing Institute (MDPI),13,10,,"Cement-stabilized rammed earth (CSRE) is a sustainable construction material. The use of it allows for economizing on the cost of a structure. These two properties of CSRE are based on the fact that the soil used for the rammed mixture is usually dug close to the construction site, so it has random characteristics. That is the reason for the lack of widely accepted prescriptions for CSRE mixture, which could ascertain high enough compressive strength. Therefore, assessing which components of CSRE have the highest impact on its compressive strength becomes an important issue. There are three machine learning regression tools, i.e., artificial neural networks, decision tree, and random forest, used for predicting the compressive strength based on the relative content of CSRE composites (clay, silt, sand, gravel, cement, and water content). The database consisted of 434 samples of CSRE, which were prepared and crushed for testing purposes. Relatively low prediction errors of aforementioned models allowed for the use of explainable artificial intelligence tools (drop-out loss, mean squared error reduction, accumulated local effect) to rank the influence of the ingredients on the dependent variable-the compressive strength. Consistent results from all above-mentioned methods are discussed and compared to some statistical analysis of selected features. This innovative approach, helpful in designing the construction material is a solid base for reliable conclusions.",,CE,Materials,rammed earth; cement stabilized rammed earth; multivariate regression; random forest; artificial intelligence; features importance ranking,Materials
Journal Article,"M., Jan Arif, F., Rezzoug, A., Afridi, M. A., Luqman, M., Khan, W. A., Kujawa, M., Alabduljabbar, H., Khan, M.",2024,Data-driven models for predicting compressive strength of 3D-printed fiber-reinforced concrete using interpretable machine learning algorithms,Elsevier Ltd,21,,,"3D printing technology is growing swiftly in the construction sector due to its numerous benefits, such as intricate designs, quicker construction, waste reduction, environmental friendliness, cost savings, and enhanced safety. Nevertheless, optimizing the concrete mix for 3D printing is a challenging task due to the numerous factors involved, requiring extensive experimentation. Therefore, this study used three machine learning techniques, including Gene Expression Programming (GEP), Multi-Expression Programming (MEP), and Decision Tree (DT), to forecast the compressive strength of 3D printed fiber-reinforced concrete (3DP-FRC). The dataset comprises 299 data points with sixteen variables gathered from experimental research studies. For training the model, 70 % of the dataset was used, while the remaining 30 % was reserved for model testing. Several statistical metrics were utilized to evaluate the accuracy and applicability of the models. In addition, SHapley Additive exPlanations (SHAP), partial dependence plots, and individual conditional expectations approach were employed for the interpretability of the models. The proposed GEP, MEP, and DT models indicated enhanced efficacy, exhibiting correlation coefficient (R) scores of 0.996, 0.987, and 0.990, with mean absolute errors (MAE) of 1.029, 4.832, and 2.513, respectively. Overall, the established GEP model demonstrated exceptional performance compared to MEP and DT, showcasing high prediction precision in assessing the strength of 3DP-FRC. Moreover, a simple empirical formulation has been devised using GEP to predict the compressive strength, offering a simplified and efficient approach for predicting 3DPFRC strength. The SHAP approach identified water, silica fume, fiber diameter, curing age, and loading directions as leading controlling parameters in predicting strength of 3DP-FRC. In summary, the proposed models can potentially minimize both the computational workload and the need for experimental trials in formulating the mixed design of 3D-printed concrete.",,CE,Materials,machine learning; 3d-printed fiber reinforced concrete; model interpretability; compressive strength,Case Studies in Construction Materials
Journal Article,"L., Le Asaye, C., Huang, Y., Le, T. Q., Yadav, O. P., Le, T. Y.",2025,Predicting and Understanding Emergency Shutdown Durations Level of Pipeline Incidents Using Machine Learning Models and Explainable AI,Multidisciplinary Digital Publishing Institute (MDPI),13,2,,"Pipeline incidents pose significant concerns due to their potential environmental, economic, and safety risks, emphasizing the critical need to understand and manage this vital infrastructure. While existing studies predominantly focus on the causes of pipeline incidents and failures, few have investigated the consequences, such as shutdown duration, and most lack comprehensive models capable of accurately predicting and providing actionable insights into the risk factors. This study bridges this gap by employing machine learning (ML) techniques, including Random Forest and Light Gradient Boosting Machine (LightGBM), for classifying pipeline incidents' emergency shutdown duration levels. These techniques are specifically designed to capture complex, nonlinear patterns and interdependencies within the data, addressing the limitations of traditional linear approaches. The proposed model has further enhanced with Explainable AI (XAI) techniques, such as Shapley Additive exPlanations (SHAP) values, to improve interpretability and provide insights into the factors influencing shutdown durations. Historical incident data, collected from the Pipeline and Hazardous Materials Safety Administration (PHMSA) from 2010 to 2022, were utilized to examine the risk factors. K-Fold Cross-Validation with 5 folds was employed to ensure the model's robustness. The results demonstrate that the LightGBM model achieved the highest accuracy of 75.0%, closely followed by Random Forest at 74.8%. The integration of XAI techniques provides actionable insights into key factors such as pipeline material, age, installation layout, and commodity type, which significantly influence shutdown durations. These findings underscore the practical implications of the proposed approach, enabling pipeline operators, emergency responders, and regulatory authorities to make informed decisions that optimize resource allocation and mitigate risks effectively.",,CEM,Risk and Uncertainty,emergency shutdown duration; pipeline incidents; explainable ai; infrastructure resilience; risk management,Processes
Journal Article,"I., Alhadidi Asi, Y. I., AlHadidi, T. I.",2024,Predicting Marshall stability and flow parameters in asphalt pavements using explainable machine-learning models,Elsevier Ltd,18,,,"The traditional method for determining the Marshall stability (MS) and Marshall flow (MF) of asphalt pavements is laborious, time consuming, and costly. This study aims to predict these parameters using explainable machine-learning techniques. A comprehensive database comprising 721 hot mix asphalt (HMA) data points was established, including variables such as aggregate percentage, asphalt content, and specific gravity. Models were constructed using the PyCaret Python library, and their performance was assessed using metrics such as the mean absolute error (MAE) and coefficient of determination (R²). The CatBoost regression model outperformed the other models, achieving R² values of 0.835 and 0.845 for MS and MF, respectively. Additionally, Shapley values were used to quantify the variable effects on the predictions. This approach enables the efficient preselection of design variables, reducing the need for extensive laboratory testing and promoting sustainable construction practices. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206085293&doi=10.1016%2Fj.treng.2024.100282&partnerID=40&md5=4d7212ed37d92c04df84dfe7cf35a1ca,CE,Transportation,bitumen content; hma; marshall flow; marshall stability; prediction model; shap,Transportation Engineering
Journal Article,"B., Chakrabortty Badhon, R. K., Anavatti, S. G., Vanhoucke, M.",2025,IRAF-BRB: An explainable AI framework for enhanced interpretability in project risk assessment,Elsevier Ltd,285,,,"In high-stakes project risk assessment, balancing predictive accuracy with interpretability is critical to fostering stakeholder trust and supporting well-informed decision-making. This study presents the Interpretable Risk Assessment Framework with Belief Rule-Based Systems (IRAF-BRB), an Explainable AI (XAI) framework specifically designed to improve transparency, accountability, and accuracy in risk assessment. IRAF-BRB combines Interpretive Structural Modeling (ISM) to map and analyze interdependencies among risk factors with an optimized Belief Rule-Based (BRB) model. A modified Differential Evolution Covariance Matrix Self-Adaptation (DECMSA) algorithm is employed to enhance the predictive power of the BRB model while preserving interpretability, ensuring that stakeholders can both trust and understand the model's outputs. By transforming complex risk data into intuitive visualizations, the IRAF-BRB framework enables project managers to identify key risk drivers and anticipate cascading effects, leading to proactive risk mitigation. Experimental results demonstrate that IRAF-BRB reduces Mean Squared Error (MSE) to 4.09e−4 in predicting risk levels for high-rise construction projects, outperforming traditional BRB models such as Differential Evolution-based BRB (DE-BRB) (8.29e−4) and Particle Swarm Optimization-based BRB (PSO-BRB) (2.53e−3). The statistical significance of these results was confirmed via a two-sample t-test (p<0.05), establishing IRAF-BRB as a reliable and effective tool for accurate and interpretable risk assessment. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004558983&doi=10.1016%2Fj.eswa.2025.127979&partnerID=40&md5=e5a603d4a7163612c0146d40d896c520,CEM,Risk and Uncertainty,project risk assessment; explainable ai; belief rule base; interpretability; expert knowledge; optimization,Expert Systems With Applications
Journal Article,"B., Chakrabortty Badhon, R. K., Anavatti, S. G., Vanhoucke, M.",2025,A Multi-Module Explainable Artificial Intelligence Framework for Project Risk Management: Enhancing Transparency in Decision-making,Elsevier Ltd,148,,,"The remarkable advancements in machine learning (ML) have led to its extensive adoption in Project Risk Management (PRM), leveraging its powerful predictive capabilities and data-driven insights that support proactive decision-making. Nevertheless, the “black-box” nature of ML models obscures the reasoning behind predictions, undermining transparency and trust. To address this, existing explainable artificial intelligence (XAI) techniques, such as Local Interpretable Model-agnostic Explanations (LIME), Global Priors-based LIME (G-LIME), and SHapley Additive exPlanations (SHAP), have been applied to interpret black-box models. Yet, they face considerable limitations in PRM, including their inability to model cascading effects and multi-level dependencies among risk factors, suffering from inconsistencies due to random sampling, and failure to capture non-linear interactions in high-dimensional risk data. In response to these shortcomings, this paper proposes the Multi-Module eXplainable Artificial Intelligence framework for Project Risk Management (MMXAI-PRM), a novel approach designed to address the unique demands of PRM. The framework consists of three modules: the Risk Relationship Insight Module (RRIM), which models risk dependencies using a Knowledge Graph (KG); the Risk Factor Influence Analysis Module (RFIAM), which introduces a Conditional Tabular Generative Adversarial Network-aided Local Interpretable Model-agnostic Explanations using Kernel Ridge Regression (CTGAN-LIME-KR) to ensure explanation consistency and handle non-linearity; and the Visualization and Interpretation Module (VIM), which synthesizes these insights into an interpretable, chain-based representation. Extensive experiments demonstrate that MMXAI-PRM delivers more consistent, stable, and accurate explanations than existing XAI methods. By improving interpretability, it enhances trust in AI-driven risk predictions and equips project managers with actionable insights, advancing decision-making in PRM. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000295981&doi=10.1016%2Fj.engappai.2025.110427&partnerID=40&md5=26a317ec9a793091eca1ec7176834e7d,CEM,Risk and Uncertainty,explainable artificial intelligence; knowledge graph; conditional tabular generative adversarial networks; local interpretable model-agnostic explanations; project risk management,Engineering Applications of Artificial Intelligence
Journal Article,"N., Ravanshadnia Bagherian-Marandi, M., Akbarzadeh-T, M. R.",2021,Two-layered fuzzy logic-based model for predicting court decisions in construction contract disputes,Springer Science and Business Media B.V.,29,4,453–484,"The dynamic nature and increasing complexity of the construction industry have led to increased conflicts in construction projects. An accurate prediction of the outcome of a dispute resolution in courts could effectively reduce the number of disputes that would otherwise conclude by spending more money through litigation. This study aims to introduce a two-layered fuzzy logic model for predicting court decisions in construction contract disputes. 100 cases of construction contract disputes are selected from the courts of Iran. A questionnaire survey is then conducted to extract a set of fuzzy rules for identifying important decision parameters and expert knowledge. Accordingly, a two-layered fuzzy logic-based decision-making architecture is proposed for the prediction model. Furthermore, the fuzzy system is trained based on 10-fold cross-validation. Analysis of results indicates that 51 out of the 100 cases are filed after the dissolution and termination of the contract show a significant impact of these clauses as the root cause in construction contract disputes. Our results present a proposed hierarchical fuzzy system that can correctly predict nearly 60% of the test data. Also, we demonstrate a methodology of using argument before ML to establish interpretable AI models. Based on our findings, a fuzzy model with a hierarchical structure may be used as a simple and efficient method for predicting court decisions in construction contract disputes. © 2021 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099812241&doi=10.1007%2Fs10506-021-09281-9&partnerID=40&md5=8714d8bff9115a45743e67f5d71c136c,CEM,Contracting / Legal,Artificial intelligence; Construction project; Fuzzy expert systems; Judicial decisions; Litigation,Artificial Intelligence and Law
Journal Article,"A., Yekrangnia Baibordy, M.",2025,Predicting pull-out strength and failure modes of metal anchors embedded in masonry structures using explainable machine learning models and empirical equations,Elsevier B.V.,26,,,"One of the most widely used methods for the seismic retrofit of masonry structures is using anchorage systems. Tensile load (also referred to as pull-out force in this context) is one of the prominent loads applied to embedded anchors. Therefore, it is crucial to analyze and design the anchorage system to resist the pull-out force. Since experimental studies are time-consuming and labor-intensive, AI techniques have recently been successfully adopted to automate the prediction of the maximum pull-out force. While this approach has been successfully applied in concrete, there is still a significant gap in its application to masonry structures. This comprehensive study aims to bridge this gap by proposing suitable machine learning (ML) models and empirical equations that not only predict the pull-out force but also predict the various failure modes of the anchored steel rods in masonry. To achieve this, 13 distinct ML models, including both individual and ensemble models, were implemented for predicting each target variable. Optimization techniques were then used to fine-tune the hyperparameters, improving the performance of each ML model. A comparative study was conducted to evaluate the models against one another. Among all the models, the tuned Voting model, identified as the best for predicting pull-out force, showed an average error of about 3.0 kN and an R2 score of 93%. On the other hand, CatBoost demonstrated a strong performance with an accuracy of 90% and F1-score of about 89% in both the training and test data for predicting the failure modes. Further, Explainable ML (XML) provided valuable insights into the most important features involved in predicting both the pull-out force and failure modes. Moreover, the results showed that the proposed empirical equation produced satisfactory outcomes compared to other existing empirical equations in this context, achieving a total accuracy of 72% and an error of 7.5 kN. As a result of this study, accurate predictive models have been introduced that require only readily available information from any construction site to proceed. Furthermore, by comparing the capability of the XML used in this study with other analytical parametric studies such as finite element analysis, it was found that XML can serve as a suitable alternative for identifying the most important features of the pull-out behavior of metal anchors in masonry. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005207320&doi=10.1016%2Fj.rineng.2025.105287&partnerID=40&md5=8e1e1c444890edada61da722f480c6b8,CE,Structural,masonry; anchorage system; pull-out; bond strength; explainable machine learning; empirical equations,Results in Engineering
Journal Article,"A., Fayaz Bansal, S. J., Krishnan, N. M. A., Kota, S. H., Nema, A. K.",2025,Interpretable Machine Learning for Cost Estimation in Underground Pipe Installation,American Society of Civil Engineers (ASCE),16,3,,"This study explores the application of interpretable machine learning (ML) models for accurately estimating excavation and installation costs associated with underground high-density polyethylene (HDPE) pipeline projects. Leveraging design data from seven projects across Asia, the research evaluates six regression models: linear regression, lasso regression, elastic net regression, random forest regression, XGBoost, and Gaussian process regression. Model performance was assessed using the R-squared (R2) metric, with XGBoost and Gaussian process regression achieving the highest R2 scores (above 0.89) on the testing data sets. Shapley additive explanations (SHAP) analysis reveals that pipe length, diameter, and depth are the most significant predictors, with pipe length contributing most substantially to cost estimations. The results indicate that ML models, particularly XGBoost and Gaussian process regression and stacked models, provide reliable cost predictions when diverse, high-quality data are available, offering a valuable tool for early-stage project budgeting and planning in the pipeline industry. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002009834&doi=10.1061%2FJPSEA2.PSENG-1794&partnerID=40&md5=a5ffcf54eeb4d4d757b14e76511e1439,CEM,Cost Estimation / Bidding,machine learning; cost estimation; underground pipe network; excavation cost; pipe laying cost; shap; interpretable machine learning,Journal of Pipeline Systems Engineering and Practice
Journal Article,"X., Han Bao, T., Huo, Y.",2024,Research on carbon emission prediction and influencing factors in the embodied stage of railway track engineering,Central South University Press,21,10,4299–4310,"Under the national “double carbon” strategic goal, the low-carbon transformation of the railway field is imperative. As an important part of railway engineering, the carbon emission generated in the embodied stage is an important source of carbon emission in railway engineering. In order to quantify carbon emissions in the embodied stage of railway track engineering and realize intelligent analysis, this paper established a calculation model of carbon emissions in the embodied stage of railway track engineering, and it also proposed a model for predicting carbon emission and analyzing influencing factors based on machine learning algorithm. First, the research boundary of embodied stage was defined, the railway track engineering was decomposed, and the carbon emission factor method was used to establish the carbon emission calculation model with the main process as the basic accounting unit. Second, the Light Gradient Boosting Machine (LightGBM) was used to build a carbon emission prediction model. An interpretable machine learning model (SHAP) was introduced to analyze the contribution of influencing factors to carbon emissions. By taking a railway track project in a southwestern mountainous area as an example, a typical unit rail section was selected to calculate carbon emissions. The results shows that the total carbon emission of 1 km length is 1 290.94 t, and the carbon emissions of material production stage accounted for the largest proportion of about 87.21%. The carbon emissions of track-laying and roadbed are relatively high, which are 47.44% and 46.44%, respectively. The LIGTBM-SHAP model was verified by extracting relevant characteristics of carbon emissions from the track engineering as influencing factors. The numerical value of each evaluation index shows that the model has a good prediction effect, and the importance rank of influencing factors in descending order are track structure form, line section, sleeper type or track plate, construction days, section slope, and section transportation distance. In the analysis of results, the single factor feature dependence graph was used to clarify the impact of categorical variables or numerical changes of each influencing factor on carbon emissions. The research results provide a more intelligent and comprehensive research model for carbon emission calculation, prediction, and analysis of railway track engineering, and they also provide a reference for carbon emission reduction work in railway construction. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207475197&doi=10.19713%2Fj.cnki.43-1423%2Fu.T20240050&partnerID=40&md5=0d7880cc0a7d6747a3cb6504f3cde89d,CE,Transportation,carbon emission projections; embodied stage; gradient boosting tree algorithm; influencing factors; interpretable machine learning,Journal of Railway Science and Engineering
Journal Article,"Y., Huang Bao, Z., Yin, G., Ren, S., Yan, X., Qi, J.",2025,Quantifying the impact of building material stock and green infrastructure on urban heat island intensity,Elsevier Ltd,280,,,"The urban heat island (UHI) effect has become one of the most critical environmental consequences of urbanization, fundamentally linked to the accumulation of building material stocks and modification of natural surfaces. This study develops a comprehensive quantitative framework to analyze the combined effects of building material stock and green infrastructure on UHI intensity in Beijing, China. Using remote sensing data and interpretable machine learning approaches, particularly partial dependence analysis, we reveal the nonlinear relationships and interaction effects between urban physical characteristics and UHI intensity. The results identify a critical threshold of brick material stock at approximately 3×108 tons, beyond which UHI intensity increases sharply. Our findings advance the understanding of urban thermal environment regulation by quantifying both the individual and interactive effects of material stocks and green infrastructure, providing insights for optimizing urban heat mitigation strategies. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004640056&doi=10.1016%2Fj.buildenv.2025.113068&partnerID=40&md5=f9a47a58af2a25aced3eb8dfd8ff953c,CE,Environmental,urban heat island; material stocks; machine learning; green infrastructure,Building and Environment
Journal Article,"C. A. M., Lee Barquilla, J.",2025,Diagnostic Analysis of Crosswalk Safety Hazards in Pedestrian Environments: A SHAP-Enhanced Machine Learning Approach With Street-View Imagery,Institute of Electrical and Electronics Engineers Inc.,13,,135589–135608,"This study investigated the influence of built environment features on crosswalk safety in dense urban settings, with a focus on visual streetscape characteristics extracted from street-view imagery using both semantic and instance segmentation. It used data from 36,750 crosswalks in Seoul, South Korea, to rigorously evaluate multiple machine learning algorithms for predicting pedestrian crash risk. Among the models assessed, the Random forest (RF) demonstrated the highest precision, aligning with the objective of enhancing pedestrian safety through accurate risk identification. The RF model enhanced by SHapley Additive exPlanations (SHAP) achieved strong predictive performance (precision: 0.91), and SHAP analysis identified visual features, particularly sky openness ratio, building coverage, and sidewalk ratio, as influential factors affecting crash risk. A lower sky openness ratio combined with a higher building ratio was associated with increased crash likelihood, whereas greater sidewalk coverage and the presence of traffic control measures, including traffic lights and crosswalk time indicators, mitigated risk. Interaction effects further highlighted the complexity of urban safety, showing that combinations of streetscape and infrastructural elements can amplify or reduce hazards. These results highlight the importance of combining visual and structural data for thorough risk assessment and further the use of interpretable machine learning in urban safety research. The findings imply that to address particular combinations of built environment elements that increase the risk of crosswalk crashes, policy and planning initiatives should concentrate on context-sensitive interventions, particularly by placing bus stops strategically, maintaining tree canopies for visibility, clearing visual clutter, and improving pedestrian infrastructure. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011716795&doi=10.1109%2FACCESS.2025.3592445&partnerID=40&md5=f410d2537710db591e429bb630f364ab,CE,Transportation,built environment; crosswalk safety; interpretable machine learning; pedestrian environment; street view imagery,IEEE Access
Journal Article,"A., Jibril Bashir, M. M., Jibrin, U. M., Abba, S. I., Malami, S. I.",2025,A new strategy using intelligent hybrid learning for prediction of water binder ratio of concrete with rice husk ash as a supplementary cementitious material,Springer Nature,10,1,,"It is important to point out that the precise prediction of water binder ratio “w/b ratio” or “W_C” is indispensable for gaining the desirable characteristics of strength and duration of concrete constructions. This research offers a new method for w/b ratio prediction based on state-of-art machine learning (ML) algorithms accompanied with explainable artificial intelligence (XAI) methods. The main aspect of the research approach is described using 192 database containing different mix design parameters and the environmental conditions. With the help of ensemble learning models such as Random forest (RF), Recurrent Neural Network (RNN) model, Relevance Vector Machine (RVM) and Response surface methodology (RSM), the prediction model has performed better than the empirical methods with RVM-M3 surpass all other models with the highest R value equal to 0.9992 in calibration phase and RF-M3 surpassing other model combinations in verification phase with R value equal to 0.9984. Furthermore, the integration of XAI revealed the key influential variables affecting the w/b ratio prediction and the main influential variables related to w/b ratio as well as their importance are determined, where Cement (Ce) identified as the most impactful parameter that improved the prediction accuracy of RF-M3 model. The results prove that the proposed method increases the prediction accuracy and provides engineers with a dependable means of augmenting concrete mix designs to enhance concrete’s durability performance and sustainability. This research expands the understanding and principles of concrete technology, hence facilitating the use of AI-based solutions in civil engineering practices and other relevant domains. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211184106&doi=10.1007%2Fs41024-024-00541-0&partnerID=40&md5=aba386cfb4dc47526341f7c98620e24b,CE,Materials,water binder ratio; durability; compressive strength; shrinkage; sustainability,Journal of Building Pathology and Rehabilitation
Journal Article,"G., Aydın Bekdaş, Y., Cakiroglu, C., Işikdağ, U.",2025,Leveraging Neural Networks and Explainable AI for Cost-Effective Retaining Wall Design,Tech Science Press,143,2,1763–1787,"Retaining walls are utilized to support the earth and prevent the soil from spreading with natural slope angles where there are differences in the elevation of ground surfaces. As the need for retaining structures increases, the use of retaining walls is increasing. The retaining walls, which increase the stability of levels, are economical and meet existing adverse conditions. A considerable amount of retaining walls is made from steel-reinforced concrete. The construction of reinforced concrete retaining walls can be costly due to its components. For this reason, the optimum cost should be targeted in the design of retaining walls. This study presents an artificial neural network (ANN) model developed to predict the optimum dimensions of a retaining wall using soil properties, material properties, and external loading conditions. The dataset utilized to train the ANN model is generated with the Flower Pollination Algorithm. The target variables in the dataset are the length of the heel (y<inf>1</inf>), length of the toe (y<inf>2</inf>), thickness of the stem (top) (y<inf>3</inf>), thickness of the stem (bottom) (y<inf>4</inf>), foundation base thickness (y<inf>5</inf>) and cost (y<inf>6</inf>) and these are estimated by utilizing an ANN model based on the height of the wall (x<inf>1</inf>), material unit weight (x<inf>2</inf>), wall friction angle (x<inf>3</inf>), surcharge load (x<inf>4</inf>), concrete cost per m3 (x<inf>5</inf>), steel cost per ton (x<inf>6</inf>) and the soil class (x<inf>7</inf>). The model is formulated and trained as a multi-output regression model, as all outputs are numeric and continuous. The training and evaluation of the model results in a high prediction performance (R2 > 0.99). In addition, the impacts of different input features on the model predictions are revealed using the SHapley Additive exPlanations (SHAP) algorithm. The study demonstrates that when trained with a large dataset, ANN models perform very well by predicting the optimal cost with high performance. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007969554&doi=10.32604%2Fcmes.2025.063909&partnerID=40&md5=8c76594a210c8a45b6194209aea3e632,CE,Geotechnical,retaining wall; neural networks; optimum design; explainable machine learning,Computer Modeling in Engineering & Sciences (CMES)
Journal Article,"M., Oyedele Bilal, L. O.",2020,Guidelines for applied machine learning in construction industry—A case of profit margins estimation,Elsevier Ltd,43,,,"The progress in the field of Machine Learning (ML) has enabled the automation of tasks that were considered impossible to program until recently. These advancements today have incited firms to seek intelligent solutions as part of their enterprise software stack. Even governments across the globe are motivating firms through policies to tape into ML arena as it promises opportunities for growth, productivity and efficiency. In reflex, many firms embark on ML without knowing what it entails. The outcomes so far are not as expected because the ML, as hyped by tech firms, is not the silver bullet. However, whatever ML offers, firms urge to capitalise it for their competitive advantage. Applying ML to real-life construction industry problems goes beyond just prototyping predictive models. It entails intensive activities which, in addition to training robust ML models, provides a comprehensive framework for answering questions asked by construction folks when intelligent solutions are getting deployed at their premises to substitute or facilitate their decision-making tasks. Existing ML guidelines used in the IT industry are vastly restricted to training ML models. This paper presents guidelines for Applied Machine Learning (AML) in the construction industry from training to operationalising models, which are drawn from our experience of working with construction folks to deliver Construction Simulation Tool (CST). The unique aspect of these guidelines lies not only in providing a novel framework for training models but also answering critical questions related to model confidence, trust, interpretability, bias, feature importance and model extrapolation capabilities. Generally, ML models are presumed black boxes; hence argued that nobody knows what a model learns and how it generates predictions. Even very few ML folks barely know approaches to answer questions asked by the end users. Without explaining the competence of ML, the broader adoption of intelligent solutions in the construction industry cannot be attained. This paper proposed a detailed process for AML to develop intelligent solutions in the construction industry. Most discussions in the study are elaborated in the context of profit margin estimation for new projects. © 2019 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075778987&doi=10.1016%2Fj.aei.2019.101013&partnerID=40&md5=e6f3af53efc1d5f9cd45956ff4ebd1bc,CEM,Cost Estimation / Bidding,Applied machine learning; profit margin forecasting; construction simulation tool; interpretable machine learning; predictive modelling,Advanced Engineering Informatics
Journal Article,"W., Khan Bin Inqiad, M. S., Alarifi, S. S.",2025,Reliable determination of peak shear strength of H-shaped concrete squat walls using explainable machine learning techniques,Elsevier Ltd,76,,,"Flanged reinforced concrete walls also known as H-shaped walls are frequently used in nuclear facilities and conventional buildings due to their substantial lateral strength and stiffness in both directions. These walls mostly fail in shear, and it is essential to accurately estimate their peak shear strength (V<inf>p</inf>). However, the provisions of existing building codes to determine peak shear strength have significant limitations such as exclusion of the influence of flanges and consideration of insufficient input parameters. Therefore, this study aimed to construct predictive models for H-shaped walls using machine learning techniques like Bagging Regressor (BR), Gene Expression Programming (GEP), and Extreme Gradient Boosting (XGB), based on data gathered from existing literature. The gathered data had twelve inputs including shear span ratio (h<inf>w</inf>/l<inf>w</inf>), the ratio of flange thickness to web thickness (t<inf>f</inf>/t<inf>w</inf>), and loading type (M) etc. Out of all the algorithms, only GEP depicted its output as an equation. The performance of the algorithms was checked using error metrices like Objective Function (OF), and coefficient of determination (R2) etc. which showed that XGB exhibited the highest accuracy having the testing R2 equal to 0.99. Additionally, shapely (SHAP), Individual Conditional Expectation (ICE), and partial dependence plots (PDP) analysis were employed which showed that flange length, loading type, and shear span ratio are some of the most contributing variables to determine V<inf>p</inf>. Furthermore, a graphical user interface (GUI) has been developed to efficiently compute V<inf>p</inf> of RC squat H-shaped walls to help professionals in the civil engineering industry. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002402517&doi=10.1016%2Fj.istruc.2025.108802&partnerID=40&md5=094ce2544d4fca208dedad30d481d237,CE,Structural,Peak Shear Strength; H-shaped walls; Machine Learning; Shapely Analysis; Gene Expression Programming,Structures
Journal Article,"W., Khan Bin Inqiad, M. S., Mehmood, Z., Khan, N. M., Bilal, M., Sazid, M., Alarifi, S. S.",2025,Utilizing contemporary machine learning techniques for determining soilcrete properties,Springer Nature,18,1,,"Soilcrete is an innovative construction material made by combining naturally occurring earth materials with cement. It can be effectively used in areas where other construction materials are not readily available due to financial or environmental reasons since soilcrete is made from readily available natural clay. It can also help to cut down the greenhouse gas emissions from the construction industry by encouraging the use of resources that are locally available. Thus, it is imperative to reliably predict different properties of soilcrete since the accurate determination of these properties is crucial for the widespread use of soilcrete materials. However, the laboratory determination of these properties is subjected to significant time and resource constraints. As a result, this research was undertaken to provide empirical prediction models for the density, shrinkage, and strain of soilcrete mixes using two machine learning algorithms: Gene Expression Programming (GEP) and Extreme Gradient Boosting (XGB). The analysis revealed that XGB-based predictions correlated more with real-life values than GEP having training R2=0.999\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$${\text{R}}<^>{2}=0.999$$\end{document} for both density and shrinkage prediction and R2=0.944\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$${\text{R}}<^>{2}=0.944$$\end{document} for strain prediction. Moreover, several explanatory analyses including individual conditional expectation (ICE) analysis and shapely analysis were done on the XGB model which showed that water-to-binder ratio, metakaolin content, and modulus of elasticity are some of the most important variables for forecasting soilcrete materials properties. Furthermore, an interactive graphical user interface (GUI) has been developed for effective utilization in civil engineering industry to forecast these properties of soilcrete materials.",,CE,Materials,Machine learning; Soilcrete materials; Metakaolin; Mechanical properties; Graphical user interface; Explainable artificial intelligence,Earth Science Informatics
Journal Article,"R., Sprenger Bischof, M., Riedel, H., Bumann, M., Walczok, W., Drass, M., Kraus, M. A.",2023,Temp-AI-Estimator: Interior temperature prediction using domain-informed Deep Learning,Elsevier B.V,297,,,"Approximately 40% of total energy demand in the European Union is consumed by the residential buildings sector, thus also significantly contributing to carbon dioxide emissions. Circa 28% of this energy demand is attributed to space heating and cooling, primarily influenced by the building's envelope and need to ensure indoor thermal comfort. Given this significant energy consumption, there is an urgent imperative to explore energy-saving strategies and develop tools to assess the effects of various design alternatives, with a focus on wall and roof characteristics. While existing white and black-box predictive models lack generalisation capabilities, the goal of this study is to develop and train a domain-informed grey-box Deep Learning model called Temp-AI-Estimator to predict the indoor temperature of buildings and containers (acting as surrogates for residential or office buildings as well as intermodal containers) based on measurements of external meteorological conditions (e.g. exterior temperature, humidity, etc.), as well as physical properties due to building construction set-ups in questions (“ThermoProtect” (thin coating), “ThermoActive” (thick coating) and thermal insulation). The major difficulty lies in making the model generalise beyond the three geographical locations included in the dataset (Berlin, Abu Dhabi, Texel). Experiments with LSTMs and Transformers as baseline models showed overfitting on the particular conditions at the sites in the training set, while failing to generalise to a new out-of-sample location. We propose to pass the model the numerical derivatives of the time-sequences, as these are less location-specific. The estimation of the necessary initial/final conditions is delegated to a very small network to minimise the risk of overfitting. Furthermore, we included a physically motivated module for modelling the latency and difference in amplitudes between exterior conditions and interior temperatures, informed by numerical approximation schemes of the differential equation of thermal conduction. Our experiments show that our domain-informed network achieves an increase in accuracy of almost 40% in addition to yielding results that can easily be inspected by human experts due to interpretability and explainability. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167793422&doi=10.1016%2Fj.enbuild.2023.113425&partnerID=40&md5=49d3c15a82d3d1a107ec229efaa25437,CE,Energy,Prediction; Machine and Deep Learning; LSTM; Transformer; Domain-informed AI; Explainable AI; Time-series; Coating; Buildings; Solar reflection; Energy savings; Cooling systems; City cooling; Facade cooling,Energy and Buildings
Journal Article,"B. M., Menapace Brentan, A., Oberascher, M., Herrera, M., Sitzenfrei, R.",2025,Enhancing explainable AI with graph signal processing: Applications in water distribution systems,Elsevier Ltd,285,,,"Water distribution systems (WDS) face complex challenges, including real-time monitoring, operational efficiency, and resilience under varying hydraulic conditions. Artificial intelligence (AI) offers promising solutions but is often held back by its lack of transparency. This paper presents a novel framework integrating Explainable AI (XAI) with graph signal processing to enhance the interpretability of AI models applied to WDS. Specifically, it models multilayer perceptrons as dynamic, weighted, directed graphs to analyse hydraulic states. Using eigencentrality as a central graph metric, this approach identifies key drivers influencing model predictions, offering insights into both global and local system behaviour. The methodology is validated using a metamodel for hydraulic state estimation, leveraging real-world WDS benchmarks. Comparative analyses with state-of-the-art XAI approaches, such as the SHapley Additive exPlanations (SHAP values) and Integrated Gradients (IG), demonstrate the robustness, adaptability, and computational efficiency of the proposed novel framework, with processing times that are over 70 times faster. This enables real-time applications in digital twins for WDS. Moreover, the methodology supports sensor prioritization and maintenance strategies, emphasizing critical components for system resilience. The results highlight the synergy between graph theory and XAI, showcasing a scalable, transparent tool for sustainable urban water management. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010164878&doi=10.1016%2Fj.watres.2025.124022&partnerID=40&md5=7b13315fae8a7b6a3ab76c601b845d99,CE,Water,Explainable AI; Graph signal processing; Urban water; Decision making; Asset management,Water Research
Journal Article,"C., Batool Cakiroglu, F., Islam, K., Nehdi, M. L.",2024,Explainable ensemble learning predictive model for thermal conductivity of cement-based foam,Elsevier Ltd,421,,,"Cement-based foam has emerged as a strong contender in sustainable construction owing to its superior thermal and sound insulation properties, fire resistance, and cost-effectiveness. To effectively use cement-based foam as a thermal insulation material, it is important to accurately predict its thermal conductivity. The current study aims at coining an accurate methodology for predicting the thermal conductivity of cement-based foam using state-of-the-art machine learning techniques. A comprehensive experimental dataset of 504 data points was developed and used for training ensemble learning models including XGBoost, CatBoost, LightGBM and Random Forest. The independent variables of this dataset affecting the thermal conductivity are the cast density, percentage of pozzolan, porosity, percentage of moisture, and duration of hydration in days. Using the Isolation Forest algorithm proved effective in detecting and eliminating outliers in the dataset. All the ensemble learning techniques explored in this study achieved superior predictive accuracy with a coefficient of determination greater than 0.98 on the test dataset. The influence of the input features on the thermal conductivity was visualized using the SHapley Additive exPlanations (SHAP) approach and individual conditional expectation (ICE) plots. The cast density had the greatest effect on thermal conductivity. The explainable machine learning models demonstrated superior accuracy, efficiency, and reliability in estimating the thermal insulation of cement-based foam, opening the door for wider acceptance of this material in sustainable energy efficient construction. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186685439&doi=10.1016%2Fj.conbuildmat.2024.135663&partnerID=40&md5=c4d8b1f42a909a2c9874ffc1535c1bb9,CE,Materials,Cement-based foam; Thermal conductivity; Model prediction; Machine learning; Explainable; Ensemble Learning,Construction and Building Materials
Journal Article,"C., Batool Cakiroglu, F., Sangi, A. J., Fatima, B., Nehdi, M. L.",2025,Explainable machine learning predictive model for mechanical strength of recycled ceramic tile-based concrete,Elsevier Ltd,44,,,"Valorizing industrial byproducts in construction applications is a promising approach for enhancing sustainability. Global annual production of ceramic waste including broken tiles is a considerable challenge. Yet, such ceramic tile waste has great potential in sustainable concrete production, for instance as fine and coarse aggregate. Effective use of ceramic tile waste in concrete requires accurate prediction of recycled tile concrete mechanical strength. This study deploys state-of-the-art machine learning techniques for predicting the compressive and tensile strength of ceramic tile-based concrete. The authors recently performed an extensive experimental program on the mechanical characterization of ceramic tile-based concrete, allowing to build a comprehensive database of 252 data points with varying key mixture proportions. The latter was used to develop a data-driven machine learning (ML) modeling framework for predicting the mechanical properties of the concrete using XGBoost, CatBoost, LightGBM, and Extra Trees regressors. The independent variables of the dataset affecting mechanical strength included the cast density, percentage of ceramic tiles used as coarse and fine aggregate, water-to-cement ratio, water absorption capacity, and hydration age. The influence of different input features on the model predictions was visualized using SHAP feature importance plots. Ultimately, a machine learning-based and user-friendly graphical interface was created and made available through the Streamlit platform to aid in the design of ceramic tile-based sustainable concrete. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000166618&doi=10.1016%2Fj.mtcomm.2025.112139&partnerID=40&md5=b0bcb4f60f842d46ee25c56e04297772,CE,Materials,Ceramic tile; Aggregate; Mechanical strength; Predictive model; Machine learning; Explainable ensemble learning; Graphical interface,Materials Today Communications
Journal Article,"C., Islam Cakiroglu, K., Bekdaş, G., Işikdağ, U., Mangalathu, S.",2022,Explainable machine learning models for predicting the axial compression capacity of concrete filled steel tubular columns,Elsevier Ltd,356,,,"Concrete-filled steel tubular (CFST) columns have been popular in the construction industry due to enhanced mechanical properties such as higher strength and ductility, higher seismic resistance, and aesthetics. Extensive experimental, numerical and analytical studies have been conducted in the past few decades to assess the structural response of CFST columns under various loading conditions. However, there is still uncertainty in predicting the capacity of CFST columns, and most of the current codes are conservative. In this paper, data-driven machine learning (ML) models have been developed to predict the axial compression capacity of rectangular CFST columns. An extensive database of 719 experiments was collected from literature and is randomly used to train, test, and validate the ML models. Seven ML models, namely lasso regression, random forest, Adaptive Boosting (AdaBoost), Gradient Boosting Machine (GBM), Light Gradient Boosting Machine (LightGBM), Extreme Gradient Boosting (XGBoost), and Categorical Gradient Boosting (CatBoost), are evaluated to predict the compression capacity of CFST stub columns under axial load. The performance of the different ML models in predicting the compressive strength of CFST columns is compared by different code equations prevalent in different parts of the world. It is found that LightGBM and CatBoost models performed better with an accuracy of 97.9% and 98.3%, respectively, compared to the existing design codes in predicting the capacity of CFST columns. Feature importance analyses and SHapley Additive exPlanations (SHAP) explain the ML model performances and make the developed models interpretable. Resistance factor is determined using the best performing ML model for compressive strength prediction of CFST stub columns following AISC 360–16 code provision. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139076371&doi=10.1016%2Fj.conbuildmat.2022.129227&partnerID=40&md5=e1addc982137aeb6cf63138520d8578f,CE,Structural,Explainable machine learning; Artificial intelligence (AI); Composite column; Compressive capacity; Resistance factor,Construction and Building Materials
Journal Article,"A., Ruggieri Cardellicchio, S., Nettis, A., Renò, V., Uva, G.",2023,Physical interpretation of machine learning-based recognition of defects for the risk management of existing bridge heritage,Elsevier Ltd,149,,,"The challenge of the research work presented in the paper is to combine the growing interest in monitoring the health condition of existing bridge heritage through systematic and periodic visual inspections with automated recognition of typical bridge defects, which can greatly facilitate the assessment of defect evolution over time. The study focused on the automated identification of defects in existing Reinforced Concrete (RC) bridges exploiting different Deep Learning (DL) approaches and techniques to interpret the obtained predictions. Ensuring the safety of infrastructures is typically a technical and economic issue. Still, in the case of the engineering infrastructure heritage, there are existing bridges and viaducts with a high historical, cultural, and symbolic value. For them, accurate knowledge and characterization of possible degradation processes become particularly important in order to define intervention strategies that combine safety and conservation requirements. With the aim to develop systematic and non-invasive investigation protocols for continuous and effective control of defects and their evolution, a database of existing RC bridge defect images was collected, and the most recurrent defect typologies were classified by domain experts. Some existing Convolutional Neural Networks (CNNs) algorithms were applied to the dataset for automatically recognizing all defects, but the specific novel contribution of the research work is the interpretation of the obtained results in a form that is humanly explainable and directly implementable in new tools for bridge inspections. To interpret the results, Class Activation Maps (CAMs) approaches were employed within available eXplainable Artificial Intelligence (XAI) techniques, which allow to observe the activation zones and nearly perfectly highlight the type of specific defect in a given image. The obtained results, besides suggesting which network works better than others and if the specific defect is effectively recognized, have been evaluated through a quasi-quantitative procedure that compared a qualitative assessment of the CNNs models reliability with two novel indexes representing new explaining metrics of the obtained results. In the end, the outcomes of the proposed study were observed also in a real-life case study. The proposed discussion opens new scenarios in the application of these techniques for supporting road management companies and public organizations in the evaluation of the road networks health state. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151658094&doi=10.1016%2Fj.engfailanal.2023.107237&partnerID=40&md5=bc8108e3aaee51e7986a0ed3abf7a3a5,CE,Structural,Civil engineering; Existing RC bridges; Reinforced concrete; Defect detection; Machine-learning; Degradation; Material defect,Engineering Failure Analysis
Journal Article,"H., Liu Chen, J., Shen, G. Q., Feng, Z.",2025,Control of existing tunnel deformation caused by shield adjacent undercrossing construction using interpretable machine learning and multiobjective optimization,Elsevier B.V.,170,,,"A hybrid intelligent framework is proposed in this paper to reduce the existing tunnel deformation caused by shield adjacent undercrossing construction (SAUC). A Bayesian optimization natural gradient boosting (BO-NGBoost) model for existing tunnel deformation prediction is developed, and the Shapley additive explanations (SHAP) approach is used to analyze the interpretability of the prediction model. The multiobjective evolutionary algorithm based on decomposition (MOEA/D) is used to optimize the construction parameters. The applicability and validity of the proposed method are tested in a case study from the Wuhan Metro. The results indicate that (1) the established BO-NGBoost existing tunnel deformation prediction model shows high accuracy. (2) Through SHAP analysis, the importance of each input parameter to the existing tunnel deformation is identified, and the key shield optimization parameters are defined. (3) By using the developed BO-NGBoost-MOEA/D algorithm to optimize the key parameters, the existing tunnel deformation is effectively controlled. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212935308&doi=10.1016%2Fj.autcon.2024.105943&partnerID=40&md5=288858a323091ef59436691300d0331e,CE,Geotechnical,Shield adjacent undercrossing construction; Existing tunnel deformation; SHAP; BO-NGBoost-MOEA/D; Multiobjective optimization,Automation in Construction
Journal Article,"K., Zhang Chen, Y., Hu, N., Ye, C., Ma, J., Zheng, T.",2024,Cost prediction for water reuse equipment using interpretable machine learning models,Elsevier Ltd,63,,,"During the initial stages of engineering projects management, the accurate estimation of equipment costs plays a crucial role in determining project approval decisions. However, there is a significant research gap regarding the estimation of investment costs for water reuse equipment. Additionally, cost predictions for such construction projects often suffer from limitations, including low accuracy, limited generalizability, and inefficiency. Advanced machine learning (ML) methods, renowned for their ability to model complex decision-making processes, offer powerful solutions. In this study, four traditional models and four ensemble models were employed to predict the cost of water reuse equipment. The results demonstrated that the ensemble models exhibited significantly superior predictive performance to that of traditional models with the three boosting ensemble models achieving the best performance (traditional models: training R2 = 29.77 %–94.85 %, testing R2 = 30.62 %–71.72 %; boosting ensemble models: training R2 = 97.42 %, testing R2 = 82.16 %–93.79 %). Furthermore, this study simplified the features of predictive models and identified the key variables that influence the cost of water reuse equipment using Shapley additive explanations (SHAP) method. The retrained ensemble models re-constructed based on the selected variables achieved significant predictive performance, with the Gradient Boosting Decision Tree (GBDT) outperforming the other models (training set R2 = 97.37 %, testing set R2 = 93.86 %). The water quantity, inflow conductivity, outflow conductivity, and recovery rate emerged as critical factors affecting the cost of water reuse equipment. Overall, the methods proposed in this study can enhance the versatility of cost prediction processes in environmental engineering scenarios, particularly those concerning the construction costs of water treatment equipment. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193451666&doi=10.1016%2Fj.jwpe.2024.105474&partnerID=40&md5=5cac12bd8d34c10d6e2df42683397888,CEM,Cost Estimation / Bidding,Water reuse equipment; Machine learning; Cost prediction; Feature selection,Journal of Water Process Engineering
Journal Article,"L., Xu Chen, C., Lim, W. H., Sharma, A., Tiang, S. S., Chong, K. S., El-Kenawy, E. S. M., Alhussan, A. A., Eid, M. M., Khafaga, D. S.",2025,Transparent and reliable construction cost prediction using advanced machine learning and explainable AI,Elsevier B.V.,70,,,"Accurate construction cost prediction is vital for project management, influencing budgeting, resource allocation, and overall success. This study proposes a comprehensive framework that combines machine learning models, uncertainty quantification through Confidence Intervals, and explainable AI techniques using SHAP (SHapley Additive exPlanations) to enhance transparency and decision-making. Ten machine learning models, including Ridge Regression, Lasso Regression, Elastic Net, K-Nearest Neighbor Regression, and advanced ensemble methods such as XGBoost, CatBoost, and HistGradient Boosting, were evaluated on the RSMeans dataset. Among these, HistGradient Boosting achieved the best performance on the testing dataset. Beyond traditional metrics, Confidence Intervals quantified prediction reliability, and SHAP identified critical cost drivers like “Formwork” and “Tributary Area,” enabling interpretable and robust prediction. This study highlights the potential of machine learning models to revolutionize construction cost estimation by integrating predictive accuracy, uncertainty analysis, and explainability. The proposed framework supports resource efficiency and enables process innovation in cost management. It also contributes to the advancement of sustainable building practices, offering a strong foundation for future research and promoting the adoption of machine learning-based solutions with enhanced transparency and confidence. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011949080&doi=10.1016%2Fj.jestch.2025.102159&partnerID=40&md5=380cef8ae7312821f8c72ef681ca2de3,CEM,Cost Estimation / Bidding,Construction cost prediction; Machine learning; Confidence intervals; Explainable AI; Ensemble methods; SHAP,"Engineering Science and Technology, an International Journal"
Journal Article,"L., Zhang Chen, H. Y., Liu, S. W., Chan, S. L.",2023,SECOND-ORDER ANALYSIS OF BEAM-COLUMNS BY MACHINE LEARNING-BASED STRUCTURAL ANALYSIS THROUGH PHYSICS-INFORMED NEURAL NETWORKS,Hong Kong Institute of Steel Construction,19,4,411–420,"The second-order analysis of slender steel members could be challenging, especially when large deflection is involved. This paper proposes a novel machine learning-based structural analysis (MLSA) method for second-order analysis of beam-columns, which could be a promising alternative to the prevailing solutions using over-simplified analytical equations or traditional finite-element-based methods. The effectiveness of the conventional machine learning method heavily depends on both the qualitative and the quantitative of the provided data. However, such data are typically scarce and expensive to obtain in structural engineering practices. To address this problem, a new and explainable machine learning-based method, named Physics-informed Neural Networks (PINN), is employed, where the physical information will be utilized to orientate the learning process to create a self-supervised learning procedure, making it possible to train the neural network with few or even no predefined datasets to achieve an accurat e approximation. This research extends the PINN method to the problems of second-order analysis of steel beam-columns. Detailed derivations of the governing equations, as well as the essential physical information for the training process, are given. The PINN framework and the training procedure are provided, where an adaptive loss weight control algorithm and the transfer learning technic are adopted to improve numerical efficiency. The practicability and accuracy of which are validated by four sets of ver ification examples. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183903194&doi=10.18057%2FIJASC.2023.19.4.10&partnerID=40&md5=265da7ccfa5f4abfe4d520b2a141b704,CE,Structural,Beam-columns; Physics-informed neural networks; Second-order analysis; Machine learning,Advanced Steel Construction
Journal Article,"Q. Y., Hu Chen, G., Wu, J.",2024,Comparative study on the prediction of the unconfined compressive strength of the one-part geopolymer stabilized soil by using different hybrid machine learning models,Elsevier Ltd,21,,,"With the development of green, low-carbon, and sustainable economic systems, the issues of high pollution and energy consumption in construction materials have become increasingly prominent. This study focuses on adopting one-part geopolymer (OPG) in soil stabilization for underground engineering, which exhibits environmental and low-carbon advantages. The unconfined compressive strength (UCS) serves as a crucial parameter for assessing stabilized soil's performance. However, it is necessary to conduct a large number of experiments, inducing high costs and time consumption. In this study, one multiple linear regression model, one Decision Tree (DT) model, five ensemble machine learning (ML) models (i.e. Random Forest [RF], Extra Tree [ET], Gradient Boosting [GB], Gradient Boosting Decision Tree [GBDT], and Extreme Gradient Boosting [XGBoost]), and hybrid models of those single ensemble models with Particle Swarm Optimization (PSO) (i.e. PSO-RF, PSO-ET, PSO-GB, PSO-GBDT, and PSO-XGBoost) were adopted and compared to achieve better prediction on the UCS of the OPG-stabilized soil. Furthermore, the interpretable method including SHAP and PDP (1D and 2D), was employed to investigate the precise mechanisms by which input parameters influenced the output label. The results revealed that the multiple linear regression model delivered the lowest accuracy, and PSO-XGBoost and PSO-ET exhibited the best performance on the prediction of the UCS with R2 value of 0.9964 and 0.9928, respectively. In addition, Curing time exerted the most significant impact on the UCS, followed by FA/GGBFS, Molarity, Water/Binder, and NaOH/Precursor. Compared to the SHAP method, the PDP offered a more intuitive approach to reveal the relationship between the inputs and output. The outcome of the study shed new light on the application of ML models in the prediction of the OPG-stabilized soil's performance in underground engineering.",,CE,Geotechnical,Ensemble learning; Hybrid machine learning; Unconfined compressive strength; One-part geopolymer; Interpretable machine learning,Case Studies in Construction Materials
Journal Article,"Y., Kadkhodaei Chen, M. H., Zhou, J.",2025,Development of the Optuna-NGBoost-SHAP model for estimating ground settlement during tunnel excavation,Elsevier B.V.,24,,60–78,"This study aims to develop and evaluate a natural gradient boosting (NGBoost) model optimized with Optuna for estimating ground settlement during tunnel excavation, incorporating Shapley additive explanations (SHAP) to perform interpretability analysis on the model's estimation results. The model's predictive performance was comprehensively assessed using datasets from two earth pressure balance shield tunneling projects in Changsha and Zhengzhou, China. Comparative analyses demonstrated the superior accuracy and generalization capability of the Optuna-NGBoost-SHAP model (training set: R2 = 0.9984, MAE = 0.1004, RMSE = 0.4193, MedAE = 0.0122; validation set: R2 = 0.9001, MAE = 1.3363, RMSE = 3.2992, MedAE = 0.3042; test set: R2 = 0.9361, MAE = 0.9961, RMSE = 2.5388, MedAE = 0.2147). SHAP value analysis quantitatively evaluated the contributions of input features to the model's estimations, identifying geometric factors (distance from the shield machine to the monitoring section and cover depth) as the most important features. The findings provide robust decision support for safety management during tunnel construction and demonstrate the reliability and efficiency of the Optuna-NGBoost-SHAP framework in estimating complex ground settlement scenarios. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011379175&doi=10.1016%2Fj.undsp.2025.03.006&partnerID=40&md5=39cf28bd679e0127063954a4d06f924c,CE,Geotechnical,Ground settlement; Hyperparameter optimization; NGBoost; Estimation; Interpretable machine learning,Underground Space
Journal Article,"Z., Quan Chen, H., Jin, R., Lin, Z., Jin, G.",2024,"Debris flow susceptibility assessment based on boosting ensemble learning techniques: a case study in the Tumen River basin, China",Springer Science and Business Media Deutschland GmbH,38,6,2359–2382,"Debris flow has always been a serious problem in mountainous areas. Accurate debris flow susceptibility (DFS) assessment and interpretable prediction results play an important role in the prevention and control of debris flow disasters. Some commonly used machine learning algorithms based on Boosting ensemble techniques were widely used in the study of geohazard susceptibility due to its excellent predictive ability. However, the Categorical Boosting (CatBoost) and Natural Gradient Boosting (NGBoost) have not yet been applied in the field of DFS assessment, and few geohazard studies systematically compare and research these boosting-based algorithms. Meanwhile, previous researches have mostly focused on comparing the predictive ability of algorithms, identifying the susceptibility zones of the entire study area, and ranking the importance of the indicators, but little thorough analysis of the relationship between the indicators and debris flow susceptibility on different types of construction land. The aims of this study were to explore the optimal boosting-based DFS model, and the distribution characteristics and change rules of DFS in the study area, so as to provide decision supports for debris flow disaster prevention and reduction. This was the first time that six boosting-based machine learning algorithms have been compared in the study of DFS assessment. After determining the optimal model, the change rules of indicators in the entire study area and two types of construction lands under different DFS levels were studied respectively. An eXplainable Artifcial Intelligence (XAI) method called SHapley Additive exPlantations (SHAP), combined with zonal statistics function in geographic information system (GIS) were adopted to explore how each indicator affects the occurrence of debris flows. The results showed that the CatBoost performed best and provided the most reasonable DFS result among six boosting-based models. We found that debris flows were more likely to occur along rivers and construction lands at low altitude. Rural areas faced more stronger pressure from rainfall and were featured by worse disaster-breeding environment than urban areas. This research enriches the application of machine learning in DFS assessment, explores the changing trends of indicators between different DFS levels, and provides suggestions for better debris flow disaster prevention and mitigation management. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186470060&doi=10.1007%2Fs00477-024-02683-6&partnerID=40&md5=8aaa484cf2740e05de2b5ab3fc37ffb5,CE,GIS / Remote Sensing,Debris flow susceptibility; Ensemble learning technique; The Tumen River basin; CatBoost; Disaster management,Stochastic Environmental Research and Risk Assessment
Journal Article,"B., Mei Cheng, L., Long, W. J., Kou, S., Li, L., Geng, S.",2023,Ai-guided proportioning and evaluating of self-compacting concrete based on rheological approach,Elsevier Ltd,399,,,"Self-Compacting Concrete (SCC) has gained significant popularity due to its exceptional workability performance. However, designing SCC poses more challenges than ordinary concrete, as it must fulfill requirements for filling ability, passing ability, and segregation resistance. Regrettably, existing test techniques lack the ability to simultaneously evaluate all these properties, and relying on experiential knowledge and cognition without tangible physical meaning. Although rheology examines the flow and deformation of fluid materials and is closely related to SCC's properties, the relationship between SCC composition, rheology, and properties remains unclear due to the complexity of the factors involved and the absence of effective tools. This study introduces a novel approach by utilizing a random forest algorithm to create multiple interpretable machine learning models for predicting the rheology, workability, and mechanical properties of SCC. Additionally, SHapley Additive exPlanation (SHAP) and Partial Dependence Plot (PDP) methods were integrated with the models to analyze how SCC composition impacts its properties by altering the rheology of the mixture. The models exhibit high accuracy in predicting both rheology and SCC properties (R2 = 0.93 ∼ 0.98, Index of Agreement = 0.92 ∼ 0.99). According to the SHAP and PDP analysis, yield stress and plastic viscosity were negatively correlated with slump flow, L-box ratio, and segregation rate, while exhibiting a positive correlation with V-funnel time and strength. Furthermore, the dependence of the segregation rate on yield stress was observed to be stronger at relatively low yield stress levels (below 40 Pa). These models provide valuable insights for designing and evaluating SCC mixtures tailored to specific requirements. Additionally, the study explores underlying mechanisms and offers guidelines for proportioning SCC in different design scenarios. The findings contribute to the advancement of SCC technology and have significant implications for the construction industry. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166324348&doi=10.1016%2Fj.conbuildmat.2023.132522&partnerID=40&md5=19a4c2911304deab8a89c2346cc881ed,CE,Materials,Self-compacting concrete; Rheological parameters; Workability; Strength; Partial Dependence Plot; SHapley Additive explanation,Construction and Building Materials
Journal Article,"Min-Yuan, Khitam Akhmad Cheng, F. K.",2024,Novel Optical-Inspired Rain Forest for the Explainable Prediction of Geopolymer Concrete Compressive Strength,American Society of Civil Engineers (ASCE),38,6,4024035,"Geopolymer concrete (GPC) is an extraordinary material for promoting sustainable development in the construction industry and reducing environmental risk. However, material properties, such as compressive strength, are commonly determined using laboratory experiments, which are costly and time-consuming to run. Therefore, optical-inspired rain forest (ORF), a sophisticated predictive model, was developed to offer an alternative mathematical solution. The developed model uses a novel mechanism that grows an operation tree into multiple operation forests and employs an optical microscope algorithm to optimize the weight and forest topology. The experimental results indicate that the proposed model outperformed several other popular artificial intelligence approaches, achieving the highest evaluation criteria of RI=0.973 and RI=0.979, respectively, for training and testing data sets. Hence, ORF is recommended as a viable tool to assist material engineers to significantly increase the utilization of GPC in construction projects.",https://doi.org/10.1061/JCCEE5.CPENG-5956,CE,Materials,Geopolymer concrete (GPC); Compressive strength; Machine learning; Operation tree; Concrete properties,Journal of Computing in Civil Engineering
Journal Article,"D., Seo Choi, S., Park, H., Hong, T., Koo, C.",2023,Forecasting personal learning performance in virtual reality-based construction safety training using biometric responses,Elsevier B.V.,156,,,"During virtual reality-based safety training, it is necessary to immediately and objectively evaluate personal learning performance. In light of this, this study proposed an interpretable machine learning approach for forecasting personal learning performance in VR-based construction safety training using real-time biometric responses. During VR-based safety training (‘fall accidents on scaffolding’), eye-tracking and EEG data were collected in real time from 30 participants (i.e., construction workers). The main findings can be summarized as follows. Compared to the full forecast model (FM), the support vector regression algorithm of the simplified forecast model (SM), which considers only principal features as independent variables, demonstrated better prediction performance (i.e., accuracy improvement: 0.087 of mean absolute error, overfitting: one-third level of the FM). This study creates new ground in the field of personalized safety training by enabling real-time monitoring and diagnosis for the cognitive states (i.e., learning performance) of construction workers during VR-based construction safety training. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173272856&doi=10.1016%2Fj.autcon.2023.105115&partnerID=40&md5=a5f694afa6f0fa89358e1c7679c1dc17,CEM,Safety,VR-based construction safety training; Learning performance; Forecast model; Biometric response; Machine learning algorithm; Construction worker,Automation in Construction
Journal Article,"F. C., Ringsquandl Collins, M., Braun, A., Hall, D. M., Borrmann, A.",2022,Shape encoding for semantic healing of design models and knowledge transfer to scan-to-BIM,ICE Publishing,175,4,160–180,"Automated parsing of design data will increasingly be a prerequisite for efficient data- and analytics-driven management of building portfolios. The high complexity and low rigidity of building information modelling (BIM) model exchange standards such as Industry Foundation Classes result in considerable differences in data quality and impede direct data availability for analytics-based decision support. Mis- or unclassified building elements are a common issue and can lead to tedious manual reworks. At the same time, scan-to-BIM processes still require considerable manual effort to identify subclass element geometry. This work leverages the benefits of a three-dimensional lightweight, geometric algorithm to generate meaningful geometric features autonomously that assist shape classification in erroneous design models and pre-segmented point clouds. Geometric deep learning is introduced in two steps; a discussion about the benefits of graph convolutional networks (GCNs) is given before a set of experiments on BIM element data sets is conducted. Utilising explainable artificial intelligence methods, the GCN performance is made suitable for human-algorithm interaction. Leveraging element geometry solely, the classification reaches a promising average performance of above 83% for the model-healing task with a reduced computation time. The encoded geometric knowledge from the design models is shown to be helpful in showcasing examples of segment classification in point clouds. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147135752&doi=10.1680%2Fjsmic.21.00032&partnerID=40&md5=37111da10ab06134e4df0affcffcd741,CEM,"Digital Construction (BIM, AI, Sensors, Robotics)","artificial intelligence; Building Information Modelling (BIM); modelling; UN SDG 9: Industry, innovation and infrastructure",Proceedings of the Institution of Civil Engineers – Smart Infrastructure and Construction
Journal Article,"P. F., Abdel-Aty Cui, M., Wang, C. Z., Yang, X. B., Song, D. D.",2025,Examining the impact of spatial inequality in socio-demographic and commute patterns on traffic crash rates: Insights from interpretable machine learning and spatial statistical models,Elsevier Ltd,167,,222–245,"Traffic crash rates are often closely related to the region's socio-demographic, commuting behavior, motivated by the risks associated with increased region density and excessive congestion. However, crash rates of different severities may vary considerably due to socio-spatial disparities and the mitigation behaviors adopted across regions. Thus, this study elucidates the intricate effects of socio-demographic dynamics and commuting behavior on overall and fatal traffic crash rates across Florida's counties, with particular emphasis on the underlying factors of spatial inequality. Employing an interpretable machine learning model, specifically eXtreme Gradient Boosting (XGBoost), we demonstrate its superiority in detecting spatial heterogeneity and the complex effects of various factors compared to traditional spatial statistical models, e.g. Spatial Lag Model (SLM) and Multiscale Geographically Weighted Regression model (MGWR). A comprehensive simulation experiment was designed to validate the dependability of modeled fittings, which confirms XGBoost as a reliable alternative to conventional spatial statistical models, particularly when dealing with datasets that including complex effects including spatial lag, spatial heterogeneity, non-linear effects and potential interaction effects. Furthermore, the totally empirical findings for Florida reveal the spatial variations in overall and fatal crash rates, correlating significantly with socio-demographic and commute pattern variables. An endogeneity test has been conducted initially for empirical datasets, accompanied by strategies to eliminate the biasing effect of endogenous variables on subsequent modeling process. Finally, key variables include population demographics, commute duration, education levels, unemployment rates, and intersection density produce heterogeneous effects on overall and fatal traffic crash rates. Notably, the study dispels the conventional belief that higher overall crash rates directly correlate with higher fatal crash rates in the same regions, underscoring the importance of distinct analysis. Policy measures should focus on improving accessibility to road infrastructure and healthcare services, with tailored approaches for sparse and densely populated areas. These findings underscore the need to address spatial inequalities in transportation infrastructure and policy measures to reduce traffic crash rates and improve road safety across all regions.",,CE,Transportation,Traffic crash rates; Socio-demographic; Commute patterns; Spatial inequality; Endogeneity; Simulated experiment; Interpretable ML,Transport Policy
Journal Article,"S., Zhang Dai, J., Huang, Z., Zeng, S.",2025,Fire Prediction and Risk Identification With Interpretable Machine Learning,John Wiley and Sons Ltd,44,5,1699–1715,"Fire safety is a primary concern in safeguarding lives and property. However, it is challenging to predict fire incidents and identify potential influencing factors due to limitations of data, model accuracy and interpretability. This paper proposes a novel scheme designed to enhance predictive and explainable capabilities by integrating multi-source data, adaptive machine learning methods, and Shapley additive explanation (SHAP) tools for more effective and applicable fire safety management. The scheme shows satisfactory prediction results by leveraging the data from grid-style management systems and our proposed machine learning method with dynamic time warping distance-based time series clustering, significantly outperforming the methods merely based on time series modeling. Moreover, clustered features help to clarify the main influencing risk factors and provide clearer insights for model interpretability. With global SHAP, community clusters capturing community fire event frequency, as well as historical records on fire police rescue, smoke alarms, and fire alarms, are found to be significant risk factors among all the features over the whole communities and periods via the model interpretability analysis, implying that communities where fires used to occur frequently are more likely to occur in future, which should be highly vigilant in real fire management. With local SHAP, specific risk factors that vary across communities can be identified for any single community with a given period. We demonstrate the potential of this integrated machine learning scheme in improving the prediction accuracy and risk identification applicability of fire incidents, which contributes to more effective and customized fire safety management. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000241155&doi=10.1002%2Ffor.3266&partnerID=40&md5=60aaa9840ae7b49b470f217a27c12587,CEM,Safety,Fire prediction; machine learning; model interpretability; risk identification; time series clustering,Journal of Forecasting
Journal Article,"T. A., Kerkez Dantzer, B.",2024,Generating interpretable rainfall-runoff models automatically from data,Elsevier Ltd,193,,,"A sudden surge of data has created new challenges in water management, spanning quality control, assimilation, and analysis. Few approaches are available to integrate growing volumes of data into interpretable results. Process-based hydrologic models have not been designed to consume large amounts of data. Alternatively, new machine learning tools can automate data analysis and forecasting, but their lack of interpretability and reliance on very large data sets limits the discovery of insights and may impact trust. To address this gap, we present a new approach, which seeks to strike a middle ground between process-, and data-based modeling. The contribution of this work is an automated and scalable methodology that discovers differential equations and latent state estimations within hydrologic systems using only rainfall and runoff measurements. We show how this enables automated tools to learn interpretable models of 6 to 18 parameters solely from measurements. We apply this approach to nearly 400 stream gaging sites across the US, showing how complex catchment dynamics can be reconstructed solely from rainfall and runoff measurements. We also show how the approach discovers surrogate models that can replicate the dynamics of a much more complex process-based model, but at a fraction of the computational complexity. We discuss how the resulting representation of watershed dynamics provides insight and computational efficiency to enable automated predictions across large sensor networks.",,CE,Water,Model discovery; Rainfall-Runoff; Dynamical systems; Explainable AI; Surrogate modeling; Data-driven,Advances in Water Resources
Journal Article,"S., Datta Das, S., Zubaidi, H. A., Obaid, I. A.",2021,Applying interpretable machine learning to classify tree and utility pole related crash injury types,Elsevier Ltd,45,3,310–316,"In spite of enormous improvements in vehicle safety, roadway design, and operations, there is still an excessive amount of traffic crashes resulting in injuries and major productivity losses. Despite the many studies on factors of crash frequency and injury severity, there is still further research to be conducted. Tree and utility pole/other pole related (TUOP) crashes present approximately 12 to 15% of all roadway departure (RwD) fatal crashes in the U.S. The count of TUOP crashes comprise nearly 22% of all fatal crashes in Louisiana. From 2010 to 2016, there were 55,857 TUOP crashes reported in Louisiana. Individually examining each of these crash reports is not a realistic option to investigate crash factors. Therefore, this study employed text mining and interpretable machine learning (IML) techniques to analyze all TUOP crashes (with available crash narratives) that occurred in Louisiana from 2010 to 2016. This study has two major goals: 1) to develop a framework for applying machine learning models to classify injury levels from unstructured textual content, and 2) to apply an IML framework that provides probability measures of keywords and their association with the injury classification. The present study employed three machine learning algorithms in the classification of injury levels based on the crash narrative data. Of the used modeling techniques, the eXtreme gradient boosting (XGBoost) model shows better performance, with accuracy ranging from 0.70 to 24% for the training data and from 0.30% to 16% for the test data. © 2021 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102086545&doi=10.1016%2Fj.iatssr.2021.01.001&partnerID=40&md5=65b7072b6b42fbd1ee3da4e4fb201dcd,CE,Transportation,TUOP crashes; Crash narratives; Text mining; Interpretable machine learning; Misclassification; Explanation,IATSS Research
Journal Article,"T., Talukdar Das, S., Shahfahad,, Naikoo, M. W., Ahmed, I. A., Rahman, A., Islam, M. K., Alam, E.",2024,Integration of fuzzy AHP and explainable AI for effective coastal risk management: A micro-scale risk analysis of tropical cyclones,Elsevier Ltd,23,,,"The east coast of India, especially the coastal region of Odisha, is highly threatened by tropical cyclones. This study develops a detailed risk map for tropical cyclones in the coastal districts of Odisha at the micro level, focusing on the assessment of risk factors at the block level. Using a multi-criteria decision making (MCDM) approach, the study considers four primary risk components: Exposure, vulnerability, susceptibility, and mitigation options. The Explainable Artificial Intelligence (XAI) framework, which uses the XGBoost model in conjunction with SHAP values, is applied to identify and elucidate the factors influencing risk levels in 69 blocks. Results indicate that about 65% of the area is at high risk to tropical cyclone, especially in the northeastern and central regions. In particular, 32 blocks are classified as high to very high-risk zones. The study shows a contrast in risk levels, with blocks in the northeast and southeast at higher risk, while blocks in the southern regions such as Ganjam and Puri and in the central parts of Kendrapara and Baleswar districts are at lower risk. The findings from this study are crucial for local authorities to identify vulnerable areas and improve cyclone preparedness and risk management strategies in Odisha.",,CE,Water,Tropical cyclone risk; Multi-criteria decision-making; Explainable AI approach; XGBoost model; Coastal areas; Block level analysis,Progress in Disaster Science
Journal Article,"Jjfg, Koks De Plaen, E. E., Ward, P. J.",2024,Towards an open pipeline for the detection of critical infrastructure from satellite imagery-a case study on electrical substations in The Netherlands,IOP Publishing Ltd,4,3,,"Critical infrastructure (CI) are at risk of failure due to the increased frequency and magnitude of climate extremes related to climate change. It is thus essential to include them in a risk management framework to identify risk hotspots, develop risk management policies and support adaptation strategies to enhance their resilience. However, the lack of information on the exposure of CI to natural hazards prevents their incorporation in large-scale risk assessments. This study sets out to improve the representation of CI for risk assessment studies by building a neural network model to detect CI assets from optical remote sensing imagery. We present a pipeline that extracts CI from OpenStreetMap, processes the imagery and assets' masks, and trains a Mask R-CNN model that allows for instance segmentation of CI at the asset level. This study provides an overview of the pipeline and tests it with the detection of electrical substations assets in the Netherlands. Several experiments are presented for different under-sampling percentages of the majority class (25%, 50% and 100%) and hyperparameters settings (batch size and learning rate). The highest scoring experiment achieved an Average Precision at an Intersection over Union of 50% of 30.93 and a tile F-score of 89.88%. This allows us to confirm the feasibility of the method and invite disaster risk researchers to use this pipeline for other infrastructure types. We conclude by exploring the different avenues to improve the pipeline by addressing the class imbalance, Transfer Learning and Explainable Artificial Intelligence.",,CE,GIS / Remote Sensing,critical infrastructure; instance segmentation; CNN; machine learning; remote sensing,Environmental Research: Infrastructure and Sustainability
Journal Article,"K. Aksu, H. Demirel",2024,Extracting Structural Elements From 3D Point Clouds in Indoor Environments via Machine Learning Techniques,Institute of Electrical and Electronics Engineers Inc.,12,,94461–94476,"The utilization of three-dimensional point clouds is an advanced approach for detecting the geometry of objects within a building environment. Nonetheless, a vast amount of data still needs to be manually processed. Intelligent automation frameworks could be deployed to overcome such issues. Hence, this study proposes a machine learning-based framework for successfully classifying structural components in indoor environments. The proposed framework consists of four stages: pre-processing, feature extraction, feature selection, and interpretability of classification results using an explainable machine learning method. According to the proposed framework, the chi-squared test stands out for optimum local neighborhood radius determination and feature selection. The CatBoost model has the highest accuracy of 82.96%, whereas the Random Forest model’s accuracy is 82.09%. However, the training time for the Random Forest is 27 times shorter than the CatBoost. Hence, both models could be preferred to other machine learning models for practical applications due to the good balance between accuracy and calculation efficiency. Additionally, the model with the highest accuracy, CatBoost, is evaluated using the Shapley Additive exPlanations to understand the impacts of features on predictions, and according to the results, Z coordinate and verticality had a relatively high impact on the model, while others had low impacts. The proposed framework uses machine learning to classify indoor point clouds, balancing processing time and accuracy for computational efficiency in practical applications. Hence, the framework could be utilized to automate the digitalization efforts of indoor environments effectively.",,CE,GIS / Remote Sensing,3D point cloud; classification; explainable machine learning; indoor environment; local neighborhood; machine learning; structural element; terrestrial laser scanning,IEEE Access
Journal Article,"G. W., Choudhary Deressa, B. S.",2025,Evaluating Productivity in Opencast Mines: A Machine Learning Analysis of Drill-Blast and Surface Miner Operations,Springer Nature,34,1,215–251,"Productivity in opencast mining, particularly in drill-blast (DB) and surface miner (SM) operations, is crucial for optimizing efficiency and reducing costs. These operations are directly affected by fragmentation, which in turn impacts equipment utilization, loading cycle times, and downstream operations. This study analyzed field data such as rock properties, machine parameters, blast design results, and post-blast fragmentation size (0.15-0.82 m), with 0.45 m identified as the optimal fragmentation size for a 12 m3 shovel bucket. Traditional productivity assessments often use simplistic models that fail to capture the complexities of mining operations. To address this, an explainable machine learning (ML) model was developed, integrating fragmentation size, rock and machine parameters, and geometric factors to evaluate DB and SM operations in opencast coal mines. Various ML techniques, such as artificial neural network (ANN), random forest regression (RFR), gradient boosting regressor (GBT), and support vector regression (SVR), were employed to analyze these parameters. Among these, the RFR model demonstrated the highest accuracy, with a coefficients of determination (R2) of 99.5% for training and 99.2% for testing in DB datasets, and 99.9% for training and 99.5% for testing in SM datasets. Furthermore, the RFR model had the lowest root mean square error, mean absolute error, and mean absolute percentage error of 10.35, 4.788, and 2.1% for DB training datasets, and 5.53, 1.75, and 1.5% for SM training datasets, respectively, underscoring its superior performance. Using SHAP (Shapley Additive exPlanations), the study identified key productivity drivers: SM cycle time, diesel consumption, and coal face length. Fragmentation size, resulting from blasting, was also found to influence shovel efficiency and overall productivity significantly. This paper highlights the effectiveness of ensemble ML models in predicting and analyzing complex productivity dynamics in opencast mining.",,CE,Geotechnical,NaN,Natural Resources Research
Journal Article,"M., Williams Di Bacco, J. H., Sugawara, D., Scorzini, A. R.",2024,Towards multi-variable tsunami damage modeling for coastal roads: Insights from the application of explainable machine learning to the 2011 Great East Japan Event,Elsevier Ltd,115,,,"The accurate assessment of tsunami-induced damage to coastal roads is crucial for effective disaster risk management. Traditional approaches, reliant on univariate fragility functions, often fail to capture the complex interplay of variables influencing road damage during tsunami events. This study addresses this limitation by employing machine learning techniques on an extensive dataset compiled after the 2011 Great East Japan tsunami. The dataset, enriched with additional explicative variables accounting for the hydraulic features of the event and the physical characteristics at roads’ location, enables a comprehensive analysis of road damage mechanisms. Results indicate that while inundation depth remains a significant predictor, factors such as wave approach angle, road orientation and potential overflow from inland watercourses also play critical roles. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205346787&doi=10.1016%2Fj.scs.2024.105856&partnerID=40&md5=daa301e2ffcd6c24322ae5929e4d8e24,CE,Transportation,RoadDamage; Tsunami; Machine learning; Multi-variable; Fragility function; Uncertainty,Sustainable Cities and Society
Journal Article,Ao Du,2024,Data-Driven Telecommunication Outage Prediction during Hurricane Events,American Society of Civil Engineers,10,3,04024046,"Telecommunication infrastructure (TI) has become an indispensable part in modern society, and its functionality is especially vital to emergency response during hurricanes. This study bridges the gap of lack of quantitative TI outage prediction models during hurricane events. County-level TI outage and power outage time-series and demographic data across eight continental US states during 10 recent hurricane events are collected. Two types of TI outage prediction models, namely, time-independent and time-dependent models, are developed. The time-independent model is intended for rapid prehurricane preparation or posthurricane outage evaluation, and is based on the partial least-squares regression technique. Relative predictor importance is also quantified via the Shapley additive explanations for better model interpretability. Moreover, to offer temporal TI outage prediction as the hurricane unfolds in real time, the time-dependent TI outage prediction model was developed, which leverages recent advances in recurrent neural networks such as the long short-term memory (LSTM) and bidirectional LSTM networks. The time-dependent model is able to handle time-series data and offers sequential TI outage prediction as new observations become available. Comprehensive model predictive performance evaluation is carried out and the explanatory power of different predictor combinations are examined. The proposed data-driven models can offer the much-needed quantitative and rapid TI outage prediction during hurricane events.",https://doi.org/10.1061/AJRUA6.RUENG-1285,CE,Environmental,Telecommunication infrastructure (TI) outages; Hurricanes; Power outages; Demographic data; Data-driven telecom outage predictive models,"ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems, Part A: Civil Engineering"
Journal Article,"S., Kurth Dulin, M., Betz, T., Robaszewski, J., Grussing, M., Linkov, I.",2025,Predicting mold severity in buildings using interpretable machine learning,Elsevier Ltd,112,,,"This study addresses the challenge of mold prevention in built environments through a machine learning approach for portfolio-wide severity prediction utilizing existing facility management datasets. While mold exposure is associated with respiratory health complications and substantial economic burden, contemporary prediction methodologies predominantly rely on micro-climate specific models requiring detailed hygrothermal data from environmental monitoring systems. Traditional approaches—such as experimentally derived growth functions or deterministic numerical models—produce surface-level mold growth indices for specific materials but provide limited insight into building-wide mold severity at portfolio scale. We demonstrate that a Light Gradient Boosting Machine (LightGBM) framework with SHapley Additive exPlanations (SHAP) techniques can effectively predict mold severity across building portfolios using U.S. Army Corp's (USACE) Sustainment Management System (SMS) data. Analysis revealed several key predictors of mold severity, such as the air changes per hour of the air handlers, the tonnage of the air condensers, how much a building was inspected, and the age of the building. Residential and storage structures also exhibited higher susceptibility to mold and moisture issues. This methodological approach offers a transition from material-specific assessments toward building-wide preventative strategies that complement established facility management protocols. The model provides interpretable insights that inform building science applications through identification of structural and operational determinants of mold development, supporting more efficient resource allocation for remediation across diverse building portfolios. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014169541&doi=10.1016%2Fj.jobe.2025.113901&partnerID=40&md5=cc60b73482ebdeef1f259e2ffdd74850,CE,Environmental,NaN,Journal of Building Engineering
Journal Article,A. Durap,2025,Interpretable machine learning for coastal wind prediction: Integrating SHAP analysis and seasonal trends,Springer Nature,29,3,,"Accurate wind speed prediction plays an important role in developing effective coastal management strategies and risk assessments, especially in coastal region managements to reduce erosion damage. In offshore wind energy, precise forecasts optimize wind farm layout and operations, maximizing energy yield and minimizing downtime. Additionally, accurate wind speed forecasts significantly improve maritime transportation safety by predicting hazardous conditions. Understanding wind patterns is also important for coastal ecosystem management and safer navigation activities. However, accurate wind speed prediction in dynamic coastal environments remains challenging due to (1) limited applications of robust machine learning (ML) models tailored to coastal meteorological complexity, (2) insufficient integration of interpretable feature analysis with predictive modeling for actionable insights, and (3) gaps in understanding how seasonal and diurnal wind patterns influence model performance in understudied regions like tropical Queensland. This study focuses on Abbot Point, Queensland, Australia, using meteorological data collected hourly from January 1 to December 31, 2023 (Latitude: -19.9496; Longitude: 148.0482). It evaluates three machine ML models-Linear Regression (LR), Decision Tree Regressor (DT), and Random Forest (RF)-to identify the most reliable approach for wind speed forecasting. The dataset includes wind direction, air temperature, relative humidity, precipitation, and barometric pressure as feature variables, with wind speed as the target variable. Novel integration of SHapley Additive exPlanations (SHAP) analysis and seasonal decomposition addresses interpretability gaps, while rigorous validation across training (70%), testing (15%), and validation (15%) datasets ensures model robustness. The RF model consistently outperformed others across training, validation, and test datasets, achieving the lowest mean square error (MSE: Train 0.183, Validation 0.875, Test 0.803), highest R2 (Train 0.966, Validation 0.831, Test 0.844), and superior Nash-Sutcliffe Efficiency (NSE: Train 0.96, Validation 0.83, Test 0.84). These results reflect the model's robust ability to capture complex relationships in the data. In contrast, LR and DT exhibited moderate accuracy, with higher MSE and lower NSE values, struggling particularly with consistency and extreme values. Complementary analyses, including wind rose plots and time series of wind speed, relative humidity, and barometric pressure, revealed high-risk periods characterized by strong winds (> 10 m/s), high humidity (> 90%), and low barometric pressure (< 1000 hPa). Seasonal analysis revealed spring/summer peaks in hazardous winds (> 10 m/s), with diurnal cycles (24-h periodicity) significantly influencing prediction accuracy-a pattern underemphasized in prior coastal ML studies. This study bridges critical gaps by demonstrating how interpretable ML enhances coastal wind prediction through: a) quantitative validation of RFR's superiority over traditional models in handling coastal meteorological variability, b) SHAP-driven identification of dominant predictors (wind direction, pressure) for targeted monitoring, c) Seasonal-temporal analysis framework for site-specific risk mitigation strategies. These findings confirm the interactions between meteorological variables that intensify storm risks and coastal hazards. Key insights include the dominant influence of southeast and south-southwest winds (100 degrees-200 degrees) and the critical role of barometric pressure in driving extreme wind events. Also, findings enable improved storm surge modeling and early warning systems by providing 6-h wind forecasts with 84% accuracy, directly informing coastal defense alignment with dominant wind-driven erosion patterns. This approach addresses the critical need for ML applications that combine predictive power with operational interpretability in coastal management contexts. The integration of ML models with detailed meteorological patterns supports the identification of high-risk periods, enabling targeted interventions such as strengthening coastal defenses and issuing early warnings. This study underscores the value of ML techniques, particularly RF, in enhancing predictive frameworks for coastal risk management and promoting sustainable, resilient coastal environments.",,CE,Water,Interpretable machine learning techniques; Coastal risk management; Forecasting; Performance metrics; Resilient coastal planning; Wind speed,Journal of Coastal Conservation
Journal Article,"E. D., Aydogan Durmaz, S., Gölcük, İ",2025,An explainable risk classification model by integrating fuzzy multiple criteria sorting and fuzzy linguistic summarization,Elsevier B.V.,181,,,"This study proposes a novel explainable risk assessment framework that integrates the fuzzy Full Consistency Method (FUCOM), fuzzy VlseKriterijumska Optimizacija I Kompromisno Resenje (VIKOR)-Sort, and fuzzy linguistic summarization to address the challenges of uncertainty, prioritization, and interpretability in risk evaluation. The model enables multi-criteria analysis under linguistic uncertainty, classifies risks into predefined categories, and generates human-readable linguistic summaries to support decision-makers. The proposed methodology is applied to a real-world construction case involving 32 risk items, which are categorized into high, medium, and low risk groups. Comparative analysis with six established multiple criteria sorting algorithms reveals strong alignment in classification outcomes. The novelty of the proposed model lies in enhancing the explainability of multiple criteria sorting algorithms by integrating fuzzy linguistic summarization into the classification process—an aspect largely overlooked in the existing literature. The results demonstrate that the proposed model not only produces consistent risk classifications across alternative methods but also enhances decision-makers’ understanding through interpretable linguistic outputs. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009648152&doi=10.1016%2Fj.asoc.2025.113515&partnerID=40&md5=cfea7ff5a0fbe8bc3d82e0e2d5f96966,CEM,Risk and Uncertainty,Explainable artificial intelligence; Fuzzy linguistic summarization; Multiple criteria decision making; Risk assessment,Applied Soft Computing
Journal Article,"M., Mahmoudian Ebrahimzadeh, A., Tajik, N., Mohammadzadeh Taleshi, M. M., Ashtari, M., Shakiba, M., Bazli, M.",2025,Interpretable machine learning models for predicting flexural bond strength between FRP/steel bars and concrete,Elsevier Ltd,74,,,"One of the main challenges in reinforced concrete with rebars is the bond strength between the rebars and the concrete in flexural elements. Various studies have been conducted on the effect of different parameters on bond strength. However, the simultaneous effect of these factors under specific conditions remains ambiguous, requiring further tests which are time-consuming and costly. Bringing together several reported data up to now and obtaining a comprehensive output would be valuable. A comprehensive dataset was constructed from 268 data points reported in 19 studies, encapsulating various parameters such as type of reinforcement, concrete strength, embedment length, and environmental conditions. Four machine learning (ML) models, including Decision Tree (DT), Random Forest (RF), Gradient Boosting (GB), and Extreme Gradient Boosting (XGB), were applied and optimized through grid search. The XGB model demonstrated superior predictive accuracy, achieving an R² score of 0.9 post-tuning, underscoring the effectiveness of hyperparameter optimization. Shapley values were utilized to interpret model predictions, highlighting embedment length, concrete compressive strength, and bar size as the most influential features. ML models offer a cost-effective and time-efficient alternative to extensive experimental testing and pave the way for integrating advanced computational techniques in civil engineering to enhance the design and durability of reinforced concrete structures. Eventually, an intuitive and user-friendly interface (UI) model, which eliminates the need for terminal commands and coding, has been developed, accessible on GitHub, and recommended for design practices. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219506798&doi=10.1016%2Fj.istruc.2025.108587&partnerID=40&md5=bf719de3c47b8bf16d5a6b9fcad62842,CE,Structural,Machine learning; Ensemble learning; XGBoost; Random Forest; Bond behavior; Anchorage system; DecisionTree,Structures
Journal Article,"K., Mohamed Ehsan, A. H., Bin Inqiad, W., Javed, M. A., Iqbal, I.",2025,Multi expression programming and interpretable machine learning for determining compressive strength of coral sand aggregate concrete,Elsevier Ltd,45,,,"The extensive utilization of natural aggregates in the construction industry can be reduced by replacing them with natural and locally available materials like coral sand aggregate. However, accurately determining the compressive strength (CS) of coral sand aggregate concrete (CSAC) is challenging due to its complex and non-linear behavior. Consequently, traditional techniques are ineffective. Also, there are very few reliable prediction models available in the literature for determining CS of CSAC. Thus, this study aimed to use machine learning (ML) algorithms like Multi Expression Programming (MEP), AdaBoost, and Bagging Regressor (BR) using dataset already available in the literature for predicting CS of CSAC. The dataset collected featured crucial input parameters like immersion period, confining pressure, size of coral aggregate etc. and a single output i.e., CS. The predictive models were validated by using residual assessment, k-fold cross validation, and external validation checks etc. which revealed that BR exhibited the highest accuracy having testing R2 value of 0.996. However, MEP provided an empirical equation as an output while BR failed to do so. In addition, explanatory techniques like shapely additive analysis (SHAP) and individual conditional expectation (ICE) analysis were used on the BR model to investigate the significance of input features as well as their relationship with the predicted outcome. Finally, a graphical user interface has been developed to help effectively implement the predictive models developed in this study in the industry. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001578907&doi=10.1016%2Fj.mtcomm.2025.112370&partnerID=40&md5=2b49a3d3827600acbb1058e254da2dd9,CE,Materials,Coral sand aggregate; Compressive strength prediction; Explainable machine learning; Marine Environment; Multi Expression Programming,Materials Today Communications
Journal Article,"S., Elashry Elhishi, A. M., El-Metwally, S.",2023,Unboxing machine learning models for concrete strength prediction using XAI,Springer Nature,13,1,,"Concrete is a cost-effective construction material widely used in various building infrastructure projects. High-performance concrete, characterized by strength and durability, is crucial for structures that must withstand heavy loads and extreme weather conditions. Accurate prediction of concrete strength under different mixtures and loading conditions is essential for optimizing performance, reducing costs, and enhancing safety. Recent advancements in machine learning offer solutions to challenges in structural engineering, including concrete strength prediction. This paper evaluated the performance of eight popular machine learning models, encompassing regression methods such as Linear, Ridge, and LASSO, as well as tree-based models like Decision Trees, Random Forests, XGBoost, SVM, and ANN. The assessment was conducted using a standard dataset comprising 1030 concrete samples. Our experimental results demonstrated that ensemble learning techniques, notably XGBoost, outperformed other algorithms with an R-Square (R2) of 0.91 and a Root Mean Squared Error (RMSE) of 4.37. Additionally, we employed the SHAP (SHapley Additive exPlanations) technique to analyze the XGBoost model, providing civil engineers with insights to make informed decisions regarding concrete mix design and construction practices.",,CE,Materials,NaN,Scientific Reports
Journal Article,"H., Elshaboury Elmousalami, N., Ibrahim, A. H., Ibrahim, A. H.",2025,Bayesian optimized ensemble learning system for predicting conceptual cost and construction duration of irrigation improvement systems,Elsevier Inc.,29,3,,"Linear construction projects, such as pipeline irrigation projects, are prone to delays and cost overruns owing to inaccurate cost and duration estimates. The research gap pertains to studies that concentrated exclusively on predicting costs or durations using backbox artificial intelligence models. Consequently, this study introduces an innovative approach that utilizes explainable artificial intelligence to forecast the conceptual cost and duration of irrigation projects simultaneously. This study analyzed data from 1,277 historical cases using factor analysis and stepwise regression to distill 25 parameters down to five key drivers. It evaluates 12 machine learning models, including multiple linear regression, artificial neural networks, and decision tree-based ensemble methods. Bayesian optimization was employed to fine-tune the performance of each algorithm. The light gradient boosting machine is identified as the most effective algorithm for cost prediction, with a Mean Absolute Percentage Error (MAPE) of 2.989 % and an Adjusted Determination Coefficient (R⁎2) of 0.931. For duration prediction, the extremely randomized tree model stands out, achieving a MAPE of 2.533 % and an R⁎2 of 0.961. The study further employs the Shapley additive explanation technique to improve the interpretability of the key drivers used for predicting both the budget and the timeline. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210231254&doi=10.1016%2Fj.kscej.2024.100014&partnerID=40&md5=2d9e884790bc39753e384ffde4de4ace,CEM,Cost Estimation / Bidding,Conceptual project cost and duration; Explainable artificial intelligence (XAI); Bayesian optimization; Irrigation improvement projects; Sustainability; Project management automation; Ensemble machine learning; Computational complexity,KSCE Journal of Civil Engineering
Journal Article,"M. Z., Gernay Esteghamati, T., Banerji, S.",2023,Evaluating fire resistance of timber columns using explainable machine learning models,Elsevier Ltd,296,,,"The global attention to using timber products as sustainable construction material urges careful assessment of their performance against different hazards, particularly fire. However, the current methods prescribed by design codes for evaluating the fire resistance of timber components tend to underpredict the outcomes of standard fire resistance tests, and lack interpretability due to the use of semi-empirical equations. This study develops explainable data-driven models to predict fire resistance of timber columns using geometry- and material-related properties based on a comprehensive experimental database. Nine different single and ensemble-based machine learning algorithms were trained, and their performance was optimized through rigorous hyperparameter tuning and feature selection. The best models were then interpreted using partial dependence and Shapley plots to infer the underlying relationship between fire resistance and column properties. Lastly, the models' predictive capabilities were compared to available prescriptive equations. The results show that a random forest-based model provides the best performance with an average ratio of predicted to observed fire resistance of 1.03 on the test set. The random forest prediction is mainly governed by column capacity at ambient temperature, and to a lesser degree, columns' cross-section dimension. In addition, the partial dependence plots indicate that the effect of density, modulus of elasticity, length, and compressive strength on fire resistance was not notable. Finally, while the considered prescriptive equations consistently underpredict fire resistance, the random forest model provides a consistently accurate and balanced prediction.",,CE,Structural,Fire resistance; Timber columns; Explainable machine learning; Ensemble learning,Engineering Structures
Journal Article,H. Fathipour-Azar,2023,Mean Cutting Force Prediction of Conical Picks Using Ensemble Learning Paradigm,Springer,56,1,221–236,"The conical pick is the most essential tool of excavation machinery such as roadheaders, continuous miners, and shearers for breaking rock in mining and civil engineering operations. For rock cuttability, however, the geometry of conical picks and mechanical parameters of rocks are the most important factors. This study aims to construct an optimized data-driven predictive model to establish a quantitative correlation between strength of rock, geometry of tool, and cutting action data with the mean cutting force (CF). For this purpose, 157 datasets of 47 different materials including rocks, ores, coals, and artificial rocks with uniaxial compressive strength (σ<inf>c</inf>), tensile strength (σ<inf>t</inf>), cone angle (θ), attack angle (γ), cutting depth (d), and mean CF (MCF) are accumulated from the literature. Then, extreme gradient boosting (XGBoost) model is constructed by fine-tuning hyperparameters using grid search, random search, genetic algorithm (GA), particle swarm optimization (PSO), and differential evolution (DE). Based on performance indices that are calculated for each model, i.e., coefficient of determination (R2), root mean square error (RMSE), and mean absolute percentage error (MAE) for the best performed model, i.e., DE-XGboost are R2= 0.812 , RMSE = 2256.90 N, and MAE = 1313.66 N for training stage and R2= 0.875 , RMSE = 2104.86 N, and MAE = 1140.42 N for testing stage, respectively. The findings also suggest that using a metaheuristic algorithm to fine-tune the hyperparameters of the XGBoost model can increase prediction accuracy. In the last step, three model interpretation methods viz., the permutation-based variable importance, H-statistic-based variable interaction, and accumulated local effects are applied to sensitivity analysis of the input parameters to predict MCF, providing key insights to model and researchers. The ALE plot demonstrated a complex non-linear relationship between predictors and the response variable. It is revealed that parameters d and θ have the highest and lowest impact on the MCF, respectively. Finally, the successful implementation of this approach provides a solid platform for future studies and can be an alternative to complicated conventional and theoretical methods. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139519290&doi=10.1007%2Fs00603-022-03095-0&partnerID=40&md5=493db5c402be3c73d6e385107607acb8,CE,Geotechnical,Mean cutting force; Conical picks; Interpretable machine learning technique; Metaheuristic optimization; Sensitivity analysis,Rock Mechanics and Rock Engineering
Journal Article,"D., Wu Feng, G.",2022,Interpretable machine learning-based modeling approach for fundamental properties of concrete structures,Elsevier Ltd,43,4,228–238,"The fundamental properties of concrete structures, i. e., failure mode, capacity, and ductility, are important parameters in structural design and assessment procedures. Accurately predicting these parameters directly determines the reliability of structural design and analysis. In this paper, an machine learning (ML)-based prediction method for the fundamental properties of concrete structures was proposed, which can directly give the properties for any given design parameters (such as geometric dimensions, material properties, load conditions, etc.). In addition, the recently developed Shapley additive explanations (SHAP) method was also used to explain the outcome of the ML model, overcoming the 'black box' issue. Three typical examples were performed to validate the effectiveness and practicability of the machine learning methods. The results show that compared with the traditional empirical model, the machine learning method has higher prediction accuracy and computational efficiency, and can give the influence of different input parameters on the output results of the model, which realizes the interpretability of the prediction results, and is an effective supplement to the 'mechanism-driven' modeling method. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123957184&doi=10.14006%2Fj.jzjgxb.2020.0491&partnerID=40&md5=034fd97149460f43b8b8b43b375dc980,CE,Materials,Workability; Compressive strength; Splitting tensile strength; Betel nut husk fiber; Machine learning; SHAP analysis,Structures
Journal Article,"S., Liu Feng, G. W., Shan, T. L., Li, K. J., Lai, S.",2024,Predicting green technology innovation in the construction field from a technology convergence perspective: A two-stage predictive approach based on interpretable machine learning,Elsevier Ltd.,372,,,"The construction industry, as a major global energy consumer and carbon emitter, plays a crucial role in achieving global sustainability. A key strategy for the green transformation of this industry-without compromising development-involves fostering green technology innovation. Nevertheless, existing studies exhibit a notable gap in identifying and evaluating potential green technology innovation opportunities within the construction field, leading to a scarcity of decision-making information for governments and innovation entities during the research and development stage. Recognizing this, our study proposes a two-stage technology opportunity prediction approach based on interpretable machine learning from the perspective of technology convergence. Diverging from previous methods, it not only predicts the probability of technology opportunity occurrence but also forecasts the technical impact of convergence opportunities. By analysing 600,442 patent documents in the green and construction fields, we identify 305 high-potential technology convergence opportunities. Our results reveal that technologies such as carbon capture and storage, pollution alarms, solar energy, forestry techniques, wind energy, energy-saving methods, and waste materials for water treatment have significant potential for convergence with construction technologies. Additionally, we analyse the influencing factors behind these convergence innovations, finding that technical similarity and proximity play crucial roles. These findings provide robust decision support for governments and industry stakeholders in formulating scientifically grounded green technology innovation strategies, thereby accelerating the green transformation of the construction industry and contributing to the goal of sustainable development.",,CEM,Sustainability (Construction),Technology convergence; Technology prediction; Interpretable machine learning; Green technology innovation; Construction industry,Journal of Environmental Management
Journal Article,"A., Sassi Flor, F., La Morgia, M., Cernera, F., Amadini, F., Mei, A., Danzi, A.",2023,Artificial intelligence for tunnel boring machine penetration rate prediction,Elsevier Ltd,140,,,"Penetration rate prediction of Tunnel Boring Machines (TBM) is critical for understanding excavation performances. In this paper, we investigate the possibility of developing machine learning models that accurately predict the Penetration Rate of a TBM using only machine parameters. We leveraged two datasets collected from the Exploratory and Main excavation of the Lot Mules 2–3 of the Brenner Base Tunnel Project. We compared the performance of two different Artificial Neural Network architectures, one based on feedforward architecture and the other on long short-term memory (LSTM). We also studied which features lead to a good estimation of the penetration rate using SHAP, an explainable AI tool, discovering that the Specific Energy (SE) and the Cutterhead Power (CP) are the most impactful features. We also explore the possibility of performing cross-tunnel prediction by training the model on the Exploratory tunnel and testing it on the Main Tunnel, obtaining promising results. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164237725&doi=10.1016%2Fj.tust.2023.105249&partnerID=40&md5=160c8f6a873a52d9d264cc344fe4bbc8,CE,Geotechnical,TBM performance prediction; Penetration rate; LSTM; BBT,Tunnelling and Underground Space Technology
Journal Article,"J. N., Jones Fuhg, R. E., Bouklas, N.",2024,Extreme sparsification of physics-augmented neural networks for interpretable model discovery in mechanics,Elsevier B.V.,426,,,"Data-driven constitutive modeling with neural networks has received increased interest in recent years due to its ability to easily incorporate physical and mechanistic constraints and to overcome the challenging and time-consuming task of formulating phenomenological constitutive laws that can accurately capture the observed material response. However, even though neural network-based constitutive laws have been shown to generalize proficiently, the generated representations are not easily interpretable due to their high number of trainable parameters. Sparse regression approaches exist that allow for obtaining interpretable expressions, but the user is tasked with creating a library of model forms which by construction limits their expressiveness to the functional forms provided in the libraries. In this work, we propose to train regularized physics-augmented neural network-based constitutive models utilizing a smoothed version of L0-regularization. This aims to maintain the trustworthiness inherited by the physical constraints, but also enables interpretability which has not been possible thus far on any type of machine learning-based constitutive model where model forms were not assumed a priori but were actually discovered. During the training process, the network simultaneously fits the training data and penalizes the number of active parameters, while also ensuring constitutive constraints such as thermodynamic consistency. We show that the method can reliably obtain interpretable and trustworthy constitutive models for compressible and incompressible hyperelasticity, yield functions, and hardening models for elastoplasticity, using synthetic and experimental data. This work aims to set a new paradigm for interpretable machine learning models in the broad area of solid mechanics where low and limited data is available along with prior knowledge of physical constraints that the learned maps need to obey. This paradigm can potentially be extended to a broader spectrum of scientific exploration. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190408016&doi=10.1016%2Fj.cma.2024.116973&partnerID=40&md5=6d449e9ffa9d69c9b7f78140e519bbf4,CE,Materials,Physics-augmented machine learning; Solid mechanics; Data-driven constitutive models,Computer Methods in Applied Mechanics and Engineering
Journal Article,"W. Z., Liao Gao, Y. X., Chen, Y. H., Lai, C. G., He, S. J., Wang, Z. L.",2024,Enhancing transparency in data-driven urban pluvial flood prediction using an explainable CNN model,Elsevier B.V.,645,,,"Mitigating severe losses caused by pluvial floods in urban areas with dense population and property requires effective flood prediction for emergency measures. Physics-based models face issues with low computational efficiency for real-time flood prediction. An alternative approach for rapid prediction instead of physics-based models is to predict from a data-driven perspective. However, data-driven approaches for urban flood prediction are often perceived as black box"" and might raise concerns. In this study, we propose an explainable deep learning (DL) approach for rapid urban pluvial flood prediction with enhanced transparency using a convolutional neural network (CNN) and the explainable artificial intelligence (AI) framework Shapley additive explanation (SHAP). We process a systematic stepwise feature selection process and establish a CNN model considering topography, drainage networks and rainfall to predict maximum inundation depths. Then, SHAP is applied to provide trustworthy explanations for the decision making in model results. The results show that: 1) Forward selection can offer insights into selecting effective input variables for improved predictions and promote understanding of DL modelling. The spatial pattern of inundation depths predicted by the proposed CNN model shows good agreement with those predicted by the physics-based model, demonstrated by average correlation coefficient (CC) and mean absolute error (MAE) values of 0.982 and 0.021 m, respectively. 2) The CNN model substantially outperforms the physics-based model in computational speed when using the same hardware, achieving speedups of 210 times on GPU and 51 times on CPU in the case study (575167 grid cells, 14.38 km2). Particularly, it can still achieve good performance on a CPU-only standard laptop without high-performance hardware, with only a modest increase in computational time. 3) The SHAP explainable analysis confirms that the CNN model correctly captures the relationships between input variables and water depth, revealing a reasonable decision-making process, enhancing its transparency. The explainable DL approach incorporating SHAP for rapid urban pluvial flood prediction is promising to build trust among stakeholders and provide a trustworthy reference for prompt measures aiming at saving lives and protecting assets during flood emergencies. Additionally, the proposed DL approach can potentially be further expanded to analyze the causes of urban flooding events and serve as a foundation for exploring the transferability of data-driven urban flood prediction, providing benefits for better urban flood risk management.""",,CE,Water,Urban pluvial flood; Fast water depth prediction; Explainable AI; Stepwise feature selection; Convolutional neural network,Journal of Hydrology
Journal Article,"S., de Arriba-Pérez García-Méndez, F., Leal, F., Veloso, B., Malheiro, B., Burguillo-Rial, J. C.",2025,An explainable machine learning framework for railway predictive maintenance using data streams from the metro operator of Portugal,Springer Nature,15,1,,"The public transportation sector generates large volumes of sensor data that, if analyzed adequately, can help anticipate failures and initiate maintenance actions, thereby enhancing quality and productivity. This work contributes to a real-time data-driven predictive maintenance solution for Intelligent Transportation Systems. The proposed method implements a processing pipeline comprised of sample pre-processing, incremental classification with Machine Learning models, and outcome explanation. This novel online processing pipeline has two main highlights: (i) a dedicated sample pre-processing module, which builds statistical and frequency-related features on the fly, and (ii) an explainability module. This work is the first to perform online fault prediction with natural language and visual explainability. The experiments were performed with the Metropt data set from the metro operator of Porto, Portugal. The results are above 98 % for f-measure and 99 % for accuracy. In the context of railway predictive maintenance, achieving these high values is crucial due to the practical and operational implications of accurate failure prediction. In the specific case of a high f-measure, this ensures that the system maintains an optimal balance between detecting the highest possible number of real faults and minimizing false alarms, which is crucial for maximizing service availability. Furthermore, the accuracy obtained enables reliability, directly impacting cost reduction and increased safety. The analysis demonstrates that the pipeline maintains high performance even in the presence of class imbalance and noise, and its explanations effectively reflect the decision-making process. These findings validate the methodological soundness of the approach and confirm its practical applicability for supporting proactive maintenance decisions in real-world railway operations. Therefore, by identifying the early signs of failure, this pipeline enables decision-makers to understand the underlying problems and act accordingly swiftly.",,CE,Transportation,Explainable sensor-driven computational intelligence; Intelligent transportation systems; Online supervised machine learning; Predictive maintenance; Railway sector safety and reliability,Scientific Reports
Journal Article,", Gupta Gautam, K. K., Bhowmik, D.",2024,Exploring sustainable solutions for soil stabilization through explainable Gaussian process-assisted multi-objective optimization,Elsevier Ltd,40,,,"The adoption of sustainable solutions in soil stabilization has piqued the interest of the scientific community due to the potential reduction in carbon footprint. In this regard, the research community has started looking for the alternate sustainable solutions to limit the quantity of conventionally used ecologically unfriendly soil stabilizers like lime and cement by utilizing the agricultural and industrial by-products. The production of conventional soil stabilizers (lime and cement) is extremely energy-intensive and contributes tons of greenhouse gases to the atmosphere. In general, evaluating suitability of these additives requires in-lab investigation of soil samples with varying additive concentrations and curing periods, making this approach both resource and time-intensive. Hence, this article proposes a computational framework for accelerated characterization of soil-stabilization by using the coupled experimental-Gaussian process (GP) based machine learning (ML) model. The dataset utilized for constructing the GP models consists of input features such as the stabilizer content (lime and rice husk ash (RHA)), coir fiber content, and curing period (measured in days). The target responses are the strength measures of the stabilized soil, such as unconfined compressive strength (UCS), split tensile strength (STS), and California bearing ratio (CBR). The proposed computational framework is deployed to perform multi-objective genetic algorithm (MOGA)-based optimization to achieve maximum engineering performance from stabilized soil. The presented study demonstrated that with the optimal dosage of rice husk ash (agricultural by-product), and cashew nut shell liquid (CNSL) treated coir fiber, the requirement for lime dosage may be significantly reduced while maintaining the engineering performance of the soil. The presented computational framework can be extended to any construction practices for ensuring the strategic selection of the control variables for optimizing the desired performance.",,CE,Geotechnical,Soil stabilization; Sustainability; Explainable machine learning; Multi-objective optimization; Gaussian processes,Materials Today Communications
Journal Article,"Q., Sun Ge, H. Y., Liu, Z. Q., Wang, X.",2023,A data-driven intelligent model for landslide displacement prediction,John Wiley and Sons Inc,58,6,2211–2230,"Landslides with step-like deformation features are widely distributed in the Three Gorges Reservoir area (TGR) of China, posing a severe hazard to the inhabitants of this region. This paper proposes a multi-input and multi-output intelligent integrated displacement prediction model for landslides with step-like displacement patterns. In this new model, three interconnected and information-transmitted functional sub-models are integrated. Unsupervised learning is used to identify different landslide deformation states automatically, and the imbalance classification and explainable artificial intelligence techniques are introduced for qualitative prediction and information filtering. Probability theory and deep machine learning are adopted to provide deterministically predicted values and quantify their uncertainty. The case study of the Baijiabao landslide in the TGR region proves that the proposed model performs satisfactorily in both point and interval predictions. The intelligent integrated model can also provide the forecast of landslide deformation states, visual input information filtering and back analysis of influencing factors, which are valuable to landslide early warning and risk management.",,CE,Geotechnical,imbalanced classification feature importance; interval prediction; landslide displacement; unsupervised learning,Geological Journal
Journal Article,"A., Naser Ghasemi, M. Z.",2023,Tailoring 3D printed concrete through explainable artificial intelligence,Elsevier Ltd,56,,,"Advances on the construction front continue to rise as the next industrial revolution (Construction 4.0) nears. One promising front revolves around additively fabricated or simply 3D printed concrete. The growing number of ongoing parallel research programs has now made it possible to collect a large amount of data on such concrete as, up to this point, the open literature lacks a comprehensive database. Thus, this paper presents the largest database spanning over 300 experiments on 3D printed concrete. This database is then examined via multilinear regression as well as two explainable artificial intelligence (XAI) algorithms, namely, Random Forest and XGBoost, to arrive at a working model capable of predicting the compressive strength property for 3D concrete mixtures that incorporate the following seven features: age of specimens, as well as the magnitude of cement, water, fly ash, silica fume, fine aggregate, and superplasticizer. Findings from this work infer the superiority of XAI models in predicting the strength property of 3D printed concrete. Our analysis identifies two features, namely, the age of specimens and the quantity of fine aggregate, as the most important features that can accurately predict the compressive strength property. Finally, the deployed explainability methods successfully quantified the highly nonlinear relations between the selected features and compressive strength, and this newly acquired knowledge can help tailor functional concrete mixtures. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165064423&doi=10.1016%2Fj.istruc.2023.07.040&partnerID=40&md5=57fca7f2d63a71ffda8650af7e98efd7,CE,Materials,3D concrete; Compressive strength; Machine learning; Database,Structures
Journal Article,"E., Yagiz Ghorbani, S.",2024,Estimating the penetration rate of tunnel boring machines via gradient boosting algorithms,Elsevier Ltd,136,,,"The prediction of tunnel boring machine (TBM) performance from the rate of penetration (ROP) point of view has yet to draw a lot of attention since it is one of the main challenges for excavation with TBMs. This study examined six tunnels excavated with TBM to develop predictive models of the ROP estimation using five algorithms: Gradient Boosting (GB), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine, Adaptive Boosting (AdaBoost), and CatBoost (categorical-features-include-GB). A dataset has been developed, including uniaxial compressive strength (UCS), Rock Type, the distance between planes of weakness (DPW), and thrust force (TF), and contains more than 575 data points for each parameter. The developed models showed that the XGBoost model outperformed the other models, followed by the CatBoost, according to seven different evaluation metrics used to rank the models when the models were modeled with the default values of the algorithm parameters. After tuning the hyperparameters, the GB model outperformed the others, while the other models remained relatively unchanged. By using the overall ranking according to the metrics and considering the parameter tuning time, XGBoost and CatBoost were presented as the two best models. SHAP (Shapley additive explanations: an explainable artificial intelligence tool) values and dependency plots showed that the TF has the highest impact on the ROP, followed by UCS, Rock Type, and DPW. It is concluded that the XGBoost and CatBoost models, with coefficients of determination of 0.9878 and 0.9682, respectively, may be used for estimating the TBM penetration rate for similar rock types. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198741422&doi=10.1016%2Fj.engappai.2024.108985&partnerID=40&md5=bfffe779fa1ae7b383913ede864636c5,CE,Geotechnical,Tunnel boring machine; Rate of penetration; Rock properties; Machine learning; Gradient boosting; Artificial intelligence,Engineering Applications of Artificial Intelligence
Journal Article,"Ahmed, Ezzeldin Gondia, Mohamed, El-Dakhakhni, Wael",2022,Machine Learning–Based Decision Support Framework for Construction Injury Severity Prediction and Risk Mitigation,American Society of Civil Engineers (ASCE),8,3,04022024,"Construction is a key pillar in the global economy, but it is also an industry that has one of the highest fatality rates. The goal of the current study is to employ machine learning in order to develop a framework based on which better-informed and interpretable injury-risk mitigation decisions can be made for construction sites. Central to the framework, generalizable glass-box and black-box models are developed and validated to predict injury severity levels based on the interdependent effects of identified key injury factors. To demonstrate the framework utility, a data set pertaining to construction site injury cases is utilized. By employing the developed models, safety managers can evaluate different construction site safety risk levels, and the potential high-risk zones can be flagged for devising targeted (i.e., site-specific) proactive risk mitigation strategies. Managers can also use the framework to explore complex relationships between interdependent factors and corresponding cause-and-effect of injury severity, which can further enhance their understanding of the underlying mechanisms that shape construction safety risks. Overall, the current study offers transparent, interpretable and generalizable decision-making insights for safety managers and workplace risk practitioners to better identify, understand, predict, and control the factors influencing construction site injuries and ultimately improve the safety level of their working environments by mitigating the risks of associated project disruptions.",https://doi.org/10.1061/AJRUA6.0001239,CEM,Safety,Construction fatalities; Construction safety; Decision trees; Injury factors; Injury severity; Machine learning models; Machine learning interpretability; Mitigation strategies; Parameter optimization; Random forests,"ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems, Part A: Civil Engineering"
Journal Article,"S., Hu Grigoryan, Y. K., Ullah, N.",2025,Capturing Built Environment and Automated External Defibrillator Resource Interplay in Tianjin Downtown,Multidisciplinary Digital Publishing Institute (MDPI),14,7,,"Automated external defibrillator resources (AEDRs) are the crux of out-of-hospital cardiac arrest (OHCA) responses, enhancing safe and sustainable urban environments. However, existing studies failed to consider the nexus between built environment (BE) features and AEDRs. Can explainable machine-learning (ML) methods reveal the BE-AEDR nexus? This study applied an Optuna-based extreme gradient boosting (OP_XGBoost) decision tree model with SHapely Additive exPlanations (SHAP) and partial dependence plots (PDPs) aiming to scrutinize the spatial effects, relative importance, and non-linear impact of BE features on AEDR intensity across grid and block urban patterns in Tianjin Downtown, China. The results indicated, that (1) marginally, the AEDR intensity was most influenced by the service coverage (SC) at grid scale and nearby public service facility density (NPSF_D) at block scale, while synergistically, it was shaped by comprehensive accessibility and land-use interactions with the prioritized block pattern; (2) block-level granularity and (3) non-linear interdependencies between BE features and AEDR intensity existed as game-changers. The findings suggested an effective and generalizable approach to capture the complex interplay of the BE-AEDR and boost the AED deployment by setting health at the heart of the urban development framework.",,CE,GIS / Remote Sensing,built environment; automated external defibrillator; machine learning; pattern dependency; non-linear interplay,ISPRS International Journal of Geo-Information
Journal Article,"Y. Y., Dou Gu, M. X.",2024,Nonlinear and Threshold Effects on Station-Level Ridership: Insights from Disproportionate Weekday-to-Weekend Impacts,Multidisciplinary Digital Publishing Institute (MDPI),13,10,,"Station-level ridership is an important indicator for understanding the relationship between land use and rail transit, which is crucial for building more sustainable urban mobility systems. However, the nonlinear effects of the built environment on metro ridership, particularly concerning temporal heterogeneity, have not been adequately explained. To address this gap, this study proposes a versatile methodology that employs the eXtreme gradient boosting (XGBoost) tree to analyze the effects of factors on station-level ridership variations and compares these results with those of a multiple regression model. In contrast to conventional feature interpretation methods, this study utilized Shapley additive explanations (SHAP) to detail the nonlinear effects of each factor on station-level ridership across temporal dimensions (weekdays and weekends). Using Shanghai as a case study, the findings confirmed the presence of complex nonlinear and threshold effects of land-use, transportation, and station-type factors on station-level ridership in the association. The factor Commercial POI"" represents the most significant influence on ridership changes in both the weekday and weekend models; ""Public Facility Station"" plays a role in increasing passenger flow in the weekend model, but it shows the opposite effect on the change in ridership in the weekday model. This study highlights the importance of explainable machine learning methods for comprehending the nonlinear influences of various factors on station-level ridership.""",,CE,Transportation,metro ridership; XGBoost; nonlinear effects; temporal heterogeneity,ISPRS International Journal of Geo-Information
Journal Article,"S., Chen Guan, Y., Wang, T., Hu, H.",2025,Mitigating urban heat island through urban-rural transition zone landscape configuration: Evaluation based on an interpretable ensemble machine learning framework,Elsevier Ltd,123,,,"Research methods for mitigating urban heat islands (UHIs) have been widely documented. Nevertheless, the importance of mitigating UHIs through landscape allocation in urban-rural transition zones (URTZs) has rarely been emphasized in the context of intra-urban land scarcity and urban expansion in China. This study aimed to quantify the binary relationship between URTZ's landscape configuration and urban heat island intensity (UHII) by using an interpretable ensemble learning framework in Harbin, a megacity in China. After URTZ's identification, this study integrated Boruta algorithm, SHAP, ALE (interpretable machine learning techniques) and 7 tree-based machine learning models to assess the importance of URTZ's landscape configuration with both global and local angles. The results indicated that: construction land contributed most, with construction land ratio (23.20 %), separation degree (15.95 %), and maximum patch index (15.03 %) ranking highest. This was followed by agricultural land landscape shape index (10.31 %) and landscape diversity (9 %). Maintaining construction land ratio at 50–70 % can keep UHII unchanged; UHII at the grid landscape level can be alleviated when separation degree between construction land patches was above 0.7. The largest construction land patch within the grid was maintained at 20–40 or 50–70, which will not bring significant changes to UHII. The agricultural land landscape shape should be as simple as possible to reduce UHII; landscape diversity greater than 0.6 can reduce UHII, and <0.6 can increase UHII. These findings provide valuable insights into UHI mitigation and offer strategic guidance for ecological planning to promote sustainable development of large cities in rapidly changing URTZs. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000465841&doi=10.1016%2Fj.scs.2025.106272&partnerID=40&md5=e10ee3e51a2a92c83bcd098cdb33d020,CE,Environmental,Urban-rural transition zone; Surface urban heat island; Interpretable machine learning technology; Sustainable development; Future urban planning decisions; Harbin city,Sustainable Cities and Society
Journal Article,"P., Jiang Guo, Z., Meng, W., Bao, Y.",2025,Multi-agent collaboration for knowledge-guided data-driven design of ultra-high-performance concrete (UHPC) incorporating solid wastes,Elsevier Ltd,164,,,"Data-driven design of concrete attracts increasing interests in waste valorization and decarbonization but lacks generalizability and reliability without concrete domain knowledge. Recent research suggests that knowledge graphs are promising for imparting concrete knowledge into data-driven design, yet manual construction of knowledge graphs is inefficient and hard to scale. This paper presents a multi-agent collaboration framework to streamline knowledge-guided data-driven design of green concrete. The framework decentralize design tasks among specialized agents, and a large language model-based approach is developed to automate the extraction of concrete knowledge for constructing concrete knowledge graphs. The framework has been applied to create a knowledge graph and design green ultra-high-performance concrete (UHPC). The primary novelties of this research involve the multi-agent collaboration framework for designing UHPC and the automatic extraction of UHPC knowledge for constructing the knowledge graph. Results show that concrete knowledge is imparted into data-driven design of UHPC and enables explicit interpretation of machine learning outcomes regarding physical and chemical mechanisms, advancing the transition from purely data-driven to knowledge-guided design of eco-friendly composite materials. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010843818&doi=10.1016%2Fj.cemconcomp.2025.106230&partnerID=40&md5=c1e3321461a5511b4ddaadc1c89c0d0d,CE,Materials,Physicochemical variation; Solid waste valorization; Ultra-high-performance concrete (UHPC); Human-computer interaction; Interpretable machine learning; Knowledge based system,Cement and Concrete Composites
Journal Article,"P., Meng Guo, W., Bao, Y.",2024,Knowledge-guided data-driven design of ultra-high-performance geopolymer (UHPG),Elsevier Ltd,153,,,"Geopolymer has been identified as a promising family of sustainable construction materials alternative to cement-based materials. However, designing geopolymer utilizing solid wastes is a challenging task given the large variations of solid wastes in their physical and chemical properties. To overcome this challenge, this paper proposes a knowledge graph-guided data-driven approach to design geopolymer utilizing solid wastes, aimed at achieving high mechanical properties, low material cost, and low carbon emission, while largely improving material discovery efficiency. The proposed approach seamlessly integrates knowledge graph, machine learning, and multi-objective optimization, and has been utilized to design ultra-high performance geopolymer (UHPG). This approach has two main novelties: (1) The incorporation of knowledge graph imparts geopolymer domain knowledge, making the machine learning model interpretable and compliant with domain knowledge. (2) The consideration of physical and chemical properties of raw materials enables the utilization of various solid wastes. The results show that the proposed approach can reasonably predict geopolymer properties, interpret prediction results, and optimize UHPG design. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202032119&doi=10.1016%2Fj.cemconcomp.2024.105723&partnerID=40&md5=e9b62a53db82165e2dc2dc716f2e9d31,CE,Materials,Explainable machine learning; Interpretable artificial intelligence; Knowledge graph; Multi-objective optimization; Physicochemical information; Ultra-high performance geopolymer,Cement and Concrete Composites
Journal Article,"P. W., Meng Guo, W. A., Bao, Y.",2024,Knowledge graph-guided data-driven design of ultra-high-performance concrete (UHPC) with interpretability and physicochemical reaction discovery capability,Elsevier Ltd,430,,,"Traditional methods for designing concrete materials typically rely on labor-intensive laboratory experiments, resulting in time and cost inefficiencies. Recently, designing concrete using artificial intelligence (AI) methods has shown high efficiency, but existing AI methods often rely solely on data, which can lead to violation with scientific principles and result in models lacking reasoning abilities. To overcome these challenges, this paper presents an interpretable knowledge graph-guided data-driven design approach. By integrating advanced computing techniques with domain knowledge via knowledge graphs, this approach enables the interpretation of data-driven models and uncovers the underlying mechanisms behind predictions. This approach is applied to ultra-high-performance concrete (UHPC) involving complex physicochemical reactions. The domain knowledge about UHPC is imparted using a knowledge graph, and UHPC properties are predicted using a machine learning model considering mixing proportions, processing methods, and physiochemical properties of materials via natural language processing. The results show that the knowledge graph displays crucial design variables and their effects on UHPC properties, aiding in selecting variables for machine learning models and interpreting their results. The prediction accuracy of the machine learning model reached 0.95. The research paves the way for more transparent and scientific AI models for material design and AI-enabled discovery of scientific knowledge.",,CE,Materials,Interpretable artificial intelligence; Machine learning; Knowledge graph; Solid wastes; Physicochemical reactions; Ultra-high-performance concrete,Construction and Building Materials
Journal Article,"Q. C., Jiao Guo, S., Liang, J. Y., Duan, N., Qin, Z. Y., Lu, J.",2025,Exploring urban flood spatial heterogeneity and governance-oriented zoning in central China's mega cities based on multi-source data integration,Elsevier Inc.,27,,,"The combination of extreme rainfall and rapid urbanisation significantly escalates flood risks in major cities, underscoring the importance of accurately identifying causative factors and formulating differentiated management strategies to enhance urban resilience. This study focuses on the central urban areas of Wuhan, Changsha, and Zhengzhou, identifying flood locations outside the core area by comparing remote sensing images from multiple time periods. Use the BERT-BiLSTM-CRF model to extract flood information from social media data, enabling the rapid and accurate capture of urban flood location information within the core area. A multiyear, high-timeliness flooding disaster map identified 1008 flooding points in Changsha, 960 in Wuhan, and 675 in Zhengzhou. Interpretable machine learning methods (XGboots + SHAP) were utilised to analyse floodinducing factors, revealing that the three cities share six primary factors, including topography, vegetation index (NDVI), and impervious surface, though their degrees of influence vary significantly. Topography predominantly affects Changsha, while Wuhan experiences significant impacts from topography, NDVI, and impervious surfaces. In Zhengzhou, NDVI and construction intensity emerge as the dominant factors. Additionally, the K-means dimension reduction clustering method provides a comprehensive analysis of the characteristics of flood-related disaster factors and their contributions. This enables the accurate identification of the causes of disasters in urban areas and their varying degrees of impact. Results highlighted the critical role of topography in flood occurrences, NDVI's mitigation potential within built-up areas, and the relatively limited flooding influence of certain anthropogenic construction activities. These findings recommend that urban flood management strategies account for spatial heterogeneity.",,CE,Water,Central megacities; Urban flooding; Multi-source data fusion; Interpretable modelling; Spatial heterogeneity; Zonal management,Environmental and Sustainability Indicators
Journal Article,"Z., Liu Guo, J., Zhao, P., Li, A., Liu, X.",2023,Spatiotemporal heterogeneity of the shared e-scooter–public transport relationships in Stockholm and Helsinki,Elsevier Ltd,122,,,"Although shared e-scooters have displayed both complementary and competitive relationships with public transport, less attention has been paid to investigating the spatiotemporal variations of such relationships, and how the relationships are associated with the urban built environment. To bridge the gaps, we first explore the spatiotemporal heterogeneity of such relationships by conducting a comparative study of Stockholm and Helsinki based on the empirical data. Then, an explainable AI method is applied to examine the associations between the relationships and built environment. We found that Helsinki presents higher ratios of the competitive dominant areas (D<inf>PET</inf>) compared with Stockholm showing higher ratios of the complementary dominant areas (D<inf>PLE</inf>). The correlation analysis results indicate that the distance-to-public-transport is associated with D<inf>PET</inf> in two cities. Some factors present a non-linear effect, e.g., the density-of-the-population, etc. This study is beneficial for the better integration of shared e-scooters and public transport towards sustainable urban mobility systems. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168795585&doi=10.1016%2Fj.trd.2023.103880&partnerID=40&md5=49f9079bd62ddef46d98f1590797b6c7,CE,Transportation,Shared e-scooters; Public transport; Competitive and complementary trips; Spatiotemporal heterogeneity; Random forest,Transportation Research Part D: Transport and Environment
Journal Article,"J. A., Domínguez-Mota Guzmán-Torres, F. J., Tinoco-Guerrero, G., Tinoco-Ruiz, J. G., Alonso-Guzmán, E. M.",2024,"Extreme fine-tuning and explainable AI model for non-destructive prediction of concrete compressive strength, the case of ConcreteXAI dataset",Elsevier Ltd,192,,,"This groundbreaking study introduces a novel approach employing Extreme Fine-Tuning (XFT) combined with Explainable Artificial Intelligence (XAI) for the accurate, non destructive prediction of concrete compressive strength. By analyzing a state-of-the-art dataset containing 18,480 data points, this study developed a deep neural network that, through extensive hyperparameter optimization, achieves unprecedented prediction accuracy of approximately 98.7%. The novelty of this research lies in the sophisticated integration of XFT and XAI techniques, which not only significantly enhances prediction accuracy but also provides insightful explanations of the model's decision-making process, shedding light on the factors influencing concrete strength. This dual focus on accuracy and explainability represents a significant advancement in the application of Artificial Intelligence (AI) in material science and civil engineering, offering a powerful tool for researchers and practitioners. This study culminates in a model that outperforms existing methodologies in predicting concrete compressive strength, with an accuracy superior to 98.5% in both instances, testing and validation. By integrating XAI into this approach, we have also opened new avenues for understanding the complex relationships between concrete composition and its mechanical properties. This study marks a substantial step forward in the non-destructive evaluation of construction materials. It sets a new benchmark for transparency and interpretability in AI models within the engineering domain. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189760733&doi=10.1016%2Fj.advengsoft.2024.103630&partnerID=40&md5=01b4b8ca37587be20466798d66ed41e1,CE,Materials,Explainable artificial intelligence; Extreme fine-tuning; Compressive strength; Deep neural networks; Non-destructive tests,Advances in Engineering Software
Journal Article,"E., Wikby Haaf, P., Abed, A., Sundell, J., McGivney, E., Rosén, L., Karstunen, M.",2024,A metamodel for estimating time-dependent groundwater-induced subsidence at large scales,Elsevier B.V.,341,,,"Construction of large underground infrastructure facilities routinely leads to leakage of groundwater and reduction of pore water pressures, causing time-dependent deformation of overburden soft soil. Coupled hydrogeomechanical numerical models can provide estimates of subsidence, caused by the complex time-dependent processes of creep and consolidation, thereby increasing our understanding of when and where deformations will arise and at what magnitude. However, such hydro-mechanical models are computationally expensive and generally not feasible at larger scales, where decisions are made on design and mitigation. Therefore, a computationally efficient Machine Learning-based metamodel is implemented, which emulates 2D finite element scenario-based simulations of ground deformations with the advanced Creep-SCLAY-1S-model. The metamodel employs decision tree-based ensemble learners random forest (RF) and extreme gradient boosting (XGB), with spatially explicit hydrostratigraphic data as features. In a case study in Central Gothenburg, Sweden, the metamodel shows high predictive skill (Pearson's r of 0.9-0.98) on 25% of unseen data and good agreement with the numerical model on unseen cross-sections. Through interpretable Machine Learning, Shapley analysis provides insights into the workings of the metamodel, which alignes with process understanding. The approach provides a novel tool for efficient, scenario-based decision support on large scales based on an advanced soil model emulated by a physically plausible metamodel.",,CE,Geotechnical,Regional subsidence; Metamodeling; Machine learning,Engineering Geology
Journal Article,H. Hadj-Mabrouk,2025,Expert System Based on Ontology and Interpretable Machine Learning to Assist in the Discovery of Railway Accident Scenarios,Tech Science Press,84,3,4399–4430,"A literature review on AI applications in the field of railway safety shows that the implemented approaches mainly concern the operational, maintenance, and feedback phases following railway incidents or accidents. These approaches exploit railway safety data once the transport system has received authorization for commissioning. However, railway standards and regulations require the development of a safety management system (SMS) from the specification and design phases of the railway system. This article proposes a new AI approach for analyzing and assessing safety from the specification and design phases of the railway system with a view to improving the development of the SMS. Unlike some learning methods, the proposed approach, which is dedicated in particular to safety assessment bodies, is based on semi-supervised learning carried out in close collaboration with safety experts who contributed to the development of a database of potential accident scenarios (learning example database) relating to the risk of rail collision. The proposed decision support is based on the use of an expert system whose knowledge base is automatically generated by inductive learning in the form of an association rule (rule base) and whose main objective is to suggest to the safety expert possible hazards not considered during the development of the SMS to complete the initial hazard register. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014185960&doi=10.32604%2Fcmc.2025.067143&partnerID=40&md5=2403fd8f973e01137281ee291424d3da,CEM,Safety,artificial intelligence; ontology; semi-supervised learning; expert system; association rules; railways; safety; hazard; accident scenarios; classification; assessment,"Computers, Materials & Continua"
Journal Article,"P., Zhang Han, J., Chen, X., Shao, G., Huang, Z., Sun, J., Shi, X., Yang, X., Shao, L.",2025,Explainable Riemannian Manifold Learning for Application Scene Generalization With Distributed Acoustic Sensing System,Institute of Electrical and Electronics Engineers Inc.,,,,"Intrusion event detection in distributed fiber optic sensing systems plays a vital role in urban emergency management and public safety. However, current AI-based systems face high false positive and false negative rates, as well as limited generalizability, due to variations in dataset quality, processing methods, and model architectures. To address these issues, we propose the Riemannian-Transformer-OTDR (RTO) model, an explainable deep-learning approach based on Riemannian manifolds. By exploiting the manifold space, our method identifies stable latent representations of dynamic data distributions from similar events, embedding intrusion detection within this latent manifold. The RTO model accurately identifies identical event types across diverse scenarios, reducing misclassification rates and enhancing model stability. Field tests using a distributed fiber optic sensing system validated the effectiveness of RTO: it achieved 98.43% accuracy in a subway construction scenario and demonstrated its cross-scenario generalization ability by maintaining 94.21% accuracy in gas pipeline monitoring, with 80.79% accuracy in a 100% noisy environment. Additionally, signal structure analysis of misclassified samples revealed insights into the model’s learning process. Our analysis of performance degradation across scenarios and noise levels further clarifies the RTO mechanisms and explainability. Ultimately, the RTO enables precise, retraining-free intrusion event classification across multiple scenarios, offering a practical solution for rapid event detection deployment in various domains. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010214973&doi=10.1109%2FJIOT.2025.3586295&partnerID=40&md5=7df3b0bcecb5a32fb9550a1b3caedc22,CE,GIS / Remote Sensing,Distributed acoustic sensing (DAS); explainable deep learning; industrial intrusion event detection; Riemannian manifold; unsupervised domain adaptation,IEEE Internet of Things Journal
Journal Article,"T., Zhang Han, Z., Wu, W.",2025,Fire resistance rating prediction of timber-to-steel connections and design optimization informed by explainable machine learning,Elsevier Ltd,156,,,"Timber as a construction material is experiencing its renaissance, while fire safety is a critical factor for timber-based building design. Currently, the fire resistance rating of wood-steel-wood (WSW) connections is evaluated using empirical equations derived from experimental results. However, these equations consider a limited set of parameters and lack interpretability. This paper developed an explainable machine learning (ML) model considering comprehensive parameters related to connection's configuration, based on 140 experimental and experimental-validated numerical data. The performances of various machine learning models are evaluated in terms of predicting the fire resistance rating of connections after hyperparameter tuning. The eXtreme Gradient Boosting (XGBoost) model outperforms other ML models (R2=0.93) and empirical equations. The local sensitivity analysis (LSA), global sensitivity analysis (GSA), and SHapley Additive exPlanations (SHAP) analysis are conducted based on the XGBoost model to investigate the contributions of nine parameters to the fire resistance rating. Both sensitivity analysis and SHAP analysis identify timber thickness and load ratio as the primary factors influencing fire resistance. Finally, the calibrated XGBoost model is incorporated into a non-dominated sorting genetic algorithm (NSGA-II) to optimize the design, aiming to minimize the self-weight of the connection while maximizing the fire resistance rating and load-carrying capacity of the connection subjected to constraints on limited dimensions. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006671600&doi=10.1016%2Fj.engappai.2025.111127&partnerID=40&md5=2c08c97188b469cdc72e725f2a66055d,CE,Structural,Fire resistance rating; Timber-to-steel connections; Machine-learning models; Explainable machine-learning; Design optimization,Engineering Applications of Artificial Intelligence
Journal Article,"Haiyan, Wang Hao, Yan",2023,Modeling Dynamics of Community Resilience to Extreme Events with Explainable Deep Learning,American Society of Civil Engineers,24,2,04023013,"Community resilience provides a paradigm guiding communities? preparedness and mitigation efforts to counter the increasing risks of extreme events (EEs). Knowledge regarding how communities perform during and after EEs can inform proactive resilience enhancement practices. However, the EE impacts on impacted communities are influenced by multiple variables and their implicit interactions, which are difficult to be understood and modeled with conventional mathematical or statistical models. Thus, we calibrate a spatio-temporal deep learning model to capture the dynamics of impacted communities and use explainable artificial intelligence (XAI) approach to interpret the influence of communities? social and physical properties on EE impacts. Specifically, we couple graph convolutional neural network (GCN) and long short-term memory (LSTM) to model the mobility dynamics for 666 census tracts (i.e., communities) in 14 medium-sized US cities affected by recent hurricanes. The model takes both static variables characterizing communities? preexisting conditions and dynamic variables depicting the changing hazard exposure. The interpretation of the model predictions based on DeepLIFT shows that community resilience, inferred from the perturbation of human mobility in this research, is highly event and geography dependent. Variables including walkability, green spaces, and civil participation generally contribute to fewer mobility perturbations, i.e., resilience contributive, while automobile-oriented accessibility and social vulnerability lead to more mobility perturbations when hazard conditions are controlled. This study empirically validates the mediative role of communities? social and physical properties on EE impacts and promotes more data-driven approaches for understanding and anticipating complex, dynamic, and place-specific community resilience.",https://doi.org/10.1061/NHREFO.NHENG-1696,CE,Environmental,Community resilience; Complex systems; Deep learning; Extreme events (EEs); Explainable artificial intelligence,Natural Hazards Review
Journal Article,"N. Y., Li Hao, X. Z., Han, D. P., Nie, W. B.",2024,Quantifying the Impact of Street Greening during Full-Leaf Seasons on Emotional Perception: Guidelines for Resident Well-Being,Multidisciplinary Digital Publishing Institute (MDPI),15,1,,"Quantifying the emotional impact of street greening during the full-leaf seasons in spring, summer, and fall is important for well-being-focused urban construction. Current emotional perception models usually focus on the influence of objects identified through semantic segmentation of street view images and lack explanation. Therefore, interpretability models that quantify street greening's emotional effects are needed. This study aims to measure and explain the influence of street greening on emotions to help urban planners make decisions. This would improve the living environment, foster positive emotions, and help residents recover from negative emotions. In Hangzhou, China, we used the Baidu Map API to obtain street view images when plants were in the full-leaf state. Semantic segmentation was used to separate plant parts from street view images, enabling the calculation of the Green View Index, Plant Level Diversity, Plant Color Richness, and Tree-Sky View Factor. We created a dataset specifically designed for the purpose of emotional perception, including four distinct categories: pleasure, relaxation, boredom, and anxiety. This dataset was generated through a combination of machine learning algorithms and human evaluation. Scores range from 1 to 5, with higher values indicating stronger emotions and lower values indicating less intense ones. The random forest model and Shapley Additive Explanation (SHAP) algorithm were employed to identify the key indicators that affect emotions. Emotions were most affected by the Plant Level Diversity and Green View Index. These indicators and emotions have an intricate non-linear relationship. Specifically, a higher Green View Index (often indicating the presence of 20-35 fully grown trees within a 200 m range in street view images) and a greater Plant Level Diversity significantly promoted positive emotional responses. Our study provided local planning departments with support for well-being-focused urban planning and renewal decisions. Based on our research, we recommend the following actions: (1) increase the amount of visible green in areas with a low Green View Index; (2) plant seasonal and flowering plants like camellia, ginkgo, and goldenrain trees to enhance the diversity and colors; (3) trim plants in areas with low safety perception to improve visibility; (4) introduce evergreen plants like cinnamomum camphor, osmanthus, and pine.",,CE,Environmental,street view images; semantic segmentation; human-machine adversarial scoring; emotional perception driving factors; Plant Level Diversity; Green View Index; interpretable machine learning; Shapley value,Forests
Journal Article,"A., Thabet Harode, W., Leite, F.",2024,Formulation of Feature and Label Space Using Modified Delphi in Support of Developing a Machine-Learning Algorithm to Automate Clash Resolution,American Society of Civil Engineers (ASCE),150,3,,"To improve the current manual and iterative nature of clash resolution on construction projects, current research efforts continue to explore and test the utilization of machine-learning algorithms to automate the process. Though current research shows significant accuracy in automating clash resolution, many have failed to provide clear explanation and justification for the selection of their feature and label space. Since this is critical in developing an effective and explainable solution in machine learning, it is crucial to address this research gap. In this paper, the authors utilize an in-depth literature review and industry interviews to capture domain knowledge on how design clashes are resolved by industry experts. From analysis of the knowledge captured, we identified 23 factors considered by experts when resolving clashes and five alternative solutions/options to resolve a clash. Using a pool of industry experts, a modified Delphi approach was conducted to validate the factors and options and to determine a priority ranking. The authors identified 94 industry experts based on a predetermined qualification matrix to take part in the modified Delphi. Twelve participants responded and took part in the first round, and 11 completed the second round. A consensus was reached on all clash factors and resolution options. Factors including clashing elements type,""""constrained slope,""""critical element in the clash,""""location of the clash,""""code compliance,""and ""project stage clashing element is in""were ranked as the most important factors, while ""clashing element material""and ""insulation type""were considered the least important. Participants also showed more preference to the ""moving the clashing element with low priority in/along x-y-z directions""option to resolve clashes. These identified factors and options will be utilized to collect specific clash data to train and test effective and explainable machine-learning algorithms toward automating clash resolution. © 2024 Elsevier B.V., All rights reserved.""",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182509567&doi=10.1061%2FJCEMD4.COENG-14167&partnerID=40&md5=18368727c07b38661e5193ce6b3a5e0c,CEM,"Digital Construction (BIM, AI, Sensors, Robotics)",NaN,Journal of Construction Engineering and Management
Journal Article,"M., Lu Hasan, M.",2024,Enhanced model tree for quantifying output variances due to random data sampling: Productivity prediction applications,Elsevier B.V.,158,,,"The data used for productivity modeling represents a sample of a company's performance taken at a particular time, which leads to varying model parameters and results in a variance of the predicted productivity. This research enhances the capability of the nonlinear machine learning method called Model Tree by integrating error propagation theory to determine the variance for the predicted productivity based on a relatively large dataset available for productivity analysis. The enhanced model tree is formalized and applied in modeling structural steel fabrication productivity and piping spool fabrication productivity. The enhanced model tree is preferred over artificial neural networks due to the model's capability to explain the productivity model concerning the variance of its predicted point value and the underlying reasoning logic. The enhanced model tree will potentially find its applications in construction beyond predicting labor productivity and make a further impact on implementing explainable artificial intelligence (XAI) in wide-ranging engineering domains. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178615994&doi=10.1016%2Fj.autcon.2023.105218&partnerID=40&md5=bef427d8c1daa6474a78465167d11c05,CEM,Productivity / Workforce,Productivity model; Regression model; Variance analysis; Labor cost estimate; Model tree,Automation in Construction
Journal Article,"M. H. A., Abdallah Hasnain Ayub Khan, A., Cuisinier, O.",2025,Insights into the strength development in cement-treated soils: An explainable AI-based approach for optimized mix design,Elsevier Ltd,180,,,"Existing empirical models for predicting the unconfined compressive strength (UCS) of cement-treated soils (CTS) are based on specific soil–cement types and a limited set of strength-controlling factors (e.g., soil properties, compaction, curing, binder dosage, etc.). While machine learning models in the literature incorporate a broader range of these factors as input features, they tend to focus on improving prediction accuracy with less emphasis on offering insights into the relative importance of these factors. This research aims at deciphering the strength development in CTS by quantitatively accessing the relative importance of strength-controlling factors and identifying the dependencies between these factors using explainable artificial intelligence (XAI). A comprehensive database of CTS was collected from the literature, covering a wide range of soil and cement types. A high-performance grid-search-optimized XGB model was developed with features related to soil classification, compaction conditions, cement types and dosage, curing time, and porosity to volumetric cement content ratio (η/C<inf>iv</inf>) used as input features to predict the UCS. The model's reliability is evidenced by its good generalization capability on an independently generated laboratory dataset. Furthermore, the Shapley additive explanations (SHAP) helped in identifying the most influential parameters for UCS. SHAP feature dependence analysis further revealed interactions and dependencies between features, providing a deeper understanding of factors affecting the UCS. While compaction parameters were not ranked high individually, many features showed significant interaction and dependence on these parameters. These dependencies motivated developing design charts aimed at optimizing the UCS through compaction parameters. Consequently, innovative design charts were developed using artificially generated dataset, providing the possible combinations of dry density and water content to achieve the desired strength at fixed cement dosage, hence offering effective and cost-efficient solutions to soil stabilization projects. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215853632&doi=10.1016%2Fj.compgeo.2025.107103&partnerID=40&md5=0afeb71a03c66bb8eece860492d87d85,CE,Geotechnical,Cement-treated soils; Explainable artificial intelligence; Extreme gradient boost; Shapley additive explanations,Computers and Geotechnics
Journal Article,"R., Zhang He, L., Tiong, R. L. K.",2025,Two-stage stochastic optimization for emergency management of metro systems under uncertain storm floods,Elsevier Ltd,264,,,"Climate change and urbanization have increasingly exacerbated the threat of storm floods to urban metro systems. To enhance the flood emergency management of metro systems, this study proposes a two-stage stochastic optimization model. In this model, the closure decisions for risky stations and the allocation of flood control resources are implemented before and during the rainstorm, respectively, to maximize the average utility of passengers in the metro network. A case study on the Shanghai metro system is conducted to demonstrate the applicability and effectiveness of the proposed model. The results indicate that the two-stage stochastic optimization model can generate refined closure schemes and dynamically adaptive protection schemes for risky metro stations. Compared to one-stage strategies that do not consider the uncertainty of rainstorms, the two-stage model achieves higher passenger utility. Furthermore, the mechanisms behind the closure decisions made by the two-stage model are interpreted using an explainable artificial intelligence (XAI) technique, SHAP (SHapley Additive explanation). It is revealed that a metro station with low passenger volume in a high-rainfall sub-catchment has a greater probability of being closed before floods. Future works can be conducted to further explore feedback mechanisms between the two optimization stages or optimize the location and inventory of resource warehouses for metro systems. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007709885&doi=10.1016%2Fj.ress.2025.111325&partnerID=40&md5=5306cd436efaa8204eb396d675fe8f9a,CE,Transportation,Flood; Metro network; Optimization; Resource allocation; Station closure,Reliability Engineering & System Safety
Journal Article,"Z., Hu Hou, S., Wang, W.",2025,Interpretable machine learning models for predicting probabilistic axial buckling strength of steel circular hollow section members considering discreteness of geometries and material,SAGE Publications Inc.,28,5,828–844,"Steel circular hollow section (CHS) members are widely utilized as axial force-resisting structural members in civil engineering structures. The buckling strength under axial loads is one of the critical parameters to determine the performance of the steel CHS members, which is significantly affected by the discreteness introduced by geometries, material, and initial imperfections. However, the reduction factor employed in the modern design codes (i.e. Chinese codes and EC3) only accounts for the reduction caused by all kinds of discreteness and does not reflect the impacts of every single discreteness and imperfection. To fill the gap, this paper proposed an interpretable machine-learning method to provide the probabilistic axial buckling strength of steel CHS members prediction result in a distribution form with the consideration of detailed discreteness. The model to predict the nominal axial buckling strength of steel CHS members was first developed utilizing ten machine learning algorithms after sufficient numerical simulations, where the numerical model was verified using test results. The artificial neural network (ANN) was selected for developing the prediction model due to its highly reliable performance in testing. The developed ANN models were further interpreted utilizing Shapley Additive exPlanations (SHAP) to determine the interrelationship of different parameters. Then, the probabilistic axial bucking strength prediction model was established based on the developed ANN models, where the Latin hypercube sampling method was applied to address the discreteness of geometries, material, and initial imperfections. The generated probabilistic axial bucking strength prediction model’s effectiveness was verified by the evidence that the machine learning prediction results can highly match the numerical results' probability density function and the result from codes while significantly reducing the computation time. Finally, the design parameters’ impact on the axial buckling strength’s discreteness was evaluated using the global sensitivity analysis (GSA) method. The result shows that the discreteness of design parameters substantially influences the distribution of the axial buckling strength of the steel CHS members and the proposed prediction model can provide an accurate probabilistic distribution prediction. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208022570&doi=10.1177%2F13694332241289175&partnerID=40&md5=121a959075ca7845043c084e43c6dc29,CE,Structural,artificial neural network; discreteness; probabilistic axial buckling strength; machine learning; steel circular hollow section members,Advances in Structural Engineering
Journal Article,"C. G., Zeng Hu, H.",2025,Decoding spatial patterns of urban thermal comfort: Explainable machine learning reveals drivers of thermal perception,Elsevier Inc.,114,,,"Thermal comfort (TC) is a pivotal indicator of urban quality of life and influences public health, productivity, and satisfaction. This study leverages remote sensing data from 2019 to 2023 to construct a national-scale TC framework using the Modified Temperature and Humidity Index (MTHI). This analysis reveals the spatial heterogeneity of TCs across China and their key driving mechanisms. The findings show a northwest-southeast gradient in the TC, with a decreasing contrast in this direction and a north-south disparity alongside a northward shift in heat discomfort centers. High-comfort zones are found in the western plateaus, northeastern regions, and southern mountains; conversely, low-comfort zones are concentrated in the northwest, North China Plain, and southern basins, particularly in the densely urbanized eastern coastal cities and central urban clusters. Coastal areas show high internal variability, whereas inland plateaus are more stable. Natural environmental factors have emerged as the primary drivers of TC. Shapley additive explanations (SHAP) values show a continuous upward trend, underscoring the crucial role of enhanced NDVI in improving TC. Although socioeconomic factors show increased SHAP values, their adverse impacts persist as urbanization and rising building density exacerbate TC deterioration. Landscape factors exert complex effects on TC, with water body landscapes displaying an optimal regulatory range. Interactions among driving factors, characterized by direct and complex trade-offs, further modulate and intensify the effects of TC. The proposed multiscale optimization framework provides strategic insights for managing China's urban thermal environment and offers guidance for other regions with similar climate challenges.",,CE,Environmental,Thermal comfort; Spatial heterogeneity; Explainable machine learning; Driving mechanisms; Landscape patterns,Environmental Impact Assessment Review
Journal Article,"M., Sun Hu, J., Wu, B., Wu, H., Xu, Z.",2024,Shield Tunnel (Segment) Uplift Prediction and Control Based on Interpretable Machine Learning,Multidisciplinary Digital Publishing Institute (MDPI),16,2,,"Shield tunnel segment uplift is a common phenomenon in construction. Excessive and unstable uplift will affect tunnel quality and safety seriously, shorten the tunnel life, and is not conducive to the sustainable management of the tunnel’s entire life cycle. However, segment uplift is affected by many factors, and it is challenging to predict the uplift amount and determine its cause accurately. Existing research mainly focuses on analyzing uplift factors and the uplift trend features for specific projects, which is difficult to apply to actual projects directly. This paper sorts out the influencing factors of segment uplift and designs a spatial-temporal data fusion mechanism for prediction. On this basis, we extract the key influencing factors of segment uplift, construct a prediction model of segment uplift amount based on Extreme Gradient Boosting (XGBoost) v2.0.3, and use SHapley Additive exPlanation (SHAP) v0.44.0 to locate factors affecting uplift, forming an Auxiliary Decision-making System for Segment Uplift Control (ADS-SUC). An ADS-SUC not only detects the sudden change of the segment uplift successfully and predicts the segment uplift in practical engineering accurately, it also provides a feasible method to control the uplift in time, which is of great significance for reducing the construction risk of the tunnel project and ensuring the quality of the completed tunnel. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183415195&doi=10.3390%2Fsu16020910&partnerID=40&md5=ae27e9ea0f0c89636e13fe2fcb25cbc1,CE,Geotechnical,shield tunnel construction; segment uplift prediction; segment uplift control; interpretability; machine learning,Sustainability
Journal Article,"T., Zhang Hu, H., Cheng, C., Li, H., Zhou, J.",2024,Explainable machine learning: Compressive strength prediction of FRP-confined concrete column,Elsevier Ltd,39,,,"Fibre reinforced polymers (FRP) are widely used for the strengthening of beams, slabs and columns due to their light weight and high strength. Compared to the traditional methods like steel cages or steel jackets, it reduces the time and economic cost significantly. This study focuses on the prediction of the compressive strength of FRP-confined concrete columns, which is influenced by multiple parameters. Initially, through experimental observations and the analysis of parameter mechanisms, the key parameters affecting compressive strength were identified. Six prediction models, namely linear regression, ridge regression, decision tree, random forest, and eXtreme Gradient Boosting, were subsequently constructed and evaluated to determine the optimal machine learning model. A comparison between the machine learning models and mathematical models was conducted. Furthermore, SHapley Additive exPlanation (SHAP) analysis was performed to ascertain the parameter importance and sensitivity of the machine learning model, and the analysis results were compared with existing models. The results indicate that compared to other machine learning and mathematical models, the eXtreme Gradient Boosting model demonstrates superior robustness and accuracy. The thickness and elastic modulus of FRP, as well as the concrete strength, are identified as crucial parameters that positively influence compressive strength. The relationships between the former two parameters and compressive strength follow a logarithmic function, while the relationship between concrete strength and compressive strength is approximately linear. These findings hold implications for the future development of codes and standards. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190333629&doi=10.1016%2Fj.mtcomm.2024.108883&partnerID=40&md5=0fb192c108edfc955b34a2139c040f5b,CE,Structural,Fibre reinforced polymers (FRP); Concrete column; Compressive strength; Machine learning; SHapley additive exPlanation (SHAP),Materials Today Communications
Journal Article,"W. R., Wu Hu, K., Liu, H., Luo, W. B., Li, X. X., Guan, P.",2025,Interpretable machine learning approach for TBM tunnel crown convergence prediction with Bayesian optimization,Frontiers Media S.A.,13,,,"Accurate prediction of crown convergence in Tunnel Boring Machine (TBM) tunnels is critical for ensuring construction safety, optimizing support design, and improving construction efficiency. This study proposes an interpretable machine learning method based on Bayesian optimization (BO) and SHapley Additive exPlanations (SHAP) for predicting crown convergence (CC) in TBM tunnels. Firstly, a dataset comprising 1,501 samples was constructed using tunnel engineering data. Then, six classical ML models, namely, Support Vector Regression, Decision Tree, Random Forest, Light Gradient Boosting Machine (LightGBM), eXtreme Gradient Boosting, and K-nearest neighbors-were developed, and BO was applied to tune the hyperparameters of each model to achieve accurate prediction of CC. Subsequently, the SHAP method was adopted to interpret the LightGBM model, quantifying the contribution of each input feature to the model's predictions. The results indicate that the LightGBM model achieved the best prediction performance on the test set, with root mean squared error, mean absolute error, mean absolute percentage error, and determination coefficient values of 0.9122 mm, 0.6027 mm, 0.0644, and 0.9636, respectively; the average SHAP values for the six input features of the LightGBM model were ranked as follows: Time (0.1366) > Rock grade (0.0871) > Depth ratio (0.0528) > Still arch (0.0200) > Saturated compressive strength (0.0093) > Rock quality designation (0.0047). Validation using data from a TBM water conveyance tunnel in Xinjiang, China, confirmed the method's practical utility, positioning it as an effective auxiliary tool for safer and more efficient TBM tunnel construction.",,CE,Geotechnical,TBM tunnel; crown convergence prediction; machine learning; model explanation; bayesian optimization,Frontiers in Earth Science
Journal Article,"B. Y., Zhang Huang, Q. Y., Yang, P. F., Yang, D.",2025,An Interpretable Machine Learning Model-Based Study of the Relationship between Built Environment and Residents' Daily Travel Distance,Spatial Planning and Sustainable Development Press (SPSD Press),13,2,129–144,"Urban sustainability and reduced carbon emissions have become critical challenges in contemporary urban development, with residents' daily travel distances significantly impacting both the environmental and social aspects of city life. Despite extensive research on travel behavior, Crossroads Density: Number of Crossroads within 500m the complex relationship between the built environment and travel behavior remains inadequately understood due to the limitations of traditional linear analyses. To address this gap, this study introduces an analytical approach grounded in an interpretable machine learning framework to investigate the pivotal environmental factors influencing residents' daily travel distances and to understand their underlying mechanisms. Initially, a variety of machine learning regression models were constructed to evaluate their predictive efficacy regarding residents' daily travel distances. Among these, the CatBoost model demonstrated the most accurate fit, highlighting the robustness of the methodology. Subsequently, the SHAP (SHapley Additive exPlanations) methodology was employed to ascertain the significance of each feature's contribution, thereby elucidating the extent and non-linear dynamics of various built environmental indicators on travel distances. The findings indicate that the POI (Point of Interest) diversity index, the proportion of main roads, the proximity to the city center, and other factors significantly influence residents' travel distances, exhibiting observable interactive effects. Importantly, these results underscore the implications of the built environment on travel behavior, offering actionable insights for urban planners. By focusing on the built environment's impact on travel distances, this study's interpretable machine learning framework delves into the multi-factorial non-linear correlations, providing a valuable tool for enhancing urban spatial planning, reducing travel distances, lowering traffic emissions, and thereby contributing to the sustainable development of cities.",,CE,Transportation,Travel distance; Built environment; Machine learning; Interpretable artificial intelligence; Feature importance,International Review for Spatial Planning and Sustainable Development (IRSPSD)
Journal Article,"H., Lei Huang, L., Xu, G., Cao, S., Ren, X.",2025,Machine learning approaches for predicting mechanical properties of steel-fiber-reinforced concrete,Elsevier Ltd,45,,,"Recent advancements in machine learning enable accurate prediction of concrete's mechanical properties, offering efficiency in data analysis and reducing material waste in civil engineering. What sets this study apart from the literature is the simultaneous prediction of Compressive, Tensile, and Flexural Strength for reinforced concrete (SFRC). This integrated prediction could streamline the material selection process, enhance the durability of concrete structures, and improve performance in real-world applications such as pavements, bridges, and industrial floors. Unique variables like fiber length and content are among the input variables for predictions, which may lead to more cost-effective structural solutions. In this study, base models of the Ridge Regression and Quantile Regression were cross-validated, and the best folds of the compiled dataset for training and testing models were selected. The Sled Dog Optimizer, as a recently developed strong optimization algorithm, is utilized for fine-tuning hyperparameters of the models and enhancing prediction performance. Also, the Bayesian model combination method was employed to develop a hybrid ensemble model for reliably predicting concrete strengths by combining the capabilities of both hybrid models. Such hybrid and ensemble models are rare in the literature in this field. Also, SHAP-based explainable AI utilized for interpreting importance of variables in predictions. The hybrid ensemble model performed best in predicting Compressive Strength, achieving the lowest RMSE (3.295 MPa) and highest R² (0.974). For Flexural and Tensile Strength, it attained R² values of 0.986 and 0.974, respectively, with 90 % and 100 % of predictions showing errors below 10 %. Among hybrid models, RRSD excelled, with R² values of 0.94 for Compressive Strength and 0.98 for Flexural and Tensile Strength. These results demonstrate the reliability of machine learning for modeling SFRC properties. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000794982&doi=10.1016%2Fj.mtcomm.2025.112149&partnerID=40&md5=7c53aa26991b295ecb9a0b3c5a79381b,CE,Materials,Experimental concrete samples; Steel-fiber-reinforced concrete; Strength properties; Regression prediction models; Optimization algorithms,Materials Today Communications
Journal Article,"R. Y., Wang Huang, H. T., Makinia, J., Jin, S. T., Zhou, Z., Zhang, M., Yu, C. Y., Xie, L.",2025,Optimizing the chemical removal of phosphorus for wastewater treatment: Insights from interpretable machine learning modeling with binary classification of elasticity and productivity,Elsevier B.V.,215,,,"Ensuring compliance with total phosphorus (TP) discharge standards is essential in wastewater sector to alleviate eutrophication. This study focused on optimizing chemical removal of TP from a typical wastewater plant (WWTP) where poly aluminum chloride (PAC) is used after anaerobic-anoxic-oxic technology. With PAC consumption and TP removal in one-year daily data combined as input-output system, binary classifications of decoupling and congestion patterns representing elasticity and productivity were conducted to mitigate irregular data mappings caused by inaccurate dosing. Through interpretable machine learning (IML) modeling, influent conditions were recognized as significant factors. Biochemical oxygen demand to TP ratio exceeding 36.07 and loading capacity rates departing 99.46 %similar to 106.64 % increased decoupled and congested probability. These findings highlighted the adjust on PAC dosage for redundancy prevention according to varied influent conditions. The evaluation and modeling workflow with IML emphasized the need for systematic optimization to achieve sustainable WWTP operations and low-carbon development in wastewater sector.",,CE,Environmental,Wastewater treatment plant; Pollutant removal; Interpretable machine learning; Binary classification modeling; Process optimization,"Resources, Conservation and Recycling"
Journal Article,"X., Huang Huang, J. H., Kaewunruen, S.",2025,An explainable machine learning system for efficient use of waste glasses in durable concrete to maximise carbon credits towards net zero emissions,Elsevier Ltd,193,,539–550,"Recycling waste glass (WG) can be time-consuming, costly, and impractical. However, its incorporation into concrete significantly reduces environmental impact and carbon emissions. This paper introduces machine learning (ML) to civil engineering to optimise WG utilisation in concrete, supporting sustainability objectives. By employing a dataset of 471 experimental samples of waste glass concrete (WGC), various ML algorithms are applied, including Gradient Boosting Regressor (GBR), Random Forest (RF), Support Vector Regression (SVR), Adaptive Boosting (AdaBoost), Deep Neural Network (DNN), and k-Nearest Neighbours (kNN), to predict properties containing compressive strength (CS), alkali-silica reaction (ASR), and saved carbon credits (SCC). The proposed models achieve outstanding prediction performance with Coefficient of determination (R2) values of 0.95 for CS, 0.97 for ASR, and 0.99 for SCC using GBR and SVR, demonstrating high prediction accuracy with Root mean square error (RMSE) values of 3.31 MPa for CS, 0.03 % for ASR, and 0.11 for SCC. The SHapley Additive exPlanations (SHAP) analysis is utilised to interpret the model results, ensuring transparency and interpretability of the proposed ML models. The results reveal that the incorporation level of WG is a more significant influencing factor for these properties than the mean size of WG (MSWG).",,CE,Materials,Machine learning; Sustainable concrete; Waste glass concrete; Carbon credit optimisation,Waste Management
Journal Article,"W., Zhu Huo, Z., Sun, H., Ma, B., Yang, L.",2022,Development of machine learning models for the prediction of the compressive strength of calcium-based geopolymers,Elsevier Ltd,380,,,"Compressive strength is an important mechanical index that determines the mixture design of geopolymer, and its accurate prediction is essential. The existing experiment-based and statistical methods are time-consuming, labor-intensive and inaccurate. This study aims to develop an effective, reliable and interpretable machine learning (ML) model for predicting the compressive strength of calcium-based geopolymers. Feature engineering was constructed with molar ratios of raw material oxide composition, curing system, and mixing design. A total of eight algorithms in three types, traditional ML algorithms, integrated tree-based ML algorithms, and deep neural network algorithm, were employed to predict the compressive strength, and their differences, advantages, and disadvantages were compared. The importance of input variables in model training was evaluated. The contribution and influence pattern of input features on the development of compressive strength were revealed using the SHapley Additive exPlanations (SHAP) and inverse prediction. The results demonstrate that among the eight models proposed, the XGB model had the highest prediction accuracy (91%) and the lowest root mean squared error (3.85 MPa). Based on the importance analysis and the SHAP value, the parameters that had the greatest impact on the compressive strength were curing age, n(H<inf>2</inf>O)/n(Na<inf>2</inf>O), curing temperature, n(SiO<inf>2</inf>)/n(CaO) and the mass ratio of alkali activation solution to solid powder (L/S). The effects of input features on the compressive strength development of calcium-based geopolymers captured by SHAP and inverse predictions based on the best predictive model were consistent with the experimental results and theoretical understanding. The research in this paper facilitates the rapid prediction, improvement and optimization of the proportioning design and application of calcium-based geopolymers, and also provides a theoretical basis for the utilization of industrial and construction waste, in line with sustainable and low-carbon development strategies. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142203881&doi=10.1016%2Fj.jclepro.2022.135159&partnerID=40&md5=61116c005fd8685c5ebe8bd56ba7dee2,CE,Materials,Calcium-based geopolymer; Oxide composition; Compressive strength; Prediction; Machine learning; SHapley additive exPlanations,Journal of Cleaner Production
Journal Article,"S., Nugraha Idrees, J. A., Tahir, S., Choi, K., Choi, J., Ryu, D. H., Kim, J. H.",2024,Automatic concrete slump prediction of concrete batching plant by deep learning,Elsevier Ltd,18,,,"The workability of fresh concrete is highly important in terms of construction quality and safety. Slump tests are required every 120 m³, yet automated monitoring for each concrete batch remains unavailable in the actual concrete batching plant. To mitigate this issue, we propose an automatic slump prediction method based on the VGG16 neural network by analyzing the video from the final discharge hopper of the batching plant. Additionally, Explainable AI (XAI) is adopted to evaluate and validate our automatic concrete quality inspection approach. Iteratively examining XAI outputs and applying necessary adjustments in data preprocessing helps to achieve better overall performance. The proposed video classification method performed by averaging over the image-level predictions can classify the concrete into four slump classes with an average precision of 85% and an average F1 score of 87%. This demonstrates the possibility of continuous quality evaluation for all concrete produced in the concrete batching plant. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195180388&doi=10.1016%2Fj.dibe.2024.100474&partnerID=40&md5=d20b0d525c538abe0cd48f8922d68647,CEM,"Digital Construction (BIM, AI, Sensors, Robotics)",Concrete batching plant; Deep learning; Concrete slump; Quality inspection; Construction safety,Developments in the Built Environment
Journal Article,"M. F., Fawad Javed, M., Lodhi, R., Najeh, T., Gamil, Y.",2024,Forecasting the strength of preplaced aggregate concrete using interpretable machine learning approaches,Springer Nature,14,1,,"Preplaced aggregate concrete (PAC) also known as two-stage concrete (TSC) is widely used in construction engineering for various applications. To produce PAC, a mixture of Portland cement, sand, and admixtures is injected into a mold subsequent to the deposition of coarse aggregate. This process complicates the prediction of compressive strength (CS), demanding thorough investigation. Consequently, the emphasis of this study is on enhancing the comprehension of PAC compressive strength using machine learning models. Thirteen models are evaluated with 261 data points and eleven input variables. The result depicts that xgboost demonstrates exceptional accuracy with a correlation coefficient of 0.9791 and a normalized coefficient of determination (R2) of 0.9583. Moreover, Gradient boosting (GB) and Cat boost (CB) also perform well due to its robust performance. In addition, Adaboost, Voting regressor, and Random forest yield precise predictions with low mean absolute error (MAE) and root mean square error (RMSE) values. The sensitivity analysis (SA) reveals the significant impact of key input parameters on overall model sensitivity. Notably, gravel takes the lead with a substantial 44.7% contribution, followed by sand at 19.5%, cement at 15.6%, and Fly ash and GGBS at 5.9% and 5.1%, respectively. The best fit model i.e., XG-Boost model, was employed for SHAP analysis to assess the relative importance of contributing attributes and optimize input variables. The SHAP analysis unveiled the water-to-binder (W/B) ratio, superplasticizer, and gravel as the most significant factors influencing the CS of PAC. Furthermore, graphical user interface (GUI) have been developed for practical applications in predicting concrete strength. This simplifies the process and offers a valuable tool for leveraging the model's potential in the field of civil engineering. This comprehensive evaluation provides valuable insights to researchers and practitioners, empowering them to make informed choices in predicting PAC compressive strength in construction projects. By enhancing the reliability and applicability of predictive models, this study contributes to the field of preplaced aggregate concrete strength prediction.",,CE,Materials,Preplaced aggregate concrete; Two-stage concrete; Compressive strength prediction; Machine learning models; Construction engineering,Scientific Reports
Journal Article,"D., Guo Jiang, F., Zhang, Z., Yu, X., Dong, J., Zhang, H., Zhang, Z.",2024,Urban Built Environment as a Predictor for Coronary Heart Disease—A Cross-Sectional Study Based on Machine Learning,Multidisciplinary Digital Publishing Institute (MDPI),14,12,,"The relationship between coronary heart disease (CHD) and complex urban built environments remains a subject of considerable uncertainty. The development of predictive models via machine learning to explore the underlying mechanisms of this association, as well as the formulation of intervention policies and planning strategies, has emerged as a pivotal area of research. A cross-sectional dataset of hospital admissions for CHD over the course of a year from a hospital in Dalian City, China, was assembled and matched with multi-source built environment data via residential addresses. This study evaluates five machine learning models, including decision tree (DT), random forest (RF), eXtreme gradient boosting (XGBoost), multi-layer perceptron (MLP), and support vector machine (SVM), and compares them with multiple linear regression models. The results show that DT, RF, and XGBoost exhibit superior predictive capabilities, with all R2 values exceeding 0.70. The DT model performed the best, with an R2 value of 0.818, and the best performance was based on metrics such as MAE and MSE. Additionally, using explainable AI techniques, this study reveals the contribution of different built environment factors to CHD and identifies the significant factors influencing CHD in cold regions, ranked as age, Digital Elevation Model (DEM), house price (HP), sky view factor (SVF), and interaction factors. Stratified analyses by age and gender show variations in the influencing factors for different groups: for those under 60 years old, Road Density is the most influential factor; for the 61–70 age group, house price is the top factor; for the 71–80 age group, age is the most significant factor; for those over 81 years old, building height is the leading factor; in males, GDP is the most influential factor; and in females, age is the most influential factor. This study explores the feasibility and performance of machine learning in predicting CHD risk in the built environment of cold regions and provides a comprehensive methodology and workflow for predicting cardiovascular disease risk based on refined neighborhood-level built environment factors, offering scientific support for the construction of sustainable healthy cities. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213079839&doi=10.3390%2Fbuildings14124024&partnerID=40&md5=741624c7fc216c02df4ca872093b561d,CE,Environmental,built environment; coronary heart disease; machine learning; healthy cities,Buildings
Journal Article,"J., Li Jiang, Z., Bedra, K. B., Long, C., Wu, J., Zhong, Q.",2025,"Predicting outdoor thermal comfort in traditional villages: An explainable machine learning framework integrating model optimization, seasonal variability, and tourist-resident insights",Elsevier Ltd,282,,,"This study developed explainable machine learning (ML) models to enhance outdoor thermal comfort (OTC) prediction in traditional villages, aiming to improve resident health and rural tourism. A village-specific OTC dataset was established through field experiments, and eight ML algorithms were utilized to predict thermal sensation (TSV), comfort (TCV), and acceptance (TAV) votes, with performance compared to empirical and mechanism models. Feature engineering, data resampling, and hyperparameter optimization were applied to optimize ML models, while the Shapley Additive exPlanations (SHAP) framework with dataset segmentation addressed explainability challenges. Results demonstrated that: (1) ML models, particularly XGBoost (XGB), outperformed empirical and mechanism models in TSV predictions. (2) Feature engineering and Bayesian optimization achieved maximum accuracy improvement of 13.6 % and 4.8 %, respectively, while SMOTE resampling enhanced the prediction of minority classes. (3) Meteorological impacts on TSV, TCV, and TAV exhibited seasonal variations, with air temperature (Ta) dominating in summer and global radiation (G) prevailing in winter. Non-meteorological factors, including individual and psychological characteristics, showed divergent adaptation between tourists and residents. These findings offer valuable insights for optimizing village layouts and tourism schedules while advancing data-driven OTC management frameworks. This study bridges ML applications with traditional village sustainability, offering a foundation for intelligent built-environment science. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008995876&doi=10.1016%2Fj.buildenv.2025.113315&partnerID=40&md5=d32b7d8c23ae1cc3eedfec425b270dcc,CE,Environmental,Outdoor thermal comfort; Machine learning; Traditional village; Bayesian optimization; Shapley additive explanations,Building and Environment
Journal Article,"Y. Du, Y. Liu, Y. Yan, J. Fang, X. Jiang",2023,Risk Management of Weather-Related Failures in Distribution Systems Based on Interpretable Extra-Trees,Institute of Electrical and Electronics Engineers Inc.,11,6,1868–1877,"Weather-related failures significantly challenge the reliability of distribution systems. To enhance the risk management of weather-related failures, an interpretable extra-trees based weather-related risk prediction model is proposed in this study. In the proposed model, the interpretability is successfully introduced to extra-trees by analyzing and processing the paths of decision trees in extra-trees. As a result, the interpretability of the proposed model is reflected in the following three respects: it can output the importance, contribution, and threshold of weather variables at high risk. The importance of weather variables can help in developing a long-term risk prevention plan. The contribution of weather variables provides targeted operation and maintenance advice for the next prediction period. The threshold of weather variables at high risk is critical in further preventing high risks. Compared with the black-box machine learning risk prediction models, the proposed model over-comes the application limitations. In addition to generating predicted risk levels, it can also provide more guidance information for the risk management of weather-related failures.",,CE,Energy,Extra-tree; machine learning; interpretability; weather-related failure; distribution system,Journal of Modern Power Systems and Clean Energy
Journal Article,"W. Zhang, Y. Wen, K. J. Tseng, G. Jin",2021,Demystifying Thermal Comfort in Smart Buildings: An Interpretable Machine Learning Approach,Institute of Electrical and Electronics Engineers Inc.,8,10,8021–8031,"Thermal comfort is a key consideration in smart buildings and a number of comfort models are available nowadays to evaluate the comfort level of occupants. However, the models are often complex and hardly interpretable for the developers and operators. Indeed, the model interpretations are beneficial in multifold such as for system inspection and optimization. In this article, we propose an interpretable thermal comfort system to introduce interpretability to any black-box comfort models. First, we focus on the relationship between a model’s input features and output comfort level. The feature impact on comfort is investigated and the impact patterns are shown to be diverse for different features. Second, we unveil the model mechanisms about the data processing inside the model by building the model surrogates based on the interpretable machine learning algorithms. The surrogates offer outstanding fidelity for simulating the actual model mechanisms and the interpretations based on the surrogates are intuitive and informative. Our interpretable comfort system can be integrated with the existing building management systems. Accordingly, we can ease building owner’s concerns about adopting new black-box technologies and enable various smart building applications like smart energy management.",,CE,Energy,Deep learning; interpretable machine learning (ML); smart building; smart city; thermal comfort,IEEE Internet of Things Journal
Journal Article,"S. D., Ou Ju, G. Z., Peng, T., Wang, Y. N., Song, Q. L., Guan, P.",2025,Tunnel water inflow prediction using explainable machine learning and augmented partially missing dataset,Frontiers Media S.A.,13,,,"Accurate prediction of water inrush volumes is essential for safeguarding tunnel construction operations. This study proposes a method for predicting tunnel water inrush volumes, leveraging the eXtreme Gradient Boosting (XGBoost) model optimized with Bayesian techniques. To maximize the utility of available data, 654 datasets with missing values were imputed and augmented, forming a robust dataset for the training and validation of the Bayesian optimized XGBoost (BO-XGBoost) model. Furthermore, the SHapley Additive explanations (SHAP) method was employed to elucidate the contribution of each input feature to the predictive outcomes. The results indicate that: (1) The constructed BO-XGBoost model exhibited exceptionally high predictive accuracy on the test set, with a root mean square error (RMSE) of 7.5603, mean absolute error (MAE) of 3.2940, mean absolute percentage error (MAPE) of 4.51%, and coefficient of determination (R2) of 0.9755; (2) Compared to the predictive performance of support vector mechine (SVR), decision tree (DT), and random forest (RF) models, the BO-XGBoost model demonstrates the highest R2 values and the smallest prediction error; (3) The input feature importance yielded by SHAP is groundwater level (h) > water-producing characteristics (W) > tunnel burial depth (H) > rock mass quality index (RQD). The proposed BO-XGBoost model exhibited exceptionally high predictive accuracy on the tunnel water inrush volume prediction dataset, thereby aiding managers in making informed decisions to mitigate water inrush risks and ensuring the safe and efficient advancement of tunnel projects.",,CE,Geotechnical,tunnel water inflow; XGBoost; bayesian optimization; data augmentation; model interpretation,Frontiers in Earth Science
Journal Article,"R. Gitzel, M. W. Hoffmann, P. Z. Heiden, A. Skolik, S. Kaltenpoth, O. Müller, C. Kanak, K. Kandiah, M. F. Stroh, W. Boos, M. Zajadatz, M. Suriyah, T. Leibfried, D. S. Singhal, M. Bürger, D. Hunting, A. Rehmer, A. Boyaci",2024,"Toward Cognitive Assistance and Prognosis Systems in Power Distribution Grids—Open Issues, Suitable Technologies, and Implementation Concepts",Institute of Electrical and Electronics Engineers Inc.,12,,107927–107943,"In recent times, both geopolitical challenges and the need to counteract climate change have led to an increase in generated renewable energy as well as an increased demand for clean electrical energy. The resulting variability of electricity production and demand as well as an overall demand increase, put additional stress on the existing grid infrastructure. This leads to strongly increased maintenance demands for distribution system operators (DSOs). Today, condition monitoring is used to address these challenges. Researchers have already explored solutions for monitoring critical assets like switchgear and circuit breakers. However, with a shrinking knowledgeable technical workforce and increasing maintenance requirements, mere monitoring is insufficient. Already today, DSOs ask for actionable recommendations, optimization strategies, and prioritization methods to manage the growing task backlog effectively. In this paper we propose a vision of a grid-level cognitive assistance system that translates the outcome of diagnosis and prognosis systems into actionable work tasks for the grid operator. The solution is highly interdisciplinary and based on empirical studies of real-world requirements. We also describe the related work relevant to the multi-disciplinary aspects and summarize the research gaps that need to be closed over the next years.",,CE,Energy,Renewable energy; electrical grid infrastructure; maintenance planning; condition monitoring; AI; explainable AI; large language models; cognitive assistance system; service engineering,IEEE Access
Journal Article,"Y., Safari Kanani-Sadat, A., Nasseri, M., Homayouni, S.",2025,A novel explainable stacking ensemble model for estimating design floods: A data-driven approach for ungauged regions,Elsevier Ltd,66,,,"Accurate flood estimation is crucial for the sustainable design of resilient urban infrastructures, especially in ungauged regions where data scarcity limits traditional hydrological analyses. This study introduces an innovative explainable stacking ensemble learning framework to predict design floods with return periods of 5, 10, 25, 50, 100 and 500 years. The model integrates diverse machine learning techniques to capture the nonlinear and complex relationships between flood magnitudes and catchment characteristics. In the first level of the ensemble, four base models, Multilayer Perceptron (MLP), Support Vector Regression (SVR), Random Forest (RF), and Extremely Randomized Trees (ERT), are trained. A linear Elastic-Net regression model at the second level combines the results of these base models for the final flood estimates. Performance analysis shows that MLP performs best for lower return periods, while ERT excels for higher ones. Overall, the stacking model outperforms individual models across all return periods, demonstrating its robustness. To enhance model interpretability, Shapley Additive Explanations (SHAP) values are used to reveal the contribution of catchment characteristics to flood behavior, identifying key flood drivers. The results show how feature importance changes with return periods, supporting context-specific decision-making. This research pioneers the use of explainable AI (XAI) in stacking ensemble models for flood estimation, offering a transparent and actionable methodology for urban flood risk management. By integrating advanced modeling, explainability, and remote sensing data through Google Earth Engine, the proposed framework provides a robust decision-support tool for practitioners and policymakers, enhancing the resilience of urban infrastructure to flood hazards. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004367556&doi=10.1016%2Fj.aei.2025.103429&partnerID=40&md5=4e0356828adf2e57c69d510153767587,CE,Water,Explainable artificial intelligence; Stacking ensemble learning; Urban resilience; Flood risk management; Regional flood frequency analysis; Google earth engine,Advanced Engineering Informatics
Journal Article,"K. S., Koo Kang, C., Ryu, H. G.",2022,An interpretable machine learning approach for evaluating the feature importance affecting lost workdays at construction sites,Elsevier Ltd,53,,,"This study investigates the intensity of accidents by considering the lost workdays based on real data and derived the feature importance quantitatively. Occupational injuries lead to lost workdays for construction workers. The term “lost workdays” refers to a period in which workers cannot perform regular jobs because of severe physical or mental damage from occupational injuries. Many studies focusing on type analysis, accident prediction, and risk management of construction accidents, combined with the intensity and frequency of accidents, have been conducted to prevent and minimize accidents. However, the existing method for measuring the intensity of occupational accidents/injuries is a qualitative method that reflects experts’ opinions, mainly using brainstorming, the Delphi method, and the analytic hierarchy process. Thus, we propose a framework that combines traditional analysis and interpretable machine learning approaches. The random forest model was trained with 11,223 injured worker samples, feature importance and frequency analysis with the chi-squared test, and local interpretable model-agnostic data. According to the results of this study, optimal safety management is possible if appropriate resources are allocated. If the risk of accidents is measured with objective and quantitative data, the prevention and analysis of accidents can be more reliable, and resources can be more efficiently allocated. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129563876&doi=10.1016%2Fj.jobe.2022.104534&partnerID=40&md5=4eb18bc87b4c282f111527995f3e97e3,CEM,Safety,Construction safety; Lost workdays; Interpretable machine learning; Data analysis,Journal of Building Engineering
Journal Article,"M., Terzi Karli, F.",2025,Modelling the energy footprint of urban form in Istanbul: A big data and machine learning approach,Elsevier B.V.,333,,,"There are numerous studies examining the impact of morphological characteristics on the energy performance of buildings. However, there are a limited number of studies that investigate energy consumption in relation to urban forms across metropolitan areas using big data and AI techniques. This study aims to evaluate the energy performance of different urban forms based on their morphological characteristics in Istanbul, employing AI driven methods. In the study, the energy performance certificates of 36,901 buildings, selected through a data preprocessing process, were used to accurately represent different residential typologies. Among eight different machine learning techniques that tested both linear and nonlinear relationships among variables, the most successful results were obtained from techniques based on decision trees that capture nonlinear relationships. Among these techniques, the Random Forest model demonstrated the highest predictive performance, achieving an R-squared score of 77%. The influence of variables was further analyzed using explainable artificial intelligence (XAI), revealing that total construction area and building compactness were the most influential variables in model predictions. This approach also enabled the identification of critical threshold values of these variables affecting energy consumption. Additionally, geometric variables were employed as predictor variables in AI models to estimate energy consumption for 713,648 buildings lacking available data. In another phase of the study, clustering analysis was performed to classify 51,862 building blocks into seven distinct urban form types. The estimated energy consumption data of buildings were integrated with the classified building block data to systematically examine the energy performance of different urban forms. As a result, among the seven different urban forms, 'open skyscraper' was identified as the most efficient in terms of energy performance, while 'irregular compact low-rise' was determined to be the least efficient. The study contributes to supporting research in energy-efficient urban planning through the application of AI techniques in both analysis and modelling.",,CE,Energy,Urban morphology; Urban form; Energy; Machine learning; Explainable artificial intelligence,Energy and Buildings
Journal Article,"A. M., Tariq Khan, M. A., Ur Rehman, S. K. U., Saeed, T., Alqahtani, F. K., Sherif, M.",2024,BIM Integration with XAI Using LIME and MOO for Automated Green Building Energy Performance Analysis,Multidisciplinary Digital Publishing Institute (MDPI),17,13,,"Achieving sustainable green building design is essential to reducing our environmental impact and enhancing energy efficiency. Traditional methods often depend heavily on expert knowledge and subjective decisions, posing significant challenges. This research addresses these issues by introducing an innovative framework that integrates building information modeling (BIM), explainable artificial intelligence (AI), and multi-objective optimization. The framework includes three main components: data generation through DesignBuilder simulation, a BO-LGBM (Bayesian optimization–LightGBM) predictive model with LIME (Local Interpretable Model-agnostic Explanations) for energy prediction and interpretation, and the multi-objective optimization technique AGE-MOEA to address uncertainties. A case study demonstrates the framework’s effectiveness, with the BO-LGBM model achieving high prediction accuracy (R-squared > 93.4%, MAPE < 2.13%) and LIME identifying significant HVAC system features. The AGE-MOEA optimization resulted in a 13.43% improvement in energy consumption, CO<inf>2</inf> emissions, and thermal comfort, with an additional 4.0% optimization gain when incorporating uncertainties. This study enhances the transparency of machine learning predictions and efficiently identifies optimal passive and active design solutions, contributing significantly to sustainable construction practices. Future research should focus on validating its real-world applicability, assessing its generalizability across various building types, and integrating generative design capabilities for automated optimization. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198366142&doi=10.3390%2Fen17133295&partnerID=40&md5=1c23803c6f7fc17e3849196ee467ad7d,CEM,"Digital Construction (BIM, AI, Sensors, Robotics)",sustainable architecture; predictive modeling; energy optimization; building information modeling (BIM); explainable AI,Energies
Journal Article,"D., Akram Khan, W., Ullah, S.",2025,Enhancing landslide susceptibility predictions with XGBoost and SHAP: a data-driven explainable AI method,,40,1,,"Landslide susceptibility mapping is essential for disaster risk management, especially in geologically sensitive regions like the Himalayas, where steep slopes, heavy rainfall, and human activities intensify risks. This study integrates eXtreme Gradient Boosting (XGBoost) with SHapley Additive exPlanations (SHAP) to enhance both predictive accuracy and interpretability in Rudraprayag and Tehri Garhwal, Uttarakhand, India. Using high-resolution geospatial datasets, 20 conditioning factors were analyzed, with feature selection refined through Variable Inflation Factor (VIF) and Recursive Feature Elimination (RFE). The XGBoost model achieved 92.87% accuracy and an AUC of 0.96, outperforming traditional methods. SHAP analysis identified Distance from Roads, Elevation, Rainfall, and Slope as key influencing factors. The susceptibility map categorized 13.10% of the area as high or very high susceptibility, highlighting the need for targeted mitigation. This study underscores the role of explainable AI in landslide risk assessment, providing actionable insights for disaster preparedness and sustainable development.",,CE,Geotechnical,Landslide susceptibility mapping; machine learning; XGBoost algorithm; SHAP (SHapley additive explanations); disaster risk management,Geocarto International
Journal Article,"N. M., Ma Khan, L. Q., Bin Inqiad, W., Khan, M. S., Iqbal, I., Emad, M. Z., Alarifi, S. S.",2025,Interpretable machine learning approaches to assess the compressive strength of metakaolin blended sustainable cement mortar,Taylor & Francis,15,1,,"The use of naturally available materials such as metakaolin (MK) can greatly reduce the utilization of emission intensive materials like cement in the construction sector. This would reduce the stress on depleting natural resources and foster a sustainable construction industry. However, the laboratory determination of 28 day compressive strength (C-S) of MK-based mortar is associated with several time and resource constraints. Thus, this study was conducted to develop reliable empirical prediction models to assess CS of MK-based mortar from its mixture proportion using machine learning algorithms like gene expression programming (GEP), extreme gradient boosting (XGB), multi expression programming (MEP), bagging regressor (BR), and AdaBoost etc. A comprehensive dataset compiled from published literature having five input parameters including water-to-binder ratio, mortar age, and maximum aggregate diameter etc. was used for this purpose. The developed models were validated by means of error metrics, residual assessment, and external validation checks which revealed that XGB is the most accurate algorithm having testing \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\:{\text{R}}<^>{2}$$\end{document} of 0.998 followed by BR having \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\:{\text{R}}<^>{2}$$\end{document} values equal to 0.946 while MEP had the lowest testing \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\:{\text{R}}<^>{2}$$\end{document} of 0.893. However, MEP and GEP algorithms expressed their output in the form of empirical equations which other black-box algorithms couldn't produce. Moreover, interpretable machine learning approaches including shapely additive explanatory analysis (SHAP), individual conditional expectation (ICE), and partial dependence plots (PDP) were conducted on the XGB model which highlighted that water-to-binder ratio and sample age are some of the most significant variables to predict the C-S of MK-based cement mortars. Finally, a graphical user interface (GUI) was made for implementation of findings of this study in the civil engineering industry.",,CE,Materials,Cement mortar; Metakaolin; Interpretable machine learning; Gene expression programming; Compressive strength,Geocarto International
Journal Article,"K., Ikeda Kilic, H., Narihiro, O., Adachi, T., Kawamura, Y.",2024,A soft ground micro TBM's specific energy prediction using an eXplainable neural network through Shapley additive explanation and Optuna,Springer Nature,83,5,,"In tunnel construction, efficiently predicting the energy usage of tunnel boring machines (TBMs) is critical for optimizing operations and reducing costs. This research proposes a novel method for predicting the specific energy of micro slurry tunnel boring machines (MSTBMs) using an explainable neural network (xNN) that leverages operator-monitored data. The xNN model provides transparency and interpretability by integrating the Shapley additive explanation (SHAP) technique, enabling tunneling engineers and operators to gain valuable insights into the prediction process. Extensive data from MSTBM umbrella pipe support excavation are the foundation for training, testing, and unseen data in the xNN model. The specific energy formula derived from the operational parameters of the MSTBM defines the dependent variable for the xNN model. The test dataset evaluates the model's performance with an R-2 of 98.7%, an MSE of 2.40, and an MAE of 0.003, demonstrating its accuracy and reliability. Ten percent of the dataset was reserved as unseen data to assess the model's generalization capabilities. Upon evaluation, the model achieved an R 2 value of 89%, an MAE of 0.01, and a root mean squared error (RMSE) of 0.01. The xNN empowers operators to optimize operational parameters and promote more efficient and sustainable tunneling practices by identifying influential factors affecting energy consumption through its interpretable nature. This research has significant implications for the future of underground construction, paving the way for improved resource management.",,CE,Geotechnical,Micro slurry TBM; Soft ground tunneling; Specific energy; Operational parameters; Explainable AI; Neural networks,Bulletin of Engineering Geology and the Environment
Journal Article,"G., Lim Kim, S.",2022,Development of an Interpretable Maritime Accident Prediction System Using Machine Learning Techniques,Institute of Electrical and Electronics Engineers Inc.,10,,41313–41329,"Every year, maritime accidents cause severe damages not only to humans but also to maritime instruments like vessels. The authors of this work therefore propose a machine learning-based maritime accident prediction system that can be used to prevent maritime accidents from happening by predicting and interpreting the accidents. This work overcomes the limitations of the existing works that lack practicability in the sense that the ex-post analyses are conducted to suggest accident prevention strategies but maritime accidents are not analyzed holistically. Using extensive literature reviews and expert interviews, a large number of risk factors associated with maritime accidents are identified, and related data are collected and utilized in the work. Throughout variable selection, data retrieval, hot-spot identification, and the maritime accident prediction model construction process, various machine learning algorithms are exploited in order to construct an organized system. In addition, interpretations for the resulting accident predictions are given using interpretable machine learning algorithms so as to provide explainable results to users. Finally, the proposed system is evaluated using a SERVQUAL model and proves its effectiveness in real-world applications. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129213523&doi=10.1109%2FACCESS.2022.3168302&partnerID=40&md5=9c46d9f8f45d43f23eefee248bdb75de,CE,Transportation,Maritime accident; ocean engineering; accident prediction; interpretable machine learning,IEEE Access
Journal Article,"Y., Cho Kim, Y., Heo, H. K., Lim, L.",2024,Estimating casualties from urban fires: A focus on building and urban environment information,Elsevier Ltd,115,,,"This study developed two prediction models for urban fire occurrence and related casualties via a fire accident dataset from Seoul, South Korea, from 2017 to 2021. Our models exhibit improved predictive performance by incorporating built environment features, such as building characteristics and the urban context, alongside weather and demographic data. This approach showed improved predictive performance suitable for public health implementation. Compared with the weather- and demographic-only models, our models had an 18.1 % greater fire occurrence prediction accuracy and a 10.4 % greater casualty prediction accuracy. Major variables affecting fire occurrence include building characteristics, e.g., the floor area ratio (FAR), building age, and commercial building number. Important features affecting casualty occurrence include demographic aspects, e.g., income level and weather, and network-based features, e.g., road connectivity and fire station proximity. These findings suggest that fire prevention strategies and fire casualty prevention strategies may need to differ. Furthermore, we identify high-risk zones by conducting spatial analysis and fire risk and casualty prediction on all buildings by applying our models to Seoul's Gangnam District. These contributions can promote safe and healthy urban environments by improving fire risk prediction accuracy and providing important insights into urban planning for appropriate urban fire accident response and prevention. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205264174&doi=10.1016%2Fj.scs.2024.105839&partnerID=40&md5=7e37c847e08a9974ee315eaabde9a508,CE,Environmental,Urban safety and health; Fire casualty; Built environment; Nonlinear associations; Interpretable machine learning; Safe urban planning,Sustainable Cities and Society
Journal Article,"D., Park Ko, S.",2024,"Investigating the Correlation between Air Pollution and Housing Prices in Seoul, South Korea: Application of Explainable Artificial Intelligence in Random Forest Machine Learning",Multidisciplinary Digital Publishing Institute (MDPI),16,11,,"South Korea’s Particulate Matter (PM) concentration is among the highest among Organization for Economic Cooperation and Development (OECD) member countries. However, many studies in South Korea primarily focus on housing characteristics and the physical built environment when estimating apartment prices, often neglecting environmental factors. This study investigated factors influencing apartment prices using transaction data for Seoul apartments provided by the Ministry of Land, Infrastructure, and Transport (MOLIT) in 2019. For this purpose, the study compared and analyzed a traditional hedonic price model with a machine learning-based random forest model. The main findings are as follows: First, the evaluation results of the traditional hedonic price model and the machine learning-based random forest model indicated that the random forest model was found to be more suitable for predicting apartment prices. Second, an importance analysis using Explainable Artificial Intelligence (XAI) showed that PM is more important in determining apartment prices than access to education and bus stops, which were considered in this study. Finally, the study found that areas with higher concentrations of PM tend to have higher apartment prices. Therefore, when proposing policies to stabilize apartment prices, it is essential to consider environmental factors. Furthermore, it is necessary to devise measures such as assigning PM labels to apartments during the home purchasing process, enabling buyers to consider PM and obtain relevant information accordingly. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195866740&doi=10.3390%2Fsu16114453&partnerID=40&md5=a4f88d346740b38e3ec0194f0dcafb87,CE,Environmental,housing prices; particulate matter; machine learning; random forest; explainable artificial intelligence,Sustainability
Journal Article,"J., Lee Ko, D.",2025,Graph neural networks for classification and error detection in 2D architectural detail drawings,Elsevier B.V.,170,,,"The assessment and classification of architectural sectional drawings is critical in the architecture, engineering, and construction (AEC) field, where the accurate representation of complex structures and the extraction of meaningful patterns are key challenges. This paper established a framework for standardizing different forms of architectural drawings into a consistent graph format, and evaluated different Graph Neural Networks (GNNs) architectures, pooling methods, node features, and masking techniques. This paper demonstrates that GNNs can be practically applied in the design and review process, particularly for categorizing details and detecting errors in architectural drawings. The potential for visual explanations of model decisions using Explainable AI (XAI) is also explored to enhance the reliability and user understanding of AI models in architecture. This paper highlights the potential of GNNs in architectural data analysis and outlines the challenges and future directions for broader application in the AEC field. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212542153&doi=10.1016%2Fj.autcon.2024.105936&partnerID=40&md5=7af130d50381d84a654f1aed2c3303b7,CEM,"Digital Construction (BIM, AI, Sensors, Robotics)",Architectural detail drawings; Graph neural networks (GNN); Drawing classification; Error detection; Explainable AI (XAI),Automation in Construction
Journal Article,K. Koc,2023,Role of National Conditions in Occupational Fatal Accidents in the Construction Industry Using Interpretable Machine Learning Approach,American Society of Civil Engineers (ASCE),39,6,,"Current national occupational safety and health (OSH) initiatives follow reactive approaches, i.e., if it breaks, fix it. Existing accounts, however, failed to improve national OSH performances substantially, which imposes the need for an in-depth and proactive (fix it so it will not break) investigation of national occupational fatality risks. Despite many studies examining the fatality risk of workers based on project-, company-, and/or behavior-related factors, the role of national conditions on the countrywide fatality risk of workers has not been explored. The present research leverages the national statistics of Turkey to examine their influence on construction workers' fatality risk through a machine learning-based prediction model. Several widely used machine learning methods were adopted for determining whether the upcoming month poses a significant fatality risk for construction workers or not based on national statistics of the previous month. According to analysis results, the gradient boosting decision tree algorithm yielded the highest prediction performance in terms of f1-score. The recently developed game theory-based Shapley Additive Explanations (SHAP) algorithm was used to identify whether and how national conditions affect countrywide fatality risk of construction workers. Findings illustrate that the share of the construction sector in employment, market demand, and labor shortage are the most significant national factors in determining the fatality risk. SHAP summary and SHAP dependence plots are further presented to provide decision makers with a clearer understanding of hidden relationships between fatality risk and national conditions. In addition, a framework that can be practically used by policy makers and governmental authorities is developed to help minimize national occupational fatality risk. Overall, predicting national fatality risk in the industry and identifying the national precursors of occupational fatalities contribute to the development of macrolevel safety improvements based on country-specific conditions. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167809180&doi=10.1061%2FJMENEA.MEENG-5516&partnerID=40&md5=dc255676d0eb6859a32ea7bac7b3dbf8,CEM,Safety,Occupational health and safety (OHS); Fatal occupational accidents; National conditions; Macrolevel accident analysis; Artificial intelligence,Journal of Management in Engineering
Journal Article,K. Koc,2023,Role of Shapley Additive Explanations and Resampling Algorithms for Contract Failure Prediction of Public-Private Partnership Projects,American Society of Civil Engineers (ASCE),39,5,,"A public-private partnership (PPP) is a common procurement model implemented worldwide as a catalyst for economic growth and improved public infrastructure. However, due to their inherent characteristics, the risk of failure in some PPP projects is high, causing heavy losses to both entities. Despite distinctive progress being made in PPP projects to reduce their failure probability, there is no proper and effective framework to predict PPP project failure in advance in either developing or in developed countries. The present study aims to develop a machine learning (ML) model to predict the failure of PPP projects to prosper in adverse conditions. This research addresses two critical issues, i.e., class imbalance and interpretability of ML models, that differentiate the current study from data-driven studies to date. First, existing studies usually focused on comparing and selecting the most adequate ML methods, but this study distinctively compared the performances of nine data resampling algorithms. Besides, in order to enhance the interpretability and visibility of the proposed model, a game theory-based feature investigation algorithm, Shapley additive explanations (SHAP), was used to identify not only the most significant features, but also the conditions of the features that cause failure or success in PPP projects. The findings illustrate that the proposed model yielded the highest prediction performance once the data set was resampled with the support vector machine-synthetic minority oversampling technique (SVM-SMOTE). SHAP analysis further shows that unsolicited proposals, domestic credit to the private sector, and project type/subtype have significant impacts on the prediction rationale. Overall, this study contributes to theory through incorporating resampling methods and SHAP algorithm into ML models as well as to practice with an advanced and reliable model to predict the status of PPP projects. The data-driven model and findings are expected to respond to current policy and industry needs by proposing a robust decision-making input for detecting risky PPP projects, allocating resources more effectively based on the most critical failure factors, and promoting the transparency of PPP project outcomes. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164259067&doi=10.1061%2FJMENEA.MEENG-5492&partnerID=40&md5=7ac5853dc4928ed27d31a428a481ac1a,CEM,Contracting / Legal,Public–private partnership (PPP); Interpretable machine learning; Data resampling; Project failure; Shapley additive explanations,Journal of Management in Engineering
Journal Article,K. Koc,2023,Determining the Short-term Susceptibility of Construction Workers to Occupational Accidents Using Stochastic Gradient Boosting,Golden Light Publishing,6,1,1–15,"Employees working on construction projects have a higher risk of experiencing occupational accidents compared to workers in other sectors. Newly employed workers might face an occupational accident shortly after they start working due to not being able to notice risky environmental conditions. Despite existing studies in construction safety literature focusing on several output variables such as fatality, accident type or accident severity predictions, no studies have examined the short-term susceptibility of construction workers to occupational injuries. This study aims to develop a model to predict construction workers' susceptibility to short-term occupational accidents using interpretable machine learning (ML) methods. Hence, the primary research objective is to identify construction workers who have high probability of experiencing an occupational accident shortly after their employment. In this respect, a national dataset of occupational accidents encountered in the construction industry in Turkey was collected and subjected to various pre-processing elements (data cleaning, data scaling, and data resampling) to prepare the data for prediction. At the processing step, Stochastic Gradient Boosting (SGB) algorithm was applied for the classification purpose. In the next step, Shapley Additive Explanations (SHAP) was used as an interpretable artificial intelligence algorithm to explain how, to what extent, and in which direction the input variables affect the prediction scheme, which is another distinguishing feature of the present study compared to past studies in the subject matter. Results show that the proposed SGB model is a powerful detector for the classification problem and salary of workers, past accident in the company, and number of workers in the company were the most influencing factors. Overall, this study contributes to practice by improving the safety conditions of the newly employed workers as well as minimizing their accident probability through intensified safety training. Given that contemporary safety management applications demand a new set of data-driven inputs, proposed model is expected to help industry professionals and safety managers apply more robust safety risk mitigation and/or prevention measures.",,CEM,Safety,Construction safety management; Explainable artificial intelligence; Explainable machine learning; Stochastic gradient boosting; Worker vulnerabilit,"Journal of Construction Engineering, Management & Innovation"
Journal Article,"K., Budayan Koc, C., Ekmekcioğlu, Ö, Tokdemir, O. B.",2024,Predicting Cost Impacts of Nonconformances in Construction Projects Using Interpretable Machine Learning,American Society of Civil Engineers (ASCE),150,1,,"Nonconformance (NCR) has long been a subject of research interest for its potential to extrapolate information leading to a more productive environment in construction projects. Despite a variety of traditional attempts, a systematic understanding of how machine learning (ML) approaches can contribute to proactively detecting the severity of NCRs remains limited. This study aims to develop a data-driven ML framework to predict the cost impacts of NCRs (high severity versus low severity) in construction projects. To accomplish this aim, the random forest (RF) algorithm reinforced with a metaheuristic hyperparameter-tuning strategy, namely the gravitational search algorithm (GSA), is adopted for the binary classification problem. Furthermore, this study incorporates the Shapley additive explanations (SHAP) ensuring transparent interpretations into the GSA-RF predictive framework to tackle the inherent black-box nature of the ML rationale. The results reveal that the proposed model detects the severity of NCRs in terms of their cost impact with an overall AUROC value of 0.776 for the preseparated and blinded testing set. This indicates that the proposed model can be used confidently for newly introduced datasets from real-life cases. In addition, the SHAP analysis results emphasized the role of season, inadequate application procedure, and NCR type in detecting the severity of NCRs. Overall, this research not only makes an important contribution through its novel data-driven approaches but also provides insights for project managers concerning productivity improvements in the sector. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175444948&doi=10.1061%2FJCEMD4.COENG-13857&partnerID=40&md5=81b4393b78eb38008c5f30ffb311477d,CEM,Project Management,Nonconformance (NCR); Quality failures; Explainable artificial intelligence; Cost of quality; Tree-based ensemble model,Journal of Construction Engineering and Management
Journal Article,"K., Ekmekcioğlu Koc, Ö, Gürgün, A. P.",2023,Developing a National Data-Driven Construction Safety Management Framework with Interpretable Fatal Accident Prediction,American Society of Civil Engineers (ASCE),149,4,,"Occupational accidents are frequent in the construction industry, containing significant risks in the working environment. Therefore, early designation, taking preventive actions, and developing a proactive safety risk management plan are of paramount significance in managing safety issues in the construction industry. This study aims to develop a national data-driven safety management framework based on accident outcome prediction, which helps anatomize precursors of fatalities and thereby minimizing fatal accidents on construction sites. A national data set comprising 338,173 occupational accidents recorded in the construction industry across Turkey was used to develop a data-driven model. The random forest algorithm coupled with particle swarm optimization was used for the prediction and the interpretability of the proposed model was augmented through the game theory-based Shapley additive explanations (SHAP) approach. The findings showed that the proposed algorithm achieved satisfactory model performances for detecting construction workers who might face a fatality risk. The SHAP analysis results indicated that both company (such as number of past accidents and workers in the company) and worker-related (such as age, daily wage, experience, shift, and past accident of the workers) attributes were influential in identifying fatalities by detecting which workers might face fatal accidents under which conditions. A construction safety management plan was developed based on the analysis results, which can be used on construction sites to detect workers/conditions that are most susceptible to fatalities. The findings of the present research are expected to contribute to orchestrating effective safety management practices in construction sites by characterizing the root causes of severe accidents. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147141682&doi=10.1061%2FJCEMD4.COENG-12848&partnerID=40&md5=fae2646a7478ceb168c5ca75d0597387,CEM,Safety,Construction safety management; Occupational accidents; Occupational health and safety (OHS); Interpretable artificial intelligence; Machine learning,Journal of Construction Engineering and Management
Journal Article,"X., Li Kong, Z., Zhang, Y., Das, S.",2022,Bridge Deck Deterioration: Reasons and Patterns,SAGE Publications Ltd,2676,7,570–584,"The deck condition of bridges is one of the most important factors impacting the connectivity and efficiency of transportation networks. Bridges with quickly deteriorating deck conditions are a huge financial burden for transportation agencies and can downgrade the efficiency of the whole transportation network. This study utilizes an interpretable machine learning framework, Shapley additive explanation (SHAP), to investigate the associations between various factors, such as wearing surface, deck structure, and so forth, and bridges with quickly deteriorating deck conditions nationwide. An XGBoost model is trained to perform the binary classification task on a heavily imbalanced dataset and classify relatively young bridges (less than 20 years old) with poor/fair deck conditions and relatively old bridges (30–40 years old) with good deck conditions in the National Bridge Inventory (NBI) database. The accuracy of the predictive model is 0.91, and the AUC score is about 0.83. After applying this well-performed predictive model on the interpretable machine learning framework, the results revealed that without wearing surface, corrugated steel deck structure, wide bridge structure, and long span are highly associated with bridges with quickly deteriorating decks. The results also show that bridges with a relatively low average daily traffic (ADT) or truck percentage of ADT are in a dilemma zone, where the overall traffic or truck volume of the bridge is not low enough to prevent fast deterioration, but not high enough for eligibility for the funding required for more durable materials during construction or appropriate maintenance. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135557633&doi=10.1177%2F03611981221080140&partnerID=40&md5=ec7edcd82b3f03ba6a8c886138739252,CE,Structural,data and data science; artificial intelligence and advanced computing applications; artificial intelligence; data analytics; machine learning (artificial intelligence); pattern recognition; infrastructure; construction; bridges and structures; infrastructure management and system preservation; bridge and structures management; bridge condition data/assessment; bridge data QC/QA,Transportation Research Record
Journal Article,"S., Nyunn Kookalani, S., Xiang, S.",2022,Form-finding of lifting self-forming GFRP elastic gridshells based on machine learning interpretability methods,Techno-Press,84,5,605–618,"Glass fiber reinforced polymer (GFRP) elastic gridshells consist of long continuous GFRP tubes that form elastic deformations. In this paper, a method for the form-finding of gridshell structures is presented based on the interpretable machine learning (ML) approaches. A comparative study is conducted on several ML algorithms, including support vector regression (SVR), K-nearest neighbors (KNN), decision tree (DT), random forest (RF), AdaBoost, XGBoost, category boosting (CatBoost), and light gradient boosting machine (LightGBM). A numerical example is presented using a standard double-hump gridshell considering two characteristics of deformation as objective functions. The combination of the grid search approach and k-fold cross-validation (CV) is implemented for fine-tuning the parameters of ML models. The results of the comparative study indicate that the LightGBM model presents the highest prediction accuracy. Finally, interpretable ML approaches, including Shapely additive explanations (SHAP), partial dependence plot (PDP), and accumulated local effects (ALE), are applied to explain the predictions of the ML model since it is essential to understand the effect of various values of input parameters on objective functions. As a result of interpretability approaches, an optimum gridshell structure is obtained and new opportunities are verified for form-finding investigation of GFRP elastic gridshells during lifting construction. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144085052&doi=10.12989%2Fsem.2022.84.5.605&partnerID=40&md5=1d2a0c5f82554396c8c00674ff914ab5,CE,Structural,form-finding; gridshell structure; interpretability methods; machine learning; regression,Structural Engineering and Mechanics
Journal Article,"N., Thor Kroell, E., Göbbels, L., Schönfelder, P., Chen, X.",2025,Deep learning-based prediction of particle size distributions in construction and demolition waste recycling using convolutional neural networks on 3D laser triangulation data,Elsevier Ltd,466,,,"To enhance sustainability in the construction industry, substituting primary with recycled aggregates from construction and demolition waste (CDW) is essential. However, the necessary quality assessment of recycled aggregates, especially their particle size distribution (PSD), through sampling and manual sieving is time-consuming and prone to sampling errors due to the heterogeneity of CDW waste and fluctuating material flows combined with small sampling and manual sieving volumes. Here, we introduce a novel inline monitoring approach using convolutional neural networks (CNNs) to estimate PSDs from inline 3D laser triangulation (3DLT) sensor data of both primary and recycled aggregate particles. Analyzing 174,220 particles across nine size classes with a dual camera 3DLT sensor, a customized VGG-inspired CNN model outperformed other architectures, achieving accuracies of 80.8 % and 75.0 % for primary and recycled aggregates at particle level, respectively. Most errors were near-miss classifications, yielding a mean absolute error of 1.0 vol% in PSD predictions at material flow level. Explainable artificial intelligence techniques confirmed the reliance of CNNs on particle contours for robust classification. Our findings offer a pathway for inline PSD monitoring in processing of both primary and recycled aggregates, contributing to a more quality-orientated, circular, and sustainable construction industry. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217052742&doi=10.1016%2Fj.conbuildmat.2025.140214&partnerID=40&md5=5b7cc35b686edfffb6b68759d3868e05,CE,Materials,Machine learning; Deep learning; Inline sensor-based material flow characterization; Circular economy; Sieve analysis; Recycled concrete aggregates; Explainable artificial intelligence (XAI),Construction and Building Materials
Journal Article,"R., Boule Larocque, A. M., Cappart, Q.",2025,Estimating Road Construction Costs with Explainable Machine Learning,Institute for Operations Research and the Management Sciences,55,2,,"A preliminary estimation of construction costs is a crucial operation of any project related to civil engineering. An accurate estimation ensures a proper management of the available funds and helps the project managers in their decision-making processes. For instance, it is common that specific subtasks of the project are delegated to private subcontractors. Through a call for tenders, each eventual subcontractor has the opportunity to propose a bid with a price for supplying the service. Because the call is generally public, a competition may arise between subcontractors. This impacts the price proposed by the competitors to get the contract. In order to select a subcontractor, the project manager needs to have an accurate idea of a reasonable price for the subtask given. A price higher than expected is undesirable, but a price significantly lower than expected may also result in poor quality of service. The project manager must also be able to explain to stakeholders why a price is suited and justify why a specific subcontractor has been selected. Providing an estimation that is both accurate and transparent is a hard problem for the project manager. A growing trend is to leverage machine learning for this estimation, but designing a model that is both accurate and explainable is still a challenge. Another difficulty is that an approach that is accurate for estimating the cost of a subtask may not be efficient for another one. Based on this context, this paper introduces a framework for estimating construction costs while tackling both challenges. It is based on six machine learning models and on Shapley additive explanations. This project was commissioned by the Ministry of Transport and Sustainable Mobility, a public agency responsible for transport infrastructure in Quebec, Canada. Experiments were carried out on real data, covering historical road construction costs of 11,646 contracts and eight subtasks from 2014 to 2021. Results show that the framework is able to surpass the accuracy of human estimations up to 31.56% while being able to adequately explain how the estimations have been obtained.",,CEM,Cost Estimation / Bidding,NaN,INFORMS Journal on Applied Analytics
Journal Article,"H., Hagras Leon-Garza, H., Pena-Rios, A., Conway, A., Owusu, G.",2022,A type-2 fuzzy system-based approach for image data fusion to create building information models,Elsevier B.V.,88,,115–125,"Building Information Modelling (BIM) is a standard digital process that fuses buildings information from different sources into a 3D model during their lifecycle. For new construction sites using BIM, it is possible to monitor the cost, schedule, and changes throughout the lifecycle; however, existing buildings do not have a BIM model. Manually creating the BIM models for existing buildings is a high-cost task, both in time and money, hence there is a need for extracting information from available paper-based documentation and fuse it into a BIM model. The struggle of facility management and utility companies to fully adopt a BIM process (due to their high volumes of paper-based documentation of existing buildings) has led to the research on creating these 3D BIM models from 2D floor plan images. This paper presents a novel processing pipeline to extract 2D digital information from floorplans, fusing it into a 3D BIM model. The work focuses on fusing the available information to create the structure of the building in BIM format, which is considered the essential step before looking on working with other sources of data. In this process, we introduce a type-2 fuzzy logic based Explainable Artificial Intelligence (XAI) approach for the semantic segmentation step. The approach consists of using the outputs of type-2 fuzzy logic systems to classify a pixel as wall or background, by using information around and from the pixel of interest as the inputs to the system. After the semantic segmentation step, the output of the type-2 fuzzy logic goes through a noise removal process and finally a transformation from 2D to 3D by assigning the corresponding BIM tag to each identified element. The proposed type-2 fuzzy logic semantic segmentation approach produced comparable results (97.3% mean Intersection over Union (IoU) performance metric value) to the opaque box model approach based on Convolutional Neural Network (CNN) (99.3% mean IoU performance metric value). However, the type-2 fuzzy XAI system benefits from being an augmentable and interpretable model, which means that human users can understand the decision process and modify the model using their expert knowledge. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135398140&doi=10.1016%2Fj.inffus.2022.07.007&partnerID=40&md5=67f01eba25a15d054aa2d34e0da252a0,CEM,"Digital Construction (BIM, AI, Sensors, Robotics)",Building Information Modelling; BIM; Semantic segmentation; Type-2 fuzzy logic systems; Convolutional neural network (CNN),Information Fusion
Journal Article,"M., Bahar Letif, R., Mezouar, N.",2025,The Use of Machine Learning Models and SHAP Interaction Values to Predict the Soil Swelling Index,Budapest University of Technology and Economics,69,1,239–250,"Predicting the soil swelling index (C<inf>S</inf>) is crucial for geotechnical engineer to ensure the stability of civil engineering conceptions. Recently, ML models has sparked great interest from researchers in predicting the soil swelling index. However, due to the black-box nature of ML models, their prediction capabilities are still uninterpretable. This study aims to predict the soil swelling index using ML algorithms and interpret predictions. First, it employs the prediction capability of the Gaussian process regression (GPR) algorithm and compares it to the artificial neural network (ANN) for prediction the soil swelling index. Second, the SHAP algorithm as one recent explainable artificial intelligence (XAI) models is applied to interpret the predictions of the complex GPR and ANN models. The compiled experimental database covers 362 clayey samples gathered from different sites located in Northern Algeria. The modeling involved six input features, including the liquid limit (LL), plastic limit (PL), plasticity index (PI), water content (ω<inf>n</inf>), dry density (γ<inf>d</inf>), and void ratio (e) to predict the soil swelling index. The findings based on statistical metrics showed a good performance of GPR with R2 = 0.78 and of ANN with R2 = 0.79. Comparative study based on Wilcoxon signed-rank test and sign test indicated that the ANN outperform better than GPR. Based on the interpretations obtained by SHAP algorithm, it is observed that the liquid limit (LL) and plastic limit (PL) are the two main input features that influence the C<inf>S</inf>, indicating, the higher content of LL and PL increase the model's output. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216657620&doi=10.3311%2FPPci.36880&partnerID=40&md5=349388488a9d5329b1a7d0e616c6d7dc,CE,Geotechnical,soil swelling index; machine learning; Gaussian process regression; artificial neural network; SHAP algorithm,Periodica Polytechnica Civil Engineering
Journal Article,"D., Adriaens Li, P.",2024,Deconstruction of ESG Impacts on US Corporate Bond Pricing: The Cost of Capital Benefits Across Industry Sectors,American Society of Civil Engineers (ASCE),40,1,,"The growing interest in the financial materiality of Environmental, Social, and Governance (ESG) ratings has prompted recent investigations into their risk pricing impact in the corporate bond market. The specific implications for the Architecture, Engineering, and Construction (AEC) industry have not been explored, as prior work has primarily focused on broad-based ESG integration. To fill this gap, our study employed an interpretable machine learning technique using a sample universe of U.S. corporate bonds spanning from 2010 to 2021 to estimate the impact of ESG ratings on corporate bond issuance spreads. The results revealed an average ESG benefit of 10 basis points across all sectors. However, it is important to note that the effects of ESG ratings on bond pricing demonstrate variation across sectors and individual ESG constituent ratings. Significantly, our findings show that social and governance ratings emerge as the primary drivers influencing bond issuance costs, whereas the impact of environmental scores is comparatively less significant. Within AEC-related industries, empirical data on the influence of ESG ratings indicate discounted pricing by the market is particularly channeled through environmental and governance scores. These findings emphasize the value-added impact of enhanced ESG performance on the cost of debt financing, presenting a financially material opportunity for operational and management decision-making. By adopting sustainable strategies to improve ESG performance, organizations in the AEC industry can potentially achieve lower costs of debt when issuing bonds to secure financing for construction projects. The managerial implications extend to policymakers, corporate managers, and creditors, as they all stand to benefit from the financial implications of ESG performance. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175197848&doi=10.1061%2FJMENEA.MEENG-5521&partnerID=40&md5=3f22dfe7f59159a31e10f96148e73879,CEM,Contracting / Legal,NaN,Journal of Management in Engineering
Journal Article,"H. W. X., Lyngdoh Li, G., Krishnan, N. M. A., Das, S.",2023,Machine learning guided design of microencapsulated phase change materials-incorporated concretes for enhanced freeze-thaw durability,Elsevier Ltd,140,,,"Prolonged exposure to freeze-thaw cycles (FTCs) induces adverse effects in concrete structures in terms of a significant reduction in strength and durability. This paper proposes a microencapsulated phase change material-incorporated concrete (MPCMC) layer to mitigate FTCs in the concrete bridge decks. Toward this, the present work implements finite element analysis (FEA)-based multiscale numerical simulation, which shows the effectiveness of the proposed MPCMC protective layer toward FTC reduction. Moreover, the multiscale framework is leveraged to generate a large dataset containing 8096 data points of FTCs considering a uniform variation of five design parameters, namely the thickness of the protective MPCMC layer, volume fraction, transition temperature, size, and the latent heat of the MPCM. Thereafter, the dataset is leveraged to develop an interpretable neural network (NN)-based predictive model as a computationally efficient alternative to the complex multiscale FEA-based design tools. The interpretable machine learning (ML) model provides a reliable and efficient continuous mapping between the design parameter space and the resultant number of FTCs. Moreover, it explains general engineering intuitions and physics. Besides, the interpretable ML characterizes various underlying interplays of the design parameters in dictating the resultant FTCs. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153575315&doi=10.1016%2Fj.cemconcomp.2023.105090&partnerID=40&md5=e313df137799e39ff554682e32799007,CE,Materials,Phase-change materials; Freeze-thaw cycles; Concrete structures; Machine learning; Shapley additive explanation; Neural networks,Cement and Concrete Composites
Journal Article,"J., Wang Li, X., Yang, X., Zhang, Q., Pan, H.",2024,"Analyzing Freeway Safety Influencing Factors Using the CatBoost Model and Interpretable Machine-Learning Framework, SHAP",SAGE Publications Ltd,2678,7,563–574,"Exploring and analyzing safety influencing factors can guide targeted traffic safety management. Traditional traffic safety models are aimed at specific data problems and making adjustments to the model structure, which lack focus on predictive ability and have limited information on the analysis of influencing factors. In recent years, machine-learning methods have opened new avenues in modeling that have higher prediction accuracy, can identify complex nonlinear relationships, and can overcome over- and under-dispersion and correlation. Machine-learning methods, however, pose the problem of limited interpretability. The interpretable machine-learning framework SHAP can be an effective solution, which can not only reflect the influence of features in each sample but also generate global interpretation. This study established gradient boosting models including the CatBoost and XGBoost models as traffic safety models, which were compared with a traditional NB regression model and a zero-inflated negative binomial regression model. SHAP was used to analyze several safety influencing factors, including geometric design features, traffic operation characteristics, time of day, and land use. Results confirmed that the CatBoost model has better prediction ability and is a more suitable traffic safety model than the traditional negative binomial regression model. Among the key findings were that ramp type is the most important factor in freeway crash frequency; curve presence has a great positive impact, while truck proportion has a great negative impact; and traffic volume is highly correlated with truck proportion. These findings can provide theoretical support for safety operation management and targeted improvement measures for freeways. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176950711&doi=10.1177%2F03611981231208903&partnerID=40&md5=2a6a63e8bb1b3e2f996977bc6d04aee9,CE,Transportation,operations; freeway analysis; freeway management; modeling,Transportation Research Record
Journal Article,"Q., Song Li, Z.",2022,Ensemble-Learning-Based Prediction of Steel Bridge Deck Defect Condition,MDPI,12,11,,"This study developed an ensemble-learning-based bridge deck defect condition prediction model to help bridge managers make more rational and informed steel bridge deck maintenance decisions. Using the latest data from the NBI database for 2021, this study first used ADASYN to solve imbalance problems in the data, then built six ensemble learning models (RandomForest, ExtraTree, AdaBoost, GBDT, XGBoost, and LightGBM) and used a grid search method to determine the hyperparameters of the models. The optimal model was finally analyzed using the interpretable machine learning framework, SHAP. The results show that the optimal model is XGBoost, with an accuracy of 0.9495, an AUC of 0.9026, and an F1-Score of 0.9740. The most important factor affecting the condition of steel bridge deck defects is the condition of the bridge’s superstructure. In contrast, the condition of the bridge substructure and the year of bridge construction are relatively minor factors. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131256265&doi=10.3390%2Fapp12115442&partnerID=40&md5=cc087c44f60695519cf628ba68bff6e6,CE,Structural,steel bridge deck; data mining; artificial intelligence; ensemble learning; defect condition prediction; feature importance,Applied Sciences
Journal Article,"T., Yang Li, J., Jiang, P., Abuhussain, M. A., Zaman, A., Fawad, M., Farooq, F.",2024,Forecasting the strength of nanocomposite concrete containing carbon nanotubes by interpretable machine learning approaches with graphical user interface,Elsevier Ltd,59,,,"The sustainable development of the construction industry necessitates the utilization of multipurpose Cement Composites (CC). Therefore, the integration of nanomaterials has the potential to provide CC that exhibits superior performance and possesses several functionalities. Hence, the use of Carbon Nanotubes (CNTs) inside the concrete cementitious sector holds significant potential for implementing effective solutions toward creating a sustainable ecosystem characterized by versatile attributes. Nevertheless, the prediction of the characteristics of these composites is a significant challenge owing to their complex composite structure and non-linear response. Furthermore, the process of designing and executing experimental trials on diverse samples and across various age groups is arduous, time-consuming, and financially burdensome. There is currently a dearth of a predictive model capable of estimating the compressive strength of concrete including nanoparticles. The utilization of such models is of significant importance in the project and study of Reinforced Concrete (RC) structures including nanoparticles. Three machine learning algorithms, including Gene Expression Programming (GEP), Gradient Boosting (GB), and Extreme Gradient Boosting (XGB), were utilized in this study to forecast the Compressive Strength (CS) of nanocomposites that incorporate CNTs. The evaluation of the models' reliability was conducted by the utilization of cross-validation with K-folding and subsequent statistical error analysis. According to the results of the coefficient of determination (R2), the XGB model achieved the highest R2 value (0.95), while the GB model and GEP model both earned R2 values of 0.94. Furthermore, the validation method for the models included the implementation of statistical analysis and k-fold cross-validation. Therefore, the XGB model exhibited much lower values for statistical metrics compared to the GEP and GB models. In addition, a GEP empirical equation and a Graphical User Interface (GUI) have been created for practical applications in predicting the strength of concrete. This streamlines the procedure and provides a valuable instrument for harnessing the model's potential in the field of civil engineering. Furthermore, the use of Shapley analysis is conducted to assess the predominant factors in concrete prediction. The findings of this research indicate that the curing time, type of cement, and water-to-cement ratio significantly influence the properties of CNT-based concrete composites. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181896426&doi=10.1016%2Fj.istruc.2023.105821&partnerID=40&md5=970e5629cbd19069a4f5df19b71cb3c4,CE,Materials,Nanocomposite; Machine learning; Contour graphs; Heat map; Statistical analysis; Shapley results,Structures
Journal Article,"Y., Liu Li, Y., Miao, Y., Liu, Z., Jiang, J., Lin, J.",2025,Development of heat-resistant tunnel muck-based shotcrete for geothermal environments: Dual drive of combining explainable machine learning and microstructure characterization,Elsevier Ltd,473,,,"The in-situ material shortage and high geothermal service conditions significantly hinder the utilization of shotcrete for the construction of tunnels in plateau regions. In this study, fully tunnel muck-based shotcrete (FTMS) was developed using stone powder and sand-gravel aggregates derived from tunnel muck. The effects of stone powder content, stone powder modification methods, and six performance-reinforcing materials on the high-geothermal resistance of FTMS were systematically investigated. The results demonstrated that a stone powder content of 20 % and a modification ratio of stone powder:silica fume:modifier = 82:15:3 achieved an optimal balance between stone powder utilization and high-geothermal performance. The improvement effect of calcium carbonate whiskers on the compressive strength of FTMS under high geothermal conditions was most significant, while the optimization effect of nano alumina sol on compressive strength and mass loss rate was most prominent. The influence of dual-doped performance-reinforcing materials on three properties was predicted using artificial neural networks, and interpretability analysis was conducted using SHAP. Finally, the optimal combination strategy was determined to be the co-doping of nano alumina sol and calcium carbonate whiskers. Microstructural analysis revealed that the addition of nanomaterials forms a dense calcium carbonate network in FTMS, while promoting the secondary hydration of calcium hydroxide, resulting in high-strength hydration products such as monocarboaluminate and ettringite. Additionally, a significant transformation of C-S-H to C-A-S-H phases was observed, accompanied by a 36.6 % reduction in harmful pores. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001124320&doi=10.1016%2Fj.conbuildmat.2025.140994&partnerID=40&md5=c93e649532aca51abd58f9b5cdcbfd18,CE,Materials,High geothermal environment; Fully tunnel muck-based shotcrete; Explainable machine learning; Geothermal resistance; Nanomaterials optimized microstructure,Construction and Building Materials
Journal Article,"P., Wu Lin, M., Zhang, L.",2023,Probabilistic safety risk assessment in large-diameter tunnel construction using an interactive and explainable tree-based pipeline optimization method,Elsevier B.V.,143,,,"Due to knowledge alienation, the application of artificial intelligence (AI) techniques in tunnel construction has been greatly stunted in recent years. In order to motivate the development of techniques in tunneling works, we propose an approach that combines automated machine learning, explainable AI, and building information modeling in this paper. Considering geotechnical uncertainties from soil properties and aleatoric uncertainties from the predicting model, this approach is used to evaluate the system risk during a large-diameter tunnel excavation taking into account the two sources of uncertainties simultaneously. The geotechnical uncertainties are evaluated by the confidence interval of the mean of samples in the Monte-Carlo simulation, and the aleatoric uncertainties are evaluated by computing the prediction interval of the machine learning models. It is found that the number of Monte-Carlo samples significantly affects the evaluation of the system's reliability. Based on our results, a generalized setup with 1000 samples performs the best and is recommended considering the efficiency of computation, as well as the reliability and conservativeness of the result. This paper effectively assists with system reliability evaluation in large-diameter tunnel constructions, where the interactive and explainable AI approach largely motivates the application of AI techniques in the field of tunnel construction. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160211902&doi=10.1016%2Fj.asoc.2023.110376&partnerID=40&md5=e11e3f67866b95fb229f39807b37c36c,CEM,Safety,Large-diameter tunnel; Risk analysis; BIM; Explainable AI; Automated machine learning,Applied Soft Computing
Journal Article,"P., Zhang Lin, L., Tiong, R. L. K.",2023,Multi-objective robust optimization for enhanced safety in large-diameter tunnel construction with interactive and explainable AI,Elsevier Ltd,234,,,"Robust optimization is an ideal solution for enhancing safety in tunnel construction in the presence of unpredictable soil conditions, especially in large-diameter tunnel construction, since it requires the least amount of information about uncertainties. However, the application of robust optimization to real-world projects is greatly hampered by its dependence on mathematical models. To address this issue, this study builds a pipeline machine learning model to forecast tunnel-induced damage that can be addressed using the robust optimization (RO) algorithm with high accuracy. The optimization process is integrated into a building information modeling (BIM) platform and analyzed using the Shapley Additive ExPlanations (SHAP) technique, allowing the designer to understand and interact with the algorithm. The average improvement of testing samples using an ellipsoidal uncertainty set with a size of 0.05 is 23.8 and 4.9% on the two selected criteria, which is more conservative than using deterministic optimization (DO) and stochastic optimization (SO). This study establishes an interactive and explainable optimization platform that enables designers to make judgments under the most unfavorable soil conditions with the least amount of accessible information about the uncertainties during tunneling. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149297713&doi=10.1016%2Fj.ress.2023.109172&partnerID=40&md5=1c3038ccea645bf0a5f2a4aa171593be,CEM,Safety,Multi-objective optimization; Robust optimization; Tunnel construction; Building information modeling; Explainable AI,Reliability Engineering and System Safety
Journal Article,"C. Y., Mostafavi Liu, A.",2025,FloodGenome: interpretable machine learning for decoding features shaping property flood risk predisposition in cities,IOP Publishing Ltd,5,1,,"The study introduces FloodGenome, an interpretable machine learning model, to assess flood risk disposition in urban areas by analyzing hydrological, topographic, and built-environment features and their interactions. Utilizing data from the U.S. National Flood Insurance Program (2003-2023) across four metropolitan areas, it employs k-means clustering and a random forest model to classify and predict property flood risk levels. The model's effectiveness is proven across different metropolitan areas, highlighting the importance of factors like elevation, and impervious surfaces in determining flood risk. FloodGenome's analysis aids in evaluating future urban development impacts on flood risk and refining property flood risk assessments at a detailed level. This tool offers critical insights for flood risk management, supporting the development of urban design strategies to mitigate flood risks.",,CE,Water,urban flooding; random forest; SHAP analysis,Environmental Research: Infrastructure and Sustainability
Journal Article,"F., Liu Liu, W., Li, A., Cheng, J. C. P.",2024,Geotechnical risk modeling using an explainable transfer learning model incorporating physical guidance,Elsevier Ltd,137,,,"While Artificial intelligence (AI) has been successfully applied in assessing geotechnical risk, such methods heavily rely on data quality to achieve satisfactory performance, and their results hardly can be interpreted due to their opaque design. With this in mind, this paper aims to address the following research gap: How can we accurately model geotechnical risks using limited data and domain knowledge, and efficiently explain the results of AI model? We develop a physics-guided transfer learning (PGTL) model to enhance the explainability and accuracy of geotechnical risk modeling. With the help of a physical model that simulates the tunnel excavation, a physics-guided dataset with 1000 samples is established and used to train a deep neural network. On these bases, transfer learning is adopted to fuse the features of physics mechanisms and monitoring data, constructing an explainable prediction model of geotechnical risk. To further support risk decision-making, feature relevance techniques are employed to assess the contribution of input parameters to risk. A shield tunnel construction in Wuhan is selected as a case to validate the effectiveness of the proposed method. The PGTL exhibits a more promising accuracy with R2 of 0.777 in contrast to three popular machine learning approaches, and provides insights into parameters that significantly induce risk, enhancing site managers’ understanding of tunnel construction and being conducive to tunnel safety. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201005605&doi=10.1016%2Fj.engappai.2024.109127&partnerID=40&md5=4ac4b24aae3e250aa4a7a246a1c7a482,CE,Geotechnical,Explainable artificial intelligence; Tunnel construction; Transfer learning; Geotechnical risk,Engineering Applications of Artificial Intelligence
Journal Article,"F., Liu Liu, W., Liu, J., Zhong, B., Sun, J.",2025,Mitigating potential risk via counterfactual explanation generation in blast-based tunnel construction,Elsevier Ltd,65,,,"Machine learning and deep learning have significantly enhanced the ability to mitigate risks in blast-based tunnel construction. However, most studies fall short in model constraints, data quality, and explainability, making non-robust risk mitigation strategies. Therefore, this study aims to investigate the following questions: how to accurately assess risk for blast-based tunnel construction using limited data, and develop effective risk mitigation strategies? This research leverages counterfactual explanation generation, a key technique of explainable artificial intelligence, along with data augmentation to develop a framework for guiding risk mitigation, which includes: (1) a two-stage data augmentation technique to address data shortage and imbalance; (2) a novel counterfactual explanation generation algorithm to optimize blasting parameters and reduce risk; and (3) a post-hoc explainable approach to provide insights on feature importance. A railway tunnel in Hubei is conducted as a case study to test the validity of the proposed method. The results show that the proposed method accurately predict overbreak, achieving the highest R2 (0.883) and the lowest RMSE (1.335) compared to baseline models. Additionally, it effectively optimizes the blasting parameters to mitigate risk, reducing the average overbreak in six scenarios. The explainable analytic identifies key factors (e.g., periphery hole spacing) influencing construction risk, thereby enhancing personnel's understanding of complex construction systems. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219498834&doi=10.1016%2Fj.aei.2025.103227&partnerID=40&md5=33aab92cc2b1fe0f525d29789d873f98,CEM,Safety,Risk mitigation; Counterfactual explanations; Explainable artificial intelligence; Tunnel construction; Data augmentation,Advanced Engineering Informatics
Journal Article,"J., Liu Liu, G., Wang, N., Jiang, Y.",2025,Screening for Anomalous Safety Condition Among Existing Buildings Using Explainable Machine Learning,John Wiley and Sons Inc,2025,1,,"To ensure a safe environment for occupants, evaluating the physical status and service performance of existing buildings is essential. However, large-scale building condition assessment usually relies on the expertise and judgment of inspectors, which can be costly and laborious due to unclear priorities, ambiguous procedures, and ineffective operations. To address these challenges, this study proposes an explainable machine learning-based screening model for the anomalous safety condition among existing buildings, narrowing down the scope of buildings requiring further and detailed inspection and monitoring. Initially, an imbalanced dataset of 18,090 survey reports of existing buildings of safe and unsafe labels is collected. Then, the synthetic minority oversampling technique (SMOTE) is conducted to balance the dataset. Subsequently, seven machine learning models are trained utilizing 10-fold cross-validation with grid search. Findings reveal that, based on the balanced dataset, the performance of ensemble learning models is significantly better than that of individual machine learning models. Specifically, the XGBoost model achieves the highest performance, with a macro-F1 of 98.49%, G-mean value of 98.49%, and accuracy of 98.49%. The final predictive model (the SMOTE-based XGBoost model) is explained using the SHapley Additive exPlanations (SHAP). Service year, structure, and location are the three most important features influencing building structural safety. This study represents a promising approach for automated screening of the anomalous safety condition among buildings, optimizing resource allocation, and enhancing the effectiveness in decision-making for construction and maintenance. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004641747&doi=10.1155%2Fstc%2F6695396&partnerID=40&md5=7569347ee62a06e59b3f095407853975,CE,Structural,Counterfactual explanations,Structural Control and Health Monitoring
Journal Article,"J. L., Antariksa Liu, G., Somvanshi, S., Das, S.",2025,Revealing equity gaps in pedestrian crash data through explainable artificial intelligence clustering,Elsevier Ltd,139,,,"Pedestrian crashes represent a critical traffic safety issue, often resulting in fatal outcomes and raising significant equity concerns. This study analyzed detailed records of pedestrian-involved crashes in California from 2018 to 2021, employing a novel clustering framework enhanced by the SHapley Additive exPlanations approach. The proposed method significantly enhanced interpretability by effectively capturing complex non-linear relationships and interactions among features. The results indicate that impairment status and lighting conditions are pivotal in severe crash outcomes, while broader societal and demographic factors are more substantially associated with less severe cases. Non-injury pedestrian crashes tend to occur in less underserved, more resilient communities, whereas fatal crashes are more common in underserved communities with poor lighting and incomplete pedestrian infrastructure, particularly when pedestrians are under the influence of drugs or alcohol. The findings underscore the necessity for developing comprehensive safety measures that not only address situational risks but also consider broader societal conditions.",,CE,Transportation,Explainable artificial intelligence,Transportation Research Part D: Transport and Environment
Journal Article,"Q., Wang Liu, J. Y., Bai, B. W.",2024,Unveiling nonlinear effects of built environment attributes on urban heat resilience using interpretable machine learning,Elsevier B.V,56,,,"Built environment attributes (BEAs) play a significant role in influencing urban heat resilience (UHR). Previous research has examined the correlations and nonlinear relationships between BEAs and both land surface temperature (LST) and urban heat island (UHI) effects. Nevertheless, the investigation into the nonlinear effects of BEAs on UHR remains underexplored. Furthermore, the advantages of explainable machine learning in elucidating the mechanisms through which BEAs affect the urban thermal environment have been extensively validated. Consequently, taking Beijing, a highly urbanized city, as a case, we conducted an empirical study to investigate the nonlinear effects of BEAs on UHR. We first operationalized UHR as the differential in LST between extreme heat waves and normal heat days. Secondly, we constructed a set of influencing factors covering BEAs and control variables. Subsequently, by integrating the Gradient Boosting Decision Tree (GBDT) with SHapley Additive exPlanations (SHAP), the nonlinear relationships between BEAs and UHR are uncovered. The results demonstrate that: 1) Nonlinear relationships between BEAs and UHR are prevalent, as well as threshold effects. 2) Greening is the key BEA affecting UHR, accounting for 22.39% in contribution, with which increase, UHR increases at an accelerating rate. 3) From the city center outward, the growth of UHR exhibits a leapfrog effect, with the growth rate in the outer ring being 2.7 times that of the inner one. 4) Interactions between BEAs impact UHR. Our findings unveil the complex nonlinear effects of BEAs on UHR, clarifying the priority and optimal quantity thresholds of BEAs. We emphasize the importance of greening and urban scale, which could support decision-making for UHR planning and precise management.",,CE,Environmental,Urban heat resilience (UHR); Built environment attributes (BEAs); Nonlinear; Explainable machine learning; SHAP ,Urban Climate
Journal Article,"T., Huang Liu, T., Ou, J., Xu, N., Li, Y., Ai, Y., Xu, Z., Bai, H.",2023,Modeling the load carrying capacity of corroded reinforced concrete compression bending members using explainable machine learning,Elsevier Ltd,36,,,"Corrosion of reinforcement can lead to a decrease in the load carrying capacity of reinforced concrete structures and affect their safety. Therefore, accurate evaluation of the ultimate load carrying capacity is crucial for the maintenance and reinforcement of corroded reinforced concrete structures. In this paper, based on experimental research data of 192 corroded reinforced concrete compression bending members, data-driven analysis was conducted using ANN, SVM, RF, and AdaBoost algorithms to establish the relationship between the influencing factors of the load carrying capacity and their ultimate load carrying capacity. The input variables include the section width of the member, section height of the member, length of member, yield strength of reinforcement, diameter of longitudinal reinforcement, compressive strength of concrete, thickness of concrete cover, hoop diameter, original eccentricity, corrosion rate and the ultimate load carrying capacity is the output variable. Additionally, this study innovatively utilizes the Shapley additive explanations (SHAP) method to enhance the interpretability of the ML models, overcoming the “black box” issue associated with ML methods. Furthermore, the performance of the ML models is compared with theoretical formulas. The results indicate that the ML models exhibit good predictive performance, with higher accuracy than thetheoretical calculation formulas. And the predictive performance of ensemble learning models (RF, AdaBoost) is better than that of single learning models (ANN, SVM). The newly developed hybrid ML model is likely to become a new choice for dealing with the load carrying capacity problem of corroded reinforced concrete compression bending members. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166968908&doi=10.1016%2Fj.mtcomm.2023.106781&partnerID=40&md5=9a3e5f2d7ef6fbaee6ce40c4ca0b49cf,CE,Structural,Data augmentation,Materials Today Communications
Journal Article,"W., Chen Liu, Y., Liu, T., Liu, W., Li, J., Chen, Y.",2025,Shield tunneling efficiency and stability enhancement based on interpretable machine learning and multi-objective optimization,Elsevier B.V,22,,320–336,"Adequate control of shield machine parameters to ensure the safety and efficiency of shield construction is a difficult and complex problem. To address this problem, this paper proposes a hybrid intelligent optimization framework that combines interpretable machine learning, intelligent optimization algorithms, and multi-objective optimization and decision-making methods. The nonlinear relationship between the input parameters and ground settlement (GS) is fitted based on the light gradient boosting machine (LGBM), and the effect of the input parameters on GS is analysed based on SHapley additive exPlanation for further feature selection. Subsequently, the hyperparameters of LGBM were determined based on the sparrow search algorithm (SSA) to better fit the input–output relationship. On this basis, a multi-objective intelligent optimization model is established to solve the optimized operating parameters of shield machine by non-dominated sorting genetic algorithm II and technique for order preference by similarity to ideal solution to reduce GS and improve drilling efficiency. The results demonstrate that the SSA-LGBM model predicts GS with high accuracy, exhibiting an RMSE of 4.775, a VAF of 0.930 and an R2 of 0.931. These metrics collectively reflect the model's excellent performance in prediction accuracy, ability to explain data variability, and control of prediction bias. The multi-objective optimization model is effective in optimizing two objectives, and the improvement can reach up to 39.38%; at the same time, the model has high scalability and can also be applied to three or more objectives. The intelligent optimization framework for shield construction parameters proposed in this paper can generate the optimal parameter combinations for shield machine manipulation, and provide reference and guidance when there are conflicting optimization objectives. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002332691&doi=10.1016%2Fj.undsp.2025.01.001&partnerID=40&md5=7615fc3ae4b836d0e6aa7807bf4c4a36,CE,Geotechnical,Shield tunnelling machine; Interpretable machine learning; Hyperparameter optimization; Multi-objective optimization; Field penetration index; Ground settlement,Underground Space
Journal Article,"W., Liu Liu, F., Fang, W., Love, P. E. D.",2024,Causal discovery and reasoning for geotechnical risk analysis,Elsevier Ltd,241,,,"Artificial intelligence (AI), such as machine learning (ML) models, is profoundly impacting an organization's ability to assess safety risks during the construction of tunnels. Yet, ML models are black boxes and suffer from interpretability and transparency issues – they are unexplainable. Hence the motivation of this paper is to address the following research question: How can we effextively explain data-driven ML model's predicitve assessment of geotechnical risks in tunnel construction? We draw on the concept of ‘eXplainable AI’ (XAI) and utilize causal discovery and reasoning to help analyze and interpret the manifestation of geotechnical risks in tunnel construction by developing: (1) a sparse nonparametric and nonlinear directed acyclic diagram (DAG) used to determine the causal structure of risks between sub-systems; (2) a multiple linear regression model, which we use to estimate the effect of the causal relationships between sub-systems; and (3) a probability-based reasoning model to quantify and reason about risk. We use the San-yang Road tunnel project in Wuhan (China) to validate the feasibility and effectiveness of our proposed approach. The results indicate that our approach can accurately explain what and how risks are obtained from a data-driven probability-based ML model for ground settlement in tunnel construction. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173210863&doi=10.1016%2Fj.ress.2023.109659&partnerID=40&md5=b6c4eb0bd101301856ad0f2dfda7fb03,CE,Geotechnical,Causal discovery; Probability-based reasoning; Risk; Tunnel construction; Explainable Artificial Intelligence (XAI),Reliability Engineering & System Safety
Journal Article,"W., Liu Liu, F., Love, P. E. D., Fang, W.",2025,Causality-Guided Explainable Deep-Learning Model for Managing Tunnel-Induced Ground Settlement Risks,American Society of Civil Engineers (ASCE),39,4,,"The use of deep learning (DL) has been growing in tunnel-induced ground settlement risk modeling, eliminating the necessity for extensive prior risk management knowledge. Despite the success of deploying a DL model to predict risk, challenges prevail. (1) DL requires high-quality data, which is expensive and time-consuming to prepare, and (2) DL is a “black-box,” which is difficult to understand and interpret. In this instance, we address the following question in this paper: How can we accurately predict ground settlement with limited monitoring data using DL and concurrently provide effective explanations for the generated results? We propose a new DL approach combining explainable techniques to improve ground settlement risk modeling accuracy and explainability. Our approach comprises the following: (1) an interval type-2 fuzzy system to process limited data and improve its usability; (2) a novel causal-based feature selection to determine input parameters that have strong causal effects with ground settlement risk; and (3) a soft-type attention module to evaluate and allocate feature importance to inputs, guiding neural networks to concentrate on learning features in a more targeted manner. A case study is used to validate the feasibility and effectiveness of our approach, demonstrating its superiority in predicting ground settlement risk with a high degree of robustness. We suggest that our approach can help decision makers better understand the “how” and “what” of DL-produced outputs, improving decision making associated with managing safety in construction. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000760347&doi=10.1061%2FJCCEE5.CPENG-6209&partnerID=40&md5=30ec39ff0168da56b49ff613fdac56f9,CE,Geotechnical,Deep learning; Fuzzy system; Explainable artificial intelligence; Ground settlement risk,Journal of Computing in Civil Engineering
Journal Article,"W., Yang Liu, Z., Gui, C., Li, G., Xu, H.",2025,Investigating the Nonlinear Relationship Between the Built Environment and Urban Vitality Based on Multi-Source Data and Interpretable Machine Learning,Multidisciplinary Digital Publishing Institute (MDPI),15,9,,"Optimizing the built environment to foster urban vitality is essential for effective urban planning and sustainable development. Previous studies have predominantly focused on analyzing the relationship between the built environment and urban vitality at either a macro or micro-scale, often assuming a predefined linear relationship. In this study, we investigate the potential non-linear interactions between the built environment and urban vitality by employing an interpretable spatial machine learning framework that integrates the XGBoost model with the SHapley Additive exPlanations (SHAP) algorithm. Additionally, we analyze the determinants of urban vitality across both micro and macro-scales using multi-source data, semantic segmentation models, and street view imagery. Our findings reveal the following key insights: (1) the distribution of urban vitality exhibits spatial heterogeneity within the main urban area of Shanghai, with high vitality areas concentrated in the Huangpu District and at intersections with neighboring districts, demonstrating a decline from the center to the periphery; (2) the XGBoost model outperforms other comparative models, showcasing superior capabilities in simulating and predicting urban vitality; (3) among the various built environment factors influencing urban vitality, building coverage, population density, and distance to the CBD exert the most significant effects, while the green view index and the number of bus stops contribute relatively less; (4) all built environment factors demonstrate nonlinear impacts and exhibit certain threshold effects on urban vitality. The analytical outcomes of this study provide valuable insights for optimizing the spatial layout and resource allocation within urban settings, offering references for urban planning and sustainable development initiatives. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004794277&doi=10.3390%2Fbuildings15091414&partnerID=40&md5=1aa1c4b05575099d572688dd779a7930,CE,Environmental,urban vitality; built environment; nonlinear relationship; XGBoost; interpretable machine learning; threshold effect,Buildings
Journal Article,"X., Gong Liu, H. L., Zhou, C. F., Chen, B. B., Su, Y. M., Zhu, J. J., Lu, W.",2025,Analysis of Ground Subsidence Evolution Characteristics and Attribution Along the Beijing-Xiong'an Intercity Railway with Time-Series InSAR and Explainable Machine-Learning Technique,Multidisciplinary Digital Publishing Institute (MDPI),14,2,,"The long-term overextraction of groundwater in the Beijing-Tianjin-Hebei region has led to the formation of the world's largest groundwater depression cone and the most extensive land subsidence zone, posing a potential threat to the operational safety of high-speed railways in the region. As a critical transportation hub connecting Beijing and the Xiong'an New Area, the Beijing-Xiong'an Intercity Railway traverses geologically complex areas with significant ground subsidence issues. Monitoring and analyzing the causes of land subsidence along the railway are essential for ensuring its safe operation. Using Sentinel-1A radar imagery, this study applies PS-InSAR technology to extract the spatiotemporal evolution characteristics of ground subsidence along the railway from 2016 to 2022. By employing a buffer zone analysis and profile analysis, the subsidence patterns at different stages (pre-construction, construction, and operation) are revealed, identifying the major subsidence cones along the Yongding River, Yongqing, Daying, and Shengfang regions, and their impacts on the railway. Furthermore, the XGBoost model and SHAP method are used to quantify the primary influencing factors of land subsidence. The results show that changes in confined water levels are the most significant factor, contributing 34.5%, with strong interactions observed between the compressible layer thickness and confined water levels. The subsidence gradient analysis indicates that the overall subsidence gradient along the Beijing-Xiong'an Intercity Railway currently meets safety standards. This study provides scientific evidence for risk prevention and the control of land subsidence along the railway and holds significant implications for ensuring the safety of high-speed rail operations.",,CE,Transportation,PS-InSAR; explainable machine learning; land subsidence; evolution characteristics; attribution analysis,Land
Journal Article,"X., Shi Liu, X. Q., Peng, Z. R., He, H. D.",2023,Quantifying the effects of urban fabric and vegetation combination pattern to mitigate particle pollution in near-road areas using machine learning,Elsevier Ltd,93,,,"There are many intertwined factors such as urban fabric, vegetation combination, and meteorological conditions that influence the spatial distribution of fine particles (PM<inf>2.5</inf>, aerodynamic diameter less than 2.5 μm) in near-road urban areas, and it is difficult yet important to disentangle them. Identifying the relative importance of those factors could facilitate designing urban fabric and roadside vegetation combination patterns to mitigate the negative impacts of particles. This study first proposes an effective and reliable integrated method of the ENVI-met microclimate model and interpretable machine learning to quantify the multifactorial effects. Data from ENVI-met models with different built environments and meteorological conditions are used to train and test the machine learning (ML) models. All ML models show high accuracy in predicting PM<inf>2.5</inf> concentrations with R2 high than 0.87. Results indicate that height level and study area are the two most important features, explaining 22.8% and 20.4% of the variation in PM<inf>2.5</inf> concentration, respectively. The feature pairwise interactions account for 45.8% of ambient particle concentrations. The combination of trees and hedges in urban fabrics with longer building length than width could mitigate PM<inf>2.5</inf> concentrations in residential areas. The integrated methods in this study could be extrapolated to future research to perform source identification analysis. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150273403&doi=10.1016%2Fj.scs.2023.104524&partnerID=40&md5=8411c6464619f1c179ba3d9d3292c489,CE,Environmental,Particulate matter; Source contribution; Spatial distribution; Urban fabric; Vegetation combination,Sustainable Cities and Society
Journal Article,"Z., Liu Liu, C.",2024,The association between urban density and multiple health risks based on interpretable machine learning: A study of American urban communities,Elsevier Ltd,153,,,"With the growing complexity in urban areas, cities have become unprecedentedly intricate systems. This paper aims to develop interpretable machine learning (ML) approaches to unravel the sophisticated associations. In a case study of American urban communities, we apply interpretable ML methods to identify the associations between urban density and multiple health risks. We define urban density from three dimensions of population, built environment, and activity and measure multiple health risks based on categories of physical diseases, mental diseases and health burden. Initially, we conduct cluster analysis to control socioeconomic variables and select study samples. Then we build several ML models of multiple linear regression, decision trees, random forests, and extreme gradient boosting. Interpretable methods, including feature importance, partial dependence plots, individual conditional expectations, and Shapley additive explanations, are used to interpret the models by identifying important factors, non-linear relationships, the interactions between variables. The results show the advantages of interpretable ML methods in efficiency and transparency. Our findings verify that the associations between urban density and multiple health risks are complicated. The similarities and differences between various health risks provide valuable evidence on key factors, thresholds, and pathologic characteristics that can guide the healthy and sustainable development of cities. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199351800&doi=10.1016%2Fj.cities.2024.105170&partnerID=40&md5=3acd6ee0aaa344d63c66a57d3a3c0564,CE,Environmental,Urban density; Interpretable machine learning; Public health; Built environment; Chronic disease; Population density,Cities
Journal Article,"Z., Zhang Liu, L., Li, L.",2025,Revealing the non-linear and interacting effect of six subcategories’ construction land on energy-related carbon emissions based on interpretable machine learning: Evidence from 2859 counties in China,Elsevier Ltd,10,,,"Construction land, as the primary carrier of human industrial, residential, and transportation activities, represents a major source of carbon emissions. Exploring the impact of different types of construction land on carbon emissions is of critical importance for carbon reduction. Existing literature typically assesses the influence of different construction land types on carbon emissions at the block scale. While the block-scale analysis is limited by local environmental, complicating broader spatial impact assessments. In this study, we adopt advanced land use data from China's Third National Land Survey to measure the broad impact of different types of construction land on carbon emissions at county scale. The interpretable machine learning method was employed to analyze the global, local, and interactive effects of these construction land categories on carbon emissions. Our findings reveal that commercial, industrial, and residential lands exhibit positive associations with carbon emissions, while public facilities, special-use, and transportation lands demonstrate inhibitory effects. Importantly, the impact of construction land on emissions displays significant spatial heterogeneity. Industrial and commercial lands predominantly influence emissions in developed areas, whereas residential land's impact is more pronounced in developing regions. Furthermore, we observe that interactions between most types of construction land can inhibit carbon emissions, suggesting potential synergies in land use planning for emission reduction. These findings contribute to the broader understanding of how land use patterns influence carbon emissions and can inform more effective, targeted approaches to sustainable urban development and climate change mitigation. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013755567&doi=10.1016%2Fj.sftr.2025.101075&partnerID=40&md5=cccaf940d4f6f44b8164fa7dbc9e3eb3,CE,Environmental,Carbon emission; Land use; Interpretable machine learning,Sustainable Futures
Journal Article,"Z. W., Felton Liu, T., Mostafavi, A.",2024,Interpretable machine learning for predicting urban flash flood hotspots using intertwined land and built-environment features,Elsevier Ltd,110,,,"Pluvial flash floods are fast-moving hazards and causes significant disruptions in urban areas. With the increase in heavy precipitations, the ability to proactively identify flash floods hotspots in cities is critical for flood nowcasting and predictive monitoring of risks. While rainfall runoff models and hydrologic models are useful models for flash flood prediction, these models are computationally expensive and effort intensive to be used for flood nowcasting. To address this challenge, this study presents interpretable machine learning models for predicting urban flash flood hotspots based on intertwined land and built environment features. The task of predicting flash flood hotspots is formulated as a binary classification problem, and three recent flash flood events in U.S. cities are selected for data collection and model validation. Various features related to land and built environment characteristics are constructed using diverse datasets, and the occurrences of flash floods are captured using crowdsource data from the events. Using these features and datasets, the flash flood hotspots of cities are predicted with two ensemble models based on decision trees. The results demonstrate that the models can achieve good accuracy (0.8) in identifying flooded/non-flooded locations. Especially, the models can achieve high true positive rate (0.83-0.89) and low missing rate, demonstrating the methods' practicability for accurately predicting flooded hotspots. The model interpretation results indicate that land features related to hydrological and topological features have greater impacts on flash flood risk, than built environment features. Further analysis reveals that the feature importance, model performance, and model transferability performance vary among cities and localized specifications of the models are needed for accurate prediction of flash flood for a particular city. The data-driven machine learning models presented in this study provide a useful tool for predicting flash flood hotspots based on the intertwined features of land and the built environment in cities to enable nowcasting and proactive monitoring of flash flood hotspots for emergency response and also inform integrated urban design and development towards flash flood risk reduction.",,CE,Water,Flash flood; Crowdsourced data; Built environment; Machine learning,"Computers, Environment and Urban Systems"
Journal Article,"Z. Y., Ma Liu, X. Y., Hu, L. H., Liu, Y., Lu, S., Chen, H. L., Tan, Z.",2022,Nonlinear Cooling Effect of Street Green Space Morphology: Evidence from a Gradient Boosting Decision Tree and Explainable Machine Learning Approach,Multidisciplinary Digital Publishing Institute (MDPI),11,12,,"Mitigation of the heat island effect is critical due to the frequency of extremely hot weather. Urban street greening can achieve this mitigation and improve the quality of urban spaces and people's welfare. However, a clear definition of street green space morphology is lacking, and the nonlinear mechanism of its cooling effect is still unclear; the interaction between street green space morphology and the surrounding built environment has not been investigated. This study used machine learning, deep learning, and computer vision methods to predict land surface temperature based on street green space morphology and the surrounding built environment. The performances of the XGBoost, LightGBM, and CatBoost models were then compared, and the nonlinear cooling effects offered by the street green space morphology were analyzed using the Shapley method. The results show that streets with a high level of green environment exposure (GVI > 0.4, NDVI > 4) can accommodate more types of green space morphology while maintaining the cooling effect. Additionally, the proportion of vegetation with simple geometry (FI < 0.2), large leaves (FD < 0.65), light-colored leaves (CSI > 13), and high leaf density (TDE > 3) should be increased in streets with a low level of green environment exposure (GVI < 0.1, NDVI < 2.5). Meanwhile, streets with highly variable building heights (AFI > 1.5) or large areas covered by buildings (BC > 0.3) should increase large leaf vegetation (FD < 0.65) while decreasing dark leaf vegetation (CSI < 13). The study uses machine learning methods to construct a nonlinear cooling benefit model for street green space morphology, proposes design recommendations for different street green spaces that consider climate adaptation, and provides a reference for urban thermal environment regulation.",,CE,Environmental,street green space morphology; land surface temperature; cooling effect; gradient boosting decision tree; interpretable machine learning,Land
Journal Article,"J., Lee Loy-Benitez, H. K., Kyu Song, M., Choi, Y., Seungwon Lee, S.",2024,Transfer component analysis-driven domain adaptation approach for estimating the life of tunnel boring machine disc cutters,Elsevier Ltd,147,,,"Estimating the life of disc cutters in tunnel boring machines is crucial for tunnel excavation efficiency. Accordingly, generating accurate predictions of the state of disc cutters can prevent unexpected downtimes while optimizing cutting performance. The literature presents investigations of the application of machine learning (ML) for estimating the disc cutter life. Nevertheless, diverse excavation conditions and data scarcity have stimulated research on transfer learning (TL) frameworks that tailor models for different domains, that is, domain adaptation (DA), by generating a common subspace in which the discrepancies among them are minimized. This study presents an end-to-end framework to estimate the disc cutter life through transfer component analysis, a DA-based method for projecting different domains by considering excavation project datasets in a common space; then, different ML models are trained, including linear, probabilistic, and neural methods. The results revealed that the Gaussian process regression model was superior to other methods, with R2 = 0.99. In conclusion, TL is a valuable tool for estimating disc cutter life across different domains, enabling accurate predictions and leveraging knowledge to mitigate data scarcity challenges. In addition, an explainable AI analysis is conducted to quantify and identify relevant features impacting negatively on the disc cutter life, evidencing great contribution from the cutter rotation speed, quartz content, screw rate and specific energy. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189173375&doi=10.1016%2Fj.tust.2024.105714&partnerID=40&md5=a201fdd80a8a5d877cdbbf9cccc3a7e1,CE,Geotechnical,Disc cutter life estimation; Domain adaptation; Gaussian process regression; Transfer component analysis; Tunnel boring machines,Tunnelling and Underground Space Technology
Journal Article,"H., Chen Luo, J., Love, P. E. D., Fang, W.",2024,Explainable Transfer Learning for Modeling and Assessing Risks in Tunnel Construction,Institute of Electrical and Electronics Engineers Inc.,71,,8339–8355,"Deep learning models are black boxes. Thus, determining the source domain data contributing to transfer learning for ground settlement prediction is impossible. The research presented in this article aims to determine the source domain data (i.e., the dataset or domain used for model pretraining) that contributes most to transfer learning for risk prediction in tunnel construction and quantify its contribution to improving prediction accuracy. We propose a novel explainable transfer learning approach to quantify the selection of degraded knowledge from source and subsource domains. Our approach comprises feature selection and space point clustering, construction of a similarity metric between the target domain and each subsource domain, and construction of a stacked deep neural network model with selective transfer learning. We apply our model to a real-life tunnel project to demonstrate its feasibility and effectiveness. The results indicate that our proposed explainable transfer learning approach outperforms other transparent and opaque analysis models on risk prediction with R2 above 0.5 by adjusting the clustering, transferring, and freezing strategy, and the optimal number of freezing layers should be less than half of the total number of layers, and the best number of freezing layers is 1. We show that explaining transfer learning enables transparency in training and understanding the source domain data, contributing to ground settlement prediction. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187024266&doi=10.1109%2FTEM.2024.3369231&partnerID=40&md5=ca1cb6cd6ddc0cb0efce87a6b4c4f7e4,CE,Geotechnical,Explainable artificial intelligence (XAI); risk; transfer learning; tunneling,IEEE Transactions on Engineering Management
Journal Article,"J. Y., Liu Luo, Y. C.",2024,Adaptive and Explainable Deep Learning-Based Rapid Identification of Architectural Cracks,Institute of Electrical and Electronics Engineers Inc.,12,,111741–111751,"Concrete architectural structures are widely used in urban construction, making the health diagnosis and maintenance of these structures increasingly essential and urgent. Crack identification is crucial for maintaining the structural integrity and safety of buildings. Traditional methods rely on manual inspection, which is plagued by low accuracy, inefficiency, and safety hazards. This paper proposes a technique combining an attention-based SqueezeNet network with Gradient-weighted Class Activation Mapping (Grad-CAM) for automatically recognizing and visually explaining building cracks. By integrating the Squeeze-and-Excitation (SE) attention mechanism with the lightweight SqueezeNet network, this method can adaptively adjust the importance of feature channels by learning global information, effectively improving the network's accuracy and efficiency. The experimental results show that the Att-SqueezeNet model achieved a high precision of 0.995, a training time of only 133 seconds, and a model size of 4.9M, significantly outperforming models such as SqueezeNet, RF, CNN, VGG-19, and B-CNN. This demonstrates its robustness, rapid identification and suitability for practical applications and building crack identification. Moreover, the utilization of Grad-CAM for visualization not only offers an intuitive explanation of the model's decision-making process but also provides a more comprehensible understanding of crack detection results. This is crucial for advancing building maintenance automation, reducing reliance on manual labor, and increasing the precision and reliability of detection tasks. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201314791&doi=10.1109%2FACCESS.2024.3442926&partnerID=40&md5=e70d169e8b041bbb2cc0cfce145d032c,CE,Structural,Crack identification; SqueezeNet network; SE attention mechanism; Grad-CAM; model interpretability,IEEE Access
Journal Article,"K. Y., Samat Luo, A., Du, P. J., Liu, S. C., Liang, J. X., Abuduwaili, J., Shokparova, D., Juliev, M.",2025,Integrative analysis of transboundary land use conflicts in the Aral Sea Basin: A multi-scale assessment of drivers and strategies for sustainable management,Elsevier B.V,21,,,"Addressing escalating land use conflicts (LUCs) is critical for sustainable development in resource-scarce, transboundary regions. The Aral Sea Basin (ASB), Central Asia's largest transboundary basin characterized by arid conditions and vulnerable ecosystems, serves as a crucial case study. This research introduces an innovative framework, integrating multi-scale spatial assessments with interpretable machine learning (XGBoost-SHAP), to overcome limitations of previous fragmented analyses and provide deeper insights into LUCs dynamics. We systematically evaluated land suitability for ecological preservation, agriculture, and urban construction, quantified conflict intensity, and identified key drivers across the entire ASB, including its Amu Darya and Syr Darya sub-basins. Quantitative results reveal profound spatial heterogeneity in land use potential, with 56.29% of the basin suitable for ecological preservation, only 6.54% for agriculture, and 72.67% for urban construction-indicating dominant ecological value, limited agricultural suitability, and high urban development pressure. Conflicts were found to be pervasive and intense, driven by a complex interplay of natural factors and socio-economic pressures, with distinct upstream-downstream patterns across sub-basins. Crucially, this study provides spatially explicit evidence highlighting the urgent need for integrated, transboundary land management. The results offer actionable, data-driven insights essential for designing targeted strategies, fostering collaborative resource governance, and ultimately promoting sustainable development pathways that balance ecological integrity with human needs in the ASB and similar complex transboundary basins worldwide.",,CE,Environmental,Aral Sea Basin; Land use conflicts (LUCs); Sustainable land management; XGBoost-SHAP analysis; Transboundary resource governance,"Resources, Environment and Sustainability"
Journal Article,"X., Huo Ma, Z., Lu, J., Wong, Y. D.",2025,Deep Forest with SHapley additive explanations on detailed risky driving behavior data for freeway crash risk prediction,Elsevier Ltd,141,,,"Freeway crash risk prediction is a critical component of traffic safety management, yet existing crash risk models often fail to capture complex driving behaviors and lack interpretability. This study introduces a novel freeway crash risk prediction framework based on the Deep Forest (DF) algorithm, considering the detailed risky driving behavior data. The DF model integrates multi-grained scanning and cascade forest layers, enabling it to capture the complex relationship between risky driving behavior features. SHapley Additive Explanations (SHAP) are applied to interpret the model's predictions, including both SHAP summary and interaction results. Additionally, ablation studies are conducted to evaluate the contributions of key components like multi-grained scanning and cascade structures to the model's performance. The experimental results demonstrate that the DF model outperforms traditional machine learning models. The DF model achieves an area under the receiver operating characteristic curve of 0.825, with a balanced Sensitivity of 0.75 and Specificity of 0.816, surpassing other models. The ablation studies show that removing multi-grained scanning, cascade layers, or completely random tree forest leads to performance declines, confirming the importance of each component. The SHAP analysis highlights that sharp acceleration and braking behaviors have the most significant impact on crash risk, offering clear, interpretable insights into how driving behaviors contribute to risk. Overall, the DF model's superior performance and SHAP-based interpretability provide a powerful tool for traffic safety management. These findings emphasize the value of incorporating both driving behavior intensity and model interpretability into crash risk prediction, offering practical applications for reducing crash rates. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211039134&doi=10.1016%2Fj.engappai.2024.109787&partnerID=40&md5=0ec1cb68bf710707d4589a67fa642f9f,CE,Transportation,Crash risk prediction; Interpretability; Explainable machine learning; Risky driving behavior; Deep forest,Engineering Applications of Artificial Intelligence
Journal Article,"Y., Miao Ma, R., Chen, Z., Zhang, B., Bao, L.",2022,An interpretable analytic framework of the relationship between carsharing station development patterns and built environment for sustainable urban transportation,Elsevier Ltd,377,,,"The in-depth understanding of the relationship between development patterns of carsharing stations and built environment are important to the comprehensive station evaluation, layout optimization and urban spatial resources planning. However, the previous researches mainly study the operation of carsharing by aggregate methods with cross-sectional data and rarely discovered patterns within carsharing operation time series. Therefore, an interpretable analytic framework is proposed for predicting development patterns of carsharing stations, which is composed of a development pattern construction method based on Time Series Clustering and an interpretable prediction method based on CatBoost and SHAP models. The temporal variations of time series data are sufficiently utilized by time series clustering to identify patterns and CatBoost-SHAP has better classification performance and interpretability than general machine learning methods. The proposed framework is applied to explore the relationship between the development pattern of one-way carsharing stations and the built environment influencing factors. The result shows that the carsharing stations of Nanjing EVCARD are divided into two types: increasing pattern and decreasing pattern. The built environment factors that have the greatest impact on model output and the impact of pairwise factors are visually analysed. Moreover, this is also effective for a specific individual station to analyze the causes of its status quo. Therefore, this study provides data-driven intuitive decision references for carsharing operators, which helps the operators effectively manage carsharing stations. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139331634&doi=10.1016%2Fj.jclepro.2022.134445&partnerID=40&md5=33b67570b6d9faab0c7f22037dbd3133,CE,Transportation,Station-based one-way carsharing; Carsharing stations; Urban governance; Time series clustering; Interpretable machine learning; CatBoost-SHAP,Journal of Cleaner Production
Journal Article,"J. P. S. S., Sandamal Madushani, R. M. K., Meddage, D. P. P., Pasindu, H. R., Gomes, P. I. A.",2023,Evaluating expressway traffic crash severity by using logistic regression and explainable & supervised machine learning classifiers,Elsevier Ltd,13,,,"The number of expressway road accidents in Sri Lanka has significantly increased (by 20%) due to the expansion of the transport network and high traffic volume. It is crucial to identify the causes of these crashes for effective road safety management. However, traditional statistical methods may be insufficient due to their inherent assumptions. This study utilized explainable machine learning to investigate the factors that affect the severity of traffic crashes on expressways. The study evaluated two groups of traffic crashes: fatal or severe crashes, and other crashes that included non-severe injuries or only property damage. Five factors that contribute to crashes were analyzed: road surface condition, road alignment, location, weather condition, and lighting effect. Four machine learning models (Random Forest (RF), Decision Tree (DT), extreme gradient boosting (XGB), K-Nearest Neighbor (KNN)) were developed and compared with Logistic Regression (LR) using 223 training and 56 testing data instances. The study revealed that the machine learning algorithms provided more accurate predictions than the LR model. To explain the machine learning models, Shapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) were used. These methods revealed that all five features decreased the possibility of occurrence of fatal accidents. SHAP and LIME explanations confirmed the known interactions between factors influencing crash severity in expressway operational conditions. These explanations increase the trust of end-users and domain experts on machine learning models. Furthermore, the study concluded that using explainable machine learning methods is more effective than traditional regression analysis in evaluating safety performance. Additionally, the results of the study can be utilized to improve road safety by providing accurate explanations for decision-making processes for black-box models. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164674537&doi=10.1016%2Fj.treng.2023.100190&partnerID=40&md5=634885c00c8d2da8d554423657a9a56a,CE,Transportation,Explainable machine learning; Machine learning; Traffic crash severity; Expressways; Logistic regression,Transportation Engineering
Journal Article,"M., Gonzalez-Carreon Manfren, K. M.",2025,Tracking decarbonisation: Scalable and interpretable data-driven methods for district energy systems,Elsevier Ltd,391,,,"The urgent push for decarbonisation demands innovative, transparent methods to analyse and track decarbonisation strategies. This study addresses the problem of modelling energy consumption patterns at both building and district scales, ensuring transparency and scalability. By integrating well-established measurement and verification (M&V) techniques with interpretable data-driven modelling strategies, the research proposes a modelling workflow to track energy performance on a dynamic basis. The methods makes use of readily available metering data for electricity, district heating, and natural gas across a district, collected within a digital platform. A multi-resolution modelling approach is employed, with data at monthly, daily, and hourly intervals, that pinpoints anomalies and is meant to support a continuous refinement of operational strategies and efficiency measures. The Highfield Campus at the University of Southampton serves as the case study, illustrating how scalable, interpretable data-driven models can identify performance deviations and inform both short-term facilities management and long-term decarbonisation strategies. Findings reveal that simple and interpretable regression models can identify substantial variations in energy consumption pattern over longer time frames (ranging from months to years), whereas high-resolution analyses enhance the comprehension of dynamic operational patterns (days to hours). Both objectives can be achieved while maintaining a level of continuity in the modelling process, progressing from basic to detailed models while retaining interpretability. Further research will refine these models through additional physics-based constraints and explore deeper integrations with digital energy management platforms, offering replicable insights for broader district and urban-scale applications. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002321325&doi=10.1016%2Fj.apenergy.2025.125883&partnerID=40&md5=7ea2ad35e4cb52445e0449c8ae3a11f3,CE,Energy,Data-driven energy modelling; Interpretable machine-learning; Regression-based approaches; Time of week and temperature; Measurement and verification; Energy analytics,Applied Energy
Journal Article,"M., Tagliabue Manfren, L. C., Cecconi, F. R., Ricci, M.",2022,Long-Term Techno-Economic Performance Monitoring to Promote Built Environment Decarbonisation and Digital Transformation-A Case Study,Multidisciplinary Digital Publishing Institute (MDPI),14,2,,"Buildings' long-term techno-economic performance monitoring is critical for benchmarking in order to reduce costs and environmental impact while providing adequate services. Reliable building stock performance data provide a fundamental knowledge foundation for evidence-based energy efficiency interventions and decarbonisation strategies. Simply put, an adequate understanding of building performance is required to reduce energy consumption, as well as associated costs and emissions. In this framework, Variable-base degree-days-based methods have been widely used for weather normalisation of energy statistics and energy monitoring for Measurement and Verification (M & V) purposes. The base temperature used to calculate degree-days is determined by building thermal characteristics, operation strategies, and occupant behaviour, and thus varies from building to building. In this paper, we develop a variable-base degrees days regression model, typically used for energy monitoring and M & V, using a proxy"" variable, the cost of energy services. The study's goal is to assess the applicability of this type of model as a screening tool to analyse the impact of efficiency measures, as well as to understand the evolution of performance over time, and we test it on nine public schools in the Northern Italian city of Seregno. While not as accurate as M & V techniques, this regression-based approach can be a low-cost tool for tracking performance over time using cost data typically available in digital format and can work reasonably well with limited resolution, such as monthly data. The modelling methodology is simple, scalable and can be automated further, contributing to long-term techno-economic performance monitoring of building stock in the context of incremental built environment digitalization.""",,CE,Energy,data-driven methods; data-driven energy modelling; regression-based approaches; interpretable machine-learning; energy analytics; techno-economic analysis; Measurement and Verification,Sustainability
Journal Article,"S., Shin Mangalathu, H., Choi, E., Jeon, J. S.",2021,Explainable machine learning models for punching shear strength estimation of flat slabs without transverse reinforcement,Elsevier Ltd,39,,,"Flat slabs, despite their aesthetic qualities and widespread use in construction, are susceptible to brittle shear failure. In addition, although design provisions are available, they are often associated with high bias and variance. This study evaluates the efficiency of machine learning-based approaches in establishing accurate prediction models for the punching shear strength of flat slabs without transverse reinforcement. To this end, 380 experimental results from various literature are assembled in this study. In addition to linear regression, seven machine learning methods as ridge regression, support vector regression, decision tree, K-nearest neighbors, random forest, adaptive boosting, and extreme gradient boosting—are considered in this study to obtain the best prediction model for the punching shear strength of flat slabs. Based on random assignment of the data into training and test sets and a performance evaluation of the test set, the extreme gradient boosting model is shown to have the highest coefficient of determination and lowest mean square error estimate. The superior performance of this machine learning model is further underscored through comparison of its shear strength predictions with those of existing code provisions and empirical models. It is noted that the extreme gradient boosting model has a coefficient of determination of 0.98, and the associated coefficient of variation is 0.09. This study also employs the SHapley Additive exPlanation method to explain the importance and contribution of the factors that influence the punching shear strength in the extreme gradient boosting model. © 2021 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101879398&doi=10.1016%2Fj.jobe.2021.102300&partnerID=40&md5=b1014642c0998963427bd1d1dd7abdc5,CE,Structural,Punching shear strength; Flat slabs; Machine learning; Extreme gradient boosting; SHapley additive exPlanations,Journal of Building Engineering
Journal Article,"S. K., Srivastava Mangla, P. R., Eachempati, P., Tiwari, A. K.",2024,Exploring the impact of key performance factors on energy markets: From energy risk management perspectives,Elsevier B.V.,131,,,"Currently, there are limited mechanisms to control harmful greenhouse gas emissions. There is a need to contain these emissions at the source level; understanding the root cause is imperative. This would aid in monitoring and curbing those factors to minimize these harmful emissions and control incidences of energy risk. While there are studies evidencing the role of generic indicators like per capita carbon consumption on greenhouse gas levels, these are also equally influenced by climatic risk factors such as surface temperature. Research suggests that climatic factors significantly impact fluctuating greenhouse gas emissions. However, existing studies have not quantified the precise extent to which these factors drive harmful emissions, which, in turn, also curb energy efficiency and increase the costs of generation of energy alternatives. To address this gap, the outcome variable 'Total greenhouse gas emissions including land-use change and forestry' is examined using advanced machine learning algorithms such as Random Forest, Multi-layer perceptron models and Deep Neural Networks. Algorithms are chosen in the hierarchical order of accuracy to capture the differential capabilities of detecting the causation factors of harmful emissions. While the above algorithms see the essential features in terms of absolute value, there is a need to examine how each factor contributes to the emissions relative to the others. The Shapley framework of Explainable AI is therefore employed to scientifically assess the influence of different factors on consumption levels. The outcomes of the Shapley analysis are then validated through regression and further supported by the Fuzzy Analytical Hierarchy Process (AHP). The research also proposes adopting association rule mining to analyze the co-occurrence of specific climatic conditions on energy consumption. The findings of this study offer valuable insights for both society and experts in climate and energy, enabling them to develop specific strategies and targeted climatic policies for effective energy risk management. This would present an opportunity for economic transformation, job creation, technological advancement, and improved environmental and public health outcomes. While initial costs and challenges may be associated with a transition, the long-term benefits would help attain sustainable energy economics.",,CE,Energy,Random Forest; Multi-layer perceptron; Deep Neural Network; XAI; Regression; Fuzzy AHP; Association rule mining; Shapley; Climatic risk; Weather; Energy risk management; Energy economics; Energy efficiency; Energy demand forecasting,Energy Economics
Journal Article,"D. R., Cerqueira Martins, S. M., Santos, C. P.",2024,Seeking optimal and explainable deep learning models for inertial-based posture recognition,Elsevier B.V.,306,,,"Deep Learning (DL) models, widely used in several domains, are often applied for posture recognition. This work researches five DL architectures for posture recognition: Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Transformer, hybrid CNN-LSTM, and hybrid CNN-Transformer. Agriculture and construction working postures were addressed as use cases, by acquiring an inertial dataset during the simulation of their typical tasks in circuits. Since model performance greatly depends on the choice of the hyperparameters, a grid search was conducted to find the optimal hyperparameters. An extensive analysis of the hyperparameter combinations’ effects is presented, identifying some general tendencies. Moreover, to unveil the black-box DL models, we applied the Gradient-weighted Class Activation Mapping (Grad-CAM) explainability method on CNN's outputs to better understand the model's decision-making, in terms of the most important sensors and time steps for each window output. Innovative hybrid architectures combining CNN and LSTM or Transformer encoder were implemented, by using the convolution feature maps as LSTM's or Transformer's inputs and fusing both subnetworks’ outputs with weights learned during the training. All architectures successfully recognized the eight posture classes, with the best model of each architecture exceeding 91.5% F1-score in the test. A top F1-score of 94.33%, with an inference time of just 0.29 ms (in a regular laptop), was achieved by a hybrid CNN-Transformer. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208670686&doi=10.1016%2Fj.knosys.2024.112700&partnerID=40&md5=766d2122278fd344082f455e38a94950,CEM,Safety,Deep learning; Explainable artificial intelligence; Inertial-based posture recognition,Knowledge-Based Systems
Journal Article,"L., Sheehan Masello, B., Castignani, G., Guillen, M., Murphy, F.",2025,Predictive Modeling for Driver Insurance Premium Calculation Using Advanced Driver Assistance Systems and Contextual Information,Institute of Electrical and Electronics Engineers (IEEE),26,2,2202–2211,"Telematics devices have transformed driver risk assessment, allowing insurers to tailor premiums based on detailed evaluations of driving habits. However, integrating Advanced Driver Assistance Systems (ADAS) and contextualized geolocation data for predictive improvements remains underexplored due to the recent emergence of these technologies. This article introduces a novel risk assessment methodology that periodically computes weekly insurance premiums by incorporating ADAS risk indicators and contextualized geolocation data. Using a naturalistic dataset from a fleet of 354 commercial drivers over a year, we modeled the relationship between past claims and driving data, and use that to compute weekly premiums that penalize risky driving situations. Risk predictions are modeled through claims frequency using Poisson regression and claims occurrence probability using machine learning models, including XGBoost and TabNet, and interpreted with SHAP. The dataset is divided into weekly profiles containing aggregated driving behavior, ADAS events, and contextual attributes. Results indicate that both modeling approaches show consistent attribute impacts on driver risk. For claims occurrence probability, XGBoost achieved the lowest Log Loss, reducing it from 0.59 to 0.51 with the inclusion of all attributes; for claims frequency, no statistically significant differences were observed when including all attributes. However, adding ADAS and contextual attributes allows for a comprehensive and disaggregated interpretation of the resulting weekly premium. This dynamic pricing can be incorporated into the insurance lifecycle, enabling bespoke risk assessment based on emerging technologies, the driving context, and driver behavior.",,CEM,Risk and Uncertainty,Advanced driver assistance systems; explainable artificial intelligence; generalized linear models; machine learning; risk assessment,IEEE Transactions on Intelligent Transportation Systems (T-ITS)
Journal Article,"A., Matos Matta, L. M., Silva, J. M., Bastos Gomes, M., Pilastri, A., Cortez, P.",2025,A predictive analytics framework for early detection of production halts and quality issues,Elsevier Inc.,16,,,"This study presents a Machine Learning (ML) framework for an Ahead-of-Time (AoT) prediction of production halts and defects in particleboard manufacturing that uses only pre-production input variables. The proposed approach incorporates both Single-Task Learning (STL) and Multi-Task Learning (MTL) paradigms, which are evaluated across three production lines under two modeling strategies: Line-Specific Modeling (LSM) and Line-Agnostic Modeling (LAM). The experimental evaluation benchmarks a lightweight Logistic Regression (LogR) model against three Automated Machine Learning (AutoML) techniques: H2O AutoML, Ludwig, and a Bayesian-optimized Deep Feedforward Network (DFFN). Results show that the STL-LSM combination using LogR achieves the highest overall predictive performance. To enhance model interpretability, we apply two model-agnostic eXplainable Artificial Intelligence (XAI) techniques: SHapley Additive exPlanations (SHAP) and One-Dimensional Sensitivity Analysis (1DSA). These methods generate feature importance rankings across targets and production lines, which are evaluated using quantitative (normalized distance metrics) and qualitative measures (alignment with domain expert insights). The XAI findings reveal a strong consistency between SHAP and 1DSA, with 1DSA requiring a substantially lower computational cost. Moreover, the convergence between model-derived interpretations and expert feedback highlights the practical relevance of the proposed ML framework in supporting data-driven decision-making for particleboard production planning. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010932446&doi=10.1016%2Fj.dajour.2025.100607&partnerID=40&md5=db8dcb8c45a76356759643fb2f55075f,CEM,Productivity / Workforce,Explainable Artificial Intelligence; Machine Learning; Quality control; Industry 4.0; Predictive modeling; Process optimization,Decision Analytics Journal
Journal Article,"C., Qu Meng, D., Duan, X.",2024,Cost Estimation of Metro Construction Projects Using Interpretable Machine Learning,American Society of Civil Engineers (ASCE),38,6,,"The metro, renowned as an environmentally friendly mode of transportation due to its low energy consumption and minimal pollution, plays a crucial role in achieving sustainable urban growth. Due to the scarcity of information in the early stages of metro construction projects and the subjectivity of cost estimation (which relies heavily on the estimator's experience), it is always difficult to guarantee the accuracy of metro construction project cost estimation. Furthermore, the existing methodological models commonly used for cost estimation do not adequately consider the interpretability of the estimation results, making it difficult to promote their application in real-world scenarios. In this paper, an interpretable machine learning method is introduced into the study of cost estimation of metro construction projects, and a maximum relevance and minimum redundancy (mRMR)-light gradient boosting machine (LightGBM)-Shapley additive explanations (SHAP) interpretable assisted investment decision-making framework is proposed. The results show that the negative impact of variable multicollinearity on model prediction is avoided by quantitatively identifying the key driving variables of costing through mRMR based on the historical data of metro construction projects and macroeconomic data. LightGBM is employed to predict the cost of metro construction projects with a mean absolute percentage error of 13.00%, surpassing the accuracy of the five baseline models. The SHAP method's introduction explains the influence of key driving variables on the model prediction response at both global and local levels, which improves the decision trust of the cost estimation of metro construction projects. The study takes into account the impact of key driving variables on the model's prediction response in a real-world context and balances the needs for estimation accuracy and variable interpretability in real-world scenarios. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202615747&doi=10.1061%2FJCCEE5.CPENG-6018&partnerID=40&md5=54023f90c0353c6919dc281daaaa009e,CEM,Cost Estimation / Bidding,Metro construction projects; Cost estimation; Interpretable machine learning; Light gradient boosting machine (LightGBM); Shapley additive explanations (SHAP),Journal of Computing in Civil Engineering
Journal Article,"S. Q., Shi Meng, Z. M., Ouyang, X. W., Zhao, Y. Z., Xia, C. Z.",2025,Compressive strength and flowability of high volume eco-binder high performance concrete: A physical-data dual drive,Elsevier Ltd,74,,,"This study explores the influence of high-volume eco-binders, including ground granulated blast furnace slag (BFS) and fly ash (FLA), on the performance of high-performance concrete (HPC) through physical-data dual drive. Initially, the basic properties of the mixtures are assessed via compressive strength and slump tests. Nanoindentation experiments are conducted to examine the interfacial transition zone (ITZ) properties. The findings indicate that mixtures incorporating BFS or FLA demonstrate optimal improvements in compressive strength, flowability, and ITZ performance at substitution rates between 10 % and 30 % under the same curing conditions. However, substitution rates of 50 % or higher result in reduced performance. Machine learning techniques play a central role in this research because they can accurately predict specific properties. Data distribution analysis is utilized to guide the development of ensemble learning models. The integration of SHapley Additive Explanations (SHAP) enhances model interpretability. XGBoost outperforms traditional models, including LR, BP, KNN, SVR, and ANN. Specifically, XGBoost achieves an RMSE of 4.557, MAE of 2.687, MAPE of 8.647 %, and R2 of 0.929 for compressive strength, and an RMSE of 3.495, MAE of 1.405, MAPE of 0.938 %, and R2 of 0.968 for flowability. SHAP analysis identifies Age and Admixtures as the most significant factors influencing both compressive strength and flowability in HPC. This study provides valuable insights into improving the performance of eco-binder high-performance concrete, demonstrating the potential of machine learning in predicting and optimizing concrete performance and offering a data-driven approach to support the sustainable development of eco-friendly construction materials.",,CE,Materials,Prediction; Ensemble learning; Machine learning; Interpretable artificial intelligence; Feature importance,Structures
Journal Article,"Z., Kazanavičius Meškauskas, E.",2022,About the New Methodology and XAI-Based Software Toolkit for Risk Assessment,Multidisciplinary Digital Publishing Institute (MDPI),14,9,,"There are different approaches in different areas of what the risk is. ISO 31000 risk management standards describe risk as the effect of uncertainty on objectives. Many existing risk assessment procedures are based on the assumption that risk is the amount of any damage or loss multi-plied by the probability of an event that could cause the damage. We are proposing a new risk approach, based on Hillson’s positive risk philosophy, that risk is not just a threat but also a com-position of new opportunities, efforts that need to be put in, and uncertainty. For this approach, we composed a risk formula and a methodology based on that formula. A prototypical software tool was developed, and an experiment was performed using this tool to evaluate the risk of several interconnected projects and validate the developed risk assessment methodology. It should be mentioned that, in the methodology, the decision-making process is performed traceably; therefore, it can be stated that it has explainable artificial intelligence (XAI) traits. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130046076&doi=10.3390%2Fsu14095496&partnerID=40&md5=73f7e2b2322e9af072334408d63e2d15,CEM,Risk and Uncertainty,dynamic SWOT analysis;computing with words; fuzzy SWOT maps; XAI-based system analysis; SWOT+CWW network; risk analysis,Sustainability
Journal Article,"Q., Gao Miao, Z., Zhu, K., Guo, Z., Sun, Q., Zhou, L.",2025,Interpretable machine learning model for compressive strength prediction of self-compacting concrete with recycled concrete aggregates and SCMs,Elsevier Ltd,108,,,"The composite comprising waste concrete and industrial by-products for producing recycled aggregate self-compacting concrete (RA-SCC) mitigates natural resource depletion and reduces greenhouse gas emissions, thus offering significant environmental benefits. This study aims to predict the compressive strength of RA-SCC by using five interpretable machine learning (ML) methods including M5P model tree, Support Vector Regression (SVR), Multilayer Perceptron (MLP), Random Forest (RF) and Extreme Gradient Boosting (XGBoost). To develop the ML models, a dataset containing 454 samples was constructed, encompassing experimental data with varying material dosages, curing ages, and recycled aggregate properties. The hyperparameters were optimized using Bayesian optimization and 10-fold cross-validation. Additionally, the SHapley Additive exPlanations (SHAP) method was employed to assess the importance and contributions of the input parameters affecting compressive strength. The results indicated that the XGBoost model achieved the highest accuracy among the tested models, with R2 = 0.909, RMSE = 4.99 MPa, and MAE = 3.54 MPa. SHAP analysis revealed that cement content and age were the most influential factors positively affecting compressive strength predictions, while fly ash and recycled concrete aggregate content exhibited the highest negative effects. The ML models proposed in this study provide a systematic evaluation of the compressive strength predictions for RA-SCC and offer valuable insights for concrete mix design and quality control. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005604071&doi=10.1016%2Fj.jobe.2025.112965&partnerID=40&md5=36ec906422ba072c5c5edaab1a908cdc,CE,Materials,Recycled aggregate self-compacting concrete; Compressive strength; Machine learning;XGBoost; Bayesian optimization,Journal of Building Engineering
Journal Article,"A. M., Islam Momshad, M. H., Datta, S. D., Sobuz, M. H. R., Aayaz, R., Kabbo, M. K. I., Khan, M. M. H.",2025,Assessing the engineering properties and environmental impact with explainable machine learning analysis of sustainable concrete utilizing waste banana leaf ash as a partial cement replacement,Elsevier Ltd,24,,,"The production of cement is crucial for sustainable building construction but is costly and contributes significantly to environmental pollution. To address these issues, this research investigates the potential of partially replacing cement with waste banana leaf ash (BLA), a pozzolanic material, in concrete mixtures. The aim of this study is to evaluate the fresh, mechanical, durability, and microstructural properties of BLA-incorporated concrete, alongside its environmental and economic impacts. Experimental methods assessed the workability (slump and compaction factor), mechanical properties (compressive, split-tensile strength, and modulus of elasticity), and durability aspects through Rapid Chloride Penetration Test (RCPT), Water Permeability Test (WPT), sorptivity, and electrical resistivity tests. Results indicate that as BLA content increases, workability declines. Mechanically, BLA concrete generally shows reduced performance; however, the 10% BLA mix improves strength compared to other BLA percentages. Durability tests reveal significant improvements from 28 to 91 days for the 10% BLA mix, with RCPT, WPT, sorptivity, and resistivity increasing by 22.44%, 19.55%, 49.02%, and 18.72%, respectively. Microstructural analysis via Scanning Electron Microscopy (SEM) indicates a decrease in ettringite formation with BLA. Additionally, machine learning models were applied to predict compressive strength, identifying influential factors using SHapley additive explanations (SHAP) and Partial Dependence Plot (PDP) analyses. The 10% BLA concrete mix achieved a 14.78% reduction in Global Warming Potential (GWP) compared to conventional concrete, with a GWP of 426.29 kgCO2-eq/kg, and proved cost-effective. These findings suggest that a 10% cement replacement with BLA presents an optimal balance of environmental sustainability, cost-effectiveness, and strength for construction housing sector. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214807947&doi=10.1016%2Fj.clet.2025.100886&partnerID=40&md5=84866644a3415368d36bb455a5175dd8,CE,Materials,Banana leaf ash; Pozzolanic material; Durability properties; Sustainable construction materials; Microscopic analysis,Cleaner Engineering and Technology
Journal Article,"L., Carrasco-González Monje, R. A., Rosado, C., Sánchez-Montañés, M.",2022,Deep Learning XAI for Bus Passenger Forecasting: A Use Case in Spain,Multidisciplinary Digital Publishing Institute (MDPI),10,9,,"Time series forecasting of passenger demand is crucial for optimal planning of limited resources. For smart cities, passenger transport in urban areas is an increasingly important problem, because the construction of infrastructure is not the solution and the use of public transport should be encouraged. One of the most sophisticated techniques for time series forecasting is Long Short Term Memory (LSTM) neural networks. These deep learning models are very powerful for time series forecasting but are not interpretable by humans (black-box models). Our goal was to develop a predictive and linguistically interpretable model, useful for decision making using large volumes of data from different sources. Our case study was one of the most demanded bus lines of Madrid. We obtained an interpretable model from the LSTM neural network using a surrogate model and the 2-tuple fuzzy linguistic model, which improves the linguistic interpretability of the generated Explainable Artificial Intelligent (XAI) model without losing precision. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129757487&doi=10.3390%2Fmath10091428&partnerID=40&md5=05a9331805ff5b3b281263ff7d6b668a,CE,Transportation,2-tuple fuzzy model; deep learning; LSTM; passenger forecasting; smart city; surrogate model; time series; XAI,Mathematics
Journal Article,"S., Boamah Mukherjee, E. F., Ganguly, P., Botchwey, N.",2021,A multilevel scenario based predictive analytics framework to model the community mental health and built environment nexus,Springer Nature,11,1,,"The built environment affects mental health outcomes, but this relationship is less studied and understood. This article proposes a novel multi-level scenario-based predictive analytics framework (MSPAF) to explore the complex relationships between community mental health outcomes and the built environment conditions. The MSPAF combines rigorously validated interpretable machine learning algorithms and scenario-based sensitivity analysis to test various hypotheses on how the built environment impacts community mental health outcomes across the largest metropolitan areas in the US. Among other findings, our results suggest that declining socio-economic conditions of the built environment (e.g., poverty, low income, unemployment, decreased access to public health insurance) are significantly associated with increased reported mental health disorders. Similarly, physical conditions of the built environment (e.g., increased housing vacancies and increased travel costs) are significantly associated with increased reported mental health disorders. However, this positive relationship between the physical conditions of the built environment and mental health outcomes does not hold across all the metropolitan areas, suggesting a mixed effect of the built environment's physical conditions on community mental health. We conclude by highlighting future opportunities of incorporating other variables and datasets into the MSPAF framework to test additional hypotheses on how the built environment impacts community mental health.",,CE,Environmental,NaN,Scientific Reports
Journal Article,"T., Nakagawa Namba, T., Isoda, H., Inayama, M., Tamura, A., Miyata, Y., Kado, Y.",2025,PARAMETER IDENTIFICATION OF NON-LINEAR RESPONSE ANALYSIS MODEL FOR FULL-SCALE FIVE STORY WOODEN BUILDING,Architectural Institute of Japan,31,77,222–227,"In this paper, parameter identification using interpretable machine learning and quality engineering is applied to a shaking table test data of a full-scale 5-story wooden structure. The following results are shown; 1) By the parameter identification for detailed analysis model, good agreement with experimental result is obtained. 2) It was confirmed that the difference between the analytical results and the experimental results becomes smaller when the capacity of some walls and other elements was increased. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218643439&doi=10.3130%2Faijt.31.222&partnerID=40&md5=262a8ced26300e220a5c3ab4b15a08e5,CE,Structural,Interpretable machine learning; Nonlinear response; Parameter identification; Quality engineering; Wooden building,AIJ Journal of Technology and Design
Journal Article,"H., Ciari Naseri, F., Isler, C. A.",2025,Influence of pedestrianization on travel behavior changes: a case study of Montreal,Elsevier Ltd,44,,,"Introduction: Pedestrianization promotes active modes of transportation and provides many benefits related to the environment, economy, health, and mobility. Nonetheless, it has often faced widespread opposition from residents and business owners. Therefore, it is essential to examine the effectiveness of pedestrianization programs (i.e., transforming streets into car-free zones). Objectives: This study investigates the influence of different variables on pedestrianization effectiveness in Montreal, Canada. The effectiveness of pedestrianization is evaluated in terms of frequency and duration of walking trips, duration spent in street shops, and route change. Methods: An online survey was distributed in Montreal. A powerful machine learning method (XGBoost) is used for modeling, and two interpretation techniques (SHapley Additive exPlanations and Partial Dependence Plots) are used to interpret the results of XGBoost. The performance of the developed interpretable machine learning approach is compared with Ordinal Logistic Regression. Results: The top variables impacting the effectiveness of pedestrianization are the level of agreement in redoing pedestrianization projects every year, the opinion on the influence of pedestrianization on individual mobility, the level of satisfaction with urban furniture of pedestrian streets, the level of satisfaction with the attractiveness of pedestrian streets, and age. Conclusions: Positive attitudes toward pedestrianization and active travel satisfaction are the top determining factors in supporting pedestrianization, and they play a vital role in the effectiveness of pedestrianization programs. Therefore, improving the urban furniture and the attractiveness of car-free streets increases the effectiveness of these projects. Among socio-demographics, age is the top variable, and pedestrianization is the most effective for individuals between 33 and 45 years. Accordingly, policymakers should prioritize the implementation of such projects in areas with the highest concentration of this age group to maximize their effectiveness. Further, travel behavior changes are highly dependent on the trip purpose and the built environment. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010119680&doi=10.1016%2Fj.jth.2025.102123&partnerID=40&md5=403b4f3ad03d9c9ed8e429f3c58c60d6,CE,Transportation,Pedestrianization; Ensemble learning; Public support,Journal of Transport & Health
Journal Article,"S., Lu Naumets, M.",2021,Investigation into Explainable Regression Trees for Construction Engineering Applications,American Society of Civil Engineers (ASCE),147,8,,"The logic of an artificial intelligence (AI) model derived from machine learning algorithms and domain-specific data is analogous to an expert's perception of a complex problem. Human insight based on know-how and experience also provides the best clue to verify such analytical models generalized from data. To facilitate the acceptance and implementation of AI by industry professionals, we explored the least complicated form of model that still is sufficient to represent the complexities of real-world problems. This research established a framework to apply the M5P model tree in the context of producing explainable AI for practical applications. The explanatory information derived from M5P (a decision tree with linear regressions at leaf nodes) is instrumental in explaining how the more complicated AI model reasons for the same problem, illuminating the sufficiency of problem definition and data quality, and distinguishing valid submodels from invalid ones in the obtained model tree. A steel fabrication labor cost-estimating case and a concrete strength development case were given for method validation and application demonstration. © 2021 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107286076&doi=10.1061%2F%28ASCE%29CO.1943-7862.0002083&partnerID=40&md5=411870d721ca67db2f0a4ef34ca10717,CEM,Cost Estimation / Bidding,NaN,Journal of Construction Engineering and Management
Journal Article,"N. M., Cao Nguyen, M. T.",2025,Energy use intensity analysis of office buildings using green BIM-integrated Interpretable machine learning,Elsevier Ltd,108,,,"Accurately predicting the energy use intensity (EUI) of office buildings is essential for optimizing energy efficiency and implementing sustainable building practices. This study integrated green building information modelling (green BIM) with advanced machine learning (ML) techniques to develop EUI prediction models tailored to Taiwanese office buildings. A data set of 1006 unique BIM models was generated using Green Building Studio, which calculated the EUI for diverse configurations of architectural and environmental variables. The forensic-based investigation algorithm was employed for hyperparameter optimization, and it considerably enhanced the performance of the developed boosting-based ensemble models. Of all models, XGBoost exhibited the highest accuracy and robustness, achieving the lowest root-mean-square error (37.98 ± 11.81 MJ/m2/year) and mean absolute percentage error (1.86 % ± 0.50 %) for the test set, thereby outperforming the random forest, bagging, and adaptive weighted blended ensemble models. Feature importance analysis, conducted using the SHapley Additive exPlanations technique, revealed that the number of stories, geographical location, and floor area are the factors most strongly affecting EUI, consistent with real-world expectations. Overall, this study highlights the effectiveness of combining green-BIM-driven simulations, advanced ML models, and interpretability techniques to address energy efficiency challenges in office buildings. The findings provide actionable insights for architects, engineers, and policymakers seeking to design energy-efficient buildings and implement targeted energy-saving strategies. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004349207&doi=10.1016%2Fj.jobe.2025.112760&partnerID=40&md5=2068de461562ca272b01af757843c0e4,CE,Energy,Energy use intensity; Energy-efficient buildings; Taiwan office buildings; Building information modelling; Machine learning; Ensemble model,Journal of Building Engineering
Journal Article,"X. H., Phan Nguyen, Q. M., Nguyen, N. T., Tran, V.",2024,Interpretable machine learning model for evaluating mechanical properties of concrete made with recycled concrete aggregate,John Wiley and Sons Inc,25,4,2890–2914,"The main objective of this paper is to use the data-driven approach to predict and evaluate the mechanical properties of concrete made with recycled concrete aggregate (RCA), including compressive strength and elastic modulus. Using 358 data samples, including 10 input variables, 10 popular machine learning (ML) algorithms are introduced to select the best ML performance model for predicting RCA concrete's compressive strength and elastic modulus. Gradient Boosting and Categorial Boosting have the best performance in predicting the compressive strength of RCA concrete, with R-2 = 0.9112, RMSE = 5.3464 MPa, MAE = 4.0845 MPa, and R-2 = 0.9175, RMSE = 5.1520 MPa, MAE = 3.7567 MPa, respectively. Light Gradient Boosting and Categorial Boosting have the best performance in predicting the elastic modulus of RCA concrete, with R-2 = 0.8775, RMSE = 2.3560 GPa, MAE = 1.8330 GPa, and R-2 = 0.9300, RMSE = 2.3560 MPa, MAE = 1.2589 MPa, respectively. Based on the Shapley Additive Explanation analysis, the influence of main factors on compressive strength and elastic modulus of RCA concrete values has been analyzed qualitatively and quantitatively. RCA replacement level and cement/sand ratio slightly affect compressive strength but have a dominant influence on the elastic modulus of RCA concrete.",,CE,Materials,compressive strength; elastic modulus; interpretable machine learning; RCA replacement level; recycled concrete aggregate,Structural Concrete
Journal Article,"M., Khattak Noman, A., Alam, Z., Yaqub, M., Noroozinejad Farsangi, E.",2024,Predicting the Residual Compressive Strength of Concrete Exposed to Elevated Temperatures Using Interpretable Machine Learning,American Society of Civil Engineers (ASCE),29,4,,"The accurate prediction of residual compressive strength (RCS) of concrete plays a critical role in assessing concrete constructions' safety and structural integrity following exposure to elevated temperatures. Existing ensemble models exhibit RCS prediction capabilities, yet they are constrained by their opaque nature. This research endeavors to develop an intelligible model for RCS by employing five ensemble machine-learning models, namely, random forest (RF), adaptive boosting (AdaBoost), gradient boosting (GBoost), light gradient boosting (LGBoost), and extreme gradient boosting (XGBoost), and integrating Shapley additive explanations (SHAP) to ascertain the precise importance of each input variable in forecasting the RCS of concrete under elevated temperature conditions. The input variables encompass concrete type, compressive strength, aggregate type, water-cement ratio, heating type, heating rate, maximum core temperature, and cooling type. Model performance is appraised using established performance metrics such as mean absolute error (MAE), mean squared error (MSE), root-mean squared error (RMSE), and coefficient of determination (R2). The analytical results exhibit the efficacy of employing machine-learning models in accurately predicting the RCS of concrete under elevated temperature conditions. Among the implemented models, XGBoost demonstrated the highest performance, yielding an R2 value of 0.876, closely trailed by the LGBoost model at 0.871. The SHAP analysis elucidates the crucial role of core temperature, water-cement ratio, heating rate, and compressive strength in determining the RCS of concrete. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197363671&doi=10.1061%2FPPSCFX.SCENG-1536&partnerID=40&md5=f976d6dbccea661e7b21d8e8285cf0fd,CE,Materials,Extreme gradient boosting (XGBoost); Shapley additive explanations (SHAP) analysis; Fire; Residual compressive strength (RCS); Core temperature,Practice Periodical on Structural Design and Construction
Journal Article,"S., Scalambrino Ntalampiras, A.",2025,Automatic Prediction of Disturbance Caused by Interfloor Sound Events,Institute of Electrical and Electronics Engineers Inc.,17,1,147–154,"There is a direct correlation between noise and human health, while negative consequences may vary from sleep disruption and stress to hearing loss and reduced productivity. Despite its undeniable relevance, the underlying process governing the relationship between unpleasant sound events, and the annoyance they may cause has not been systematically studied yet. In this context, this work focuses on the disturbance caused by interfloor sound events, i.e., the audio signals transmitted through the floors of a building. Activities such as walking, running, using household appliances or other daily actions generate sounds that can be heard by those on an adjacent floor. To this end, we implemented a suitable dataset including diverse interfloor sound events annotated according to the perceived disturbance. Subsequently, we propose a framework able to quantify similarities exhibited by interfloor sound events starting from standardized time-frequency representations, which are processed by a Siamese neural network composed of a series of convolutional layers. Such similarities are then employed by a k-medoids regression scheme making disturbance predictions based on interfloor sound events with neighboring latent representations. After thorough experiments, we demonstrate the effectiveness of such a framework and its superiority over popular regression algorithms. Last but not least, the proposed solution offers interpretable predictions, which may be meaningfully utilized by human experts. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198249694&doi=10.1109%2FTCDS.2024.3424457&partnerID=40&md5=f63172265e378bf02f54277e272a4012,CE,Environmental,Audio pattern recognition; building acoustics; disturbance assessment; emotion modeling; explainable artificial intelligence (AI); interpretable AI; mental health; Siamese neural network (SNN),IEEE Transactions on Engineering Management
Journal Article,"S., Gnekpe Nyawa, C., Tchuente, D.",2023,Transparent machine learning models for predicting decisions to undertake energy retrofits in residential buildings,Springer,,,,"Transparent and explainable machine learning (ML) models are essential in various domains, e.g., energy consumption, where decarbonization is the main challenge. The European Union is focusing on energy efficiency retrofits in residential buildings to help reach its 2050 carbon emissions target. The cost of these investments is often a strong factor, requiring decision-makers to understand the motivations driving homeowners’ decisions to undertake energy retrofits. Instead of hedonic models commonly used in operational management research studies, we rely on ML methods to predict homeowners’ decisions to undertake energy retrofits, using data from 51,000 households in France. We describe the data preparation, model training, and evaluation; results show that artificial neural networks outperform other popular ML techniques (91.4%). Our post hoc method based on sensitivity analysis and feature importance contributes to the transparency and interpretability of the results. We show that the type of public aid used, head of household gender, family size, prior knowledge of aid, urban vs. rural area, geographical location, occupancy status, and working status are the most important factors in the decision to undertake energy efficiency retrofits. Our predictive methods help decision-makers to make optimal decisions about the level, type, beneficiaries of public incentives for energy retrofits, and expected outcomes; companies in the construction sector can understand homeowners’ key motivations and optimally calibrate their strategic investments and operations. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147370407&doi=10.1007%2Fs10479-023-05217-5&partnerID=40&md5=e0cd4fb997a35866edd4433c70d96640,CEM,Sustainability (Construction),Energy efficiency retrofits; Machine learning transparency; Classification; Deep learning; Interpretability; France,Annals of Operations Research
Journal Article,"P., Menolotto O'Sullivan, M., Visentin, A., O'Flynn, B., Komaris, D. S.",2024,AI-Based Task Classification With Pressure Insoles for Occupational Safety,Institute of Electrical and Electronics Engineers (IEEE),12,,21347–21357,"Pressure insoles allow for the collection of real time pressure data inside and outside a laboratory setting as they are non-intrusive and can be simply integrated into industrial environments for occupational health and safety monitoring purposes. Activity detection is important for the safety and wellbeing of workers, and the present study aims to employ pressure insoles to detect the type of industry-related task an individual is performing by using random forest, an artificial intelligence-based classification technique. Twenty subjects wore loadsol (R) pressure insoles and performed five specific tasks associated with a typical workflow: standing, walking, pick and place, assembly, and manual handling. For each activity, statistical and morphological features were extracted to create a training dataset. The classifier performed with an accuracy of 82%, and a re-analysis focusing on the five most influential features resulted in 83% accuracy. These accuracies are comparable to similar task classification studies but with the benefit of added explainability, which increases transparency and, thereby, trust in the classifier decisions. The combination of random forest and in-depth feature analysis (SHAP) provided insights into the importance of certain features and the impact of their value on the classification of each task. The insights obtained from these methods can aid in the design of pressure insoles that are optimized for the extraction of impactful features and the prevention of work-related musculoskeletal disorders in Industry 4.0 operators.",,CEM,Safety,Human activity recognition; pressure insoles; explainable AI; industry 4.0,IEEE Access
Journal Article,"T. S., Ribeiro Oluwadare, M. P., Chen, D. M., Ataabadi, M. B., Tabesh, S. H., Daomi, A. E.",2025,Applying Machine Learning Algorithms for Spatial Modeling of Flood Susceptibility Prediction over São Paulo Sub-Region,Multidisciplinary Digital Publishing Institute (MDPI),14,5,,"Floods are among the most destructive natural hazards globally, necessitating the identification of flood-prone areas for effective disaster risk management and sustainable urban development. Advanced data-driven techniques, including machine learning (ML), are increasingly used to map and mitigate flood risks. However, ML applications for flood risk assessment remain limited in Sorocaba, a sub-region of S & atilde;o Paulo, Brazil. This study employs four ML algorithms-differential evolution (DE), na & iuml;ve Bayes (NB), random forest (RF), and support vector machines (SVMs)-to develop flood susceptibility models using 16 predictor variables. Key categorical factors influencing flood susceptibility included topographical, anthropogenic, and hydrometeorological, particularly elevation, slope, NDVI, NDWI, and distance to roads. Performance metrics (F1-score and AUC) showed strong results, ranging from 0.94 to 1.00, with the DE and RF models excelling in training, testing, and external datasets. The study highlights model transferability, demonstrating applicability to other regions. Findings reveal that 41% to 50% of Sorocaba is at high flood risk. The explainable artificial intelligence technique Shapley additive explanations (SHAP) further identified moisture and the stream power index (SPI) as significant factors influencing flood occurrence. The study underscores the ML-based model's potential in highlighting flood-vulnerable areas and guiding flood mitigation strategies, land-use planning, and infrastructure resilience.",,CE,Water,flood spatial modeling; flood susceptibility mapping; machine learning; natural hazards; flood prediction,Land
Journal Article,"K. C., Hanandeh Onyelowe, S., Kamchoom, V., Ebid, A. M., Imran, H., Vaca, M. A. D., Morales, G. C. H., Ulloa, N., Arunachalam, K. P.",2025,Data-driven framework for prediction of mechanical properties of waste glass aggregates concrete,Nature,15,1,,"This research presents a novel data-driven framework for predicting the mechanical properties of waste glass aggregate concrete using six advanced metaheuristic optimization algorithms: Bat Algorithm (Bat), Cuckoo Search Algorithm (Cuckoo), Elephant Herding Optimization (Elephant), Firefly Algorithm (Firefly), Rhinoceros Optimization Algorithm (Rhino), and Gray Wolf Optimizer (Wolf). The study evaluates these models based on their ability to predict compressive strength (Fc), tensile strength (Ft), density, and slump using key statistical performance indicators such as SSE, MAE, MSE, RMSE, accuracy, R2, and KGE. Sensitivity analysis was conducted using Hoffman and Gardener's method as well as the SHAP technique to determine the most influential parameter in the prediction process. Results indicate that the Firefly and Wolf algorithms exhibited the highest prediction accuracy across all four properties, with Wolf emerging as the overall best-performing model due to its superior generalization ability, lower error rates, and high correlation with experimental results. Among the input parameters, the water-to-binder ratio was identified as the most influential factor affecting the mechanical properties of waste glass aggregate concrete, as demonstrated by both sensitivity analysis methods. This highlights the critical role of optimal water content in achieving desirable strength and workability in sustainable concrete mixtures. The study's novelty lies in the comparative assessment of multiple optimization algorithms applied to waste-based concrete, an approach that has not been extensively explored in previous research. Additionally, the integration of SHAP analysis for feature importance ranking provides an interpretable machine learning approach to concrete mix design, which enhances decision-making for engineers and researchers. The practical implications of this research extend to sustainable machine learning-based concrete design, where AI-driven optimization can help reduce the reliance on conventional trial-and-error methods. By utilizing waste glass aggregates, the study supports circular economy initiatives in construction, reducing environmental impact while maintaining structural performance. The proposed models can be implemented in real-world scenarios to optimize mix designs for large-scale applications, leading to cost-effective and eco-friendly construction materials. This research advances the field of smart construction by demonstrating the effectiveness of machine learning in sustainable material engineering, paving the way for future AI-assisted innovations in the industry.",,CE,Materials,Mechanical properties; Metaheuristic machine learning; Sensitivity analysis; Sustainable construction; Waste glass aggregate concrete,Scientific Reports
Journal Article,"B. L., Nguyen Oo, A. T., Ahn, Y., Lim, B. T. H.",2025,Predicting the number of bidders in construction competitive bidding using explainable machine learning models,Emerald Publishing,25,7,158–188,"Purpose: The number of bidders in upcoming tenders has important managerial implications for both construction clients and contractors in their decision-making in the competitive bidding process. However, there is a stagnation of research efforts on predicting the number of bidders with only a handful of studies over the past decades, which mainly focused on statistical distribution of the number of bidders. This study aims to provide a new perspective of predicting the number of bidders using machine learning (ML) algorithms. Design/methodology/approach: This study adopted a case study approach with a bidding dataset of public sector construction projects in Singapore. Six ML models were developed, and linear regression was used as a baseline model is assessing the predictive performance of ML models. Findings: The results show that ML models outperform the baseline linear regression model, in which XGBoost is the best performing model of R2 which is two times higher than the linear regression model. In addition, economic-related factors play a vital role in this prediction problem. Research limitations/implications: While the predictive performance of the developed ML models is relatively low, it indicates the challenges and complexities in this prediction problem, even with the use of artificial intelligent techniques. Originality/value: Being a pioneering work, this study sets a foundation for the use of ML models in this prediction problem and offers insights for future modelling attempts towards the development of a decision support system for construction clients and contractors. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004649144&doi=10.1108%2FCI-10-2024-0325&partnerID=40&md5=de75ffc069c23a610ef11480f654c102,CEM,Cost Estimation / Bidding,Number of bidders; Bidding; Construction; Machine learning; Prediction; Procurement; Tendering; Digitalization; Construction bidding,Construction Innovation
Journal Article,"C., Guo Pan, J., Li, H., Wu, J., Qiu, N., Wu, S.",2025,Study on the Influence Mechanism of Machine-Learning-Based Built Environment on Urban Vitality in Macau Peninsula,Multidisciplinary Digital Publishing Institute (MDPI),15,9,,"Clarifying the mechanisms by which the micro-scale built environment influences urban vitality is an important scientific challenge, to guide precise urban planning in the context of urban renewal. In this study, we quantify the intensity of human activities through Baidu heat maps, analyze social interaction patterns using social media check-in data, and integrate built environment elements such as road network topology, 3D building morphology, and the spatial distribution of points of interest (POIs). A machine learning technique combining a real-encoded Accelerated Genetic Algorithm-Projective Pathfinding Model (RAGA-PPM) and Shapley Additive Projection for Interpretability (SHAP) for Interpretability Analysis (IPA) was used to investigate the nonlinear mechanisms of 17 factors affecting urban vitality in Macau Peninsula, China. Firstly, the explanatory power of the built environment for comprehensive vitality was significantly better than the other dimensions. Two factors, population vitality and microblogging check-in vitality, contributed the most to the composite vitality value. Secondly, road network density was the most important built environment factor affecting urban vitality in Macau Peninsula (SHAP = 0.025). Finally, the impacts of built environment factors on urban vitality showed nonlinearities, and the threshold effects of the core factors (road network density, spatial fractal dimension, and openness to the sky) showed a consistent neighborhood-level pattern. This study establishes a framework for micro-vitality mechanisms in high-density cities, addressing the limitations of traditional methods in modeling complex nonlinear relationships. The methodological integration of RAGA-PPM and SHAP advances the innovative paradigm of applying interpretable machine learning to the study of urban form. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004861323&doi=10.3390%2Fbuildings15091557&partnerID=40&md5=71692d6c94f24250d71081d2bf63aed2,CE,GIS / Remote Sensing,built environment; urban vitality; projection pursuit; genetic algorithm; Macau Peninsula,Buildings
Journal Article,"Y. Pei, Y. Wen, S. Pan",2024,Road Traffic Accident Risk Prediction and Key Factor Identification Framework Based on Explainable Deep Learning,Institute of Electrical and Electronics Engineers Inc.,12,,120597–120611,"The prediction and identification of key factors in road traffic accidents are crucial for accident prevention, yet previous studies have often examined these aspects separately. To comprehensively assess the risk level of road traffic accidents and their key determinants, this paper proposes a comprehensive forecasting and analysis framework that offers a novel perspective for identifying key risk factors from a modeling standpoint compared to existing methods. The CNN-BiLSTM-Attention model was developed for predicting the risk value of road accidents, and DeepSHAP was employed to interpret the model and extract the key factors contributing to traffic accidents. This deep learning framework combines convolutional neural networks (CNN) and Bi-directional long short-term memory (BiLSTM), while incorporating a spatial-temporal local attention mechanism to enhance its capability in capturing spatiotemporal features. Through analysis and experimentation on real-world datasets, our model demonstrates superior accuracy in predicting traffic accident risk compared to the benchmark model, achieving a Mean Absolute Error (MAE) of 0.2475 on the UK dataset and 0.2683 on the US dataset. The results obtained from DeepSHAP were found to be more rational and informative in identifying key factors of different severity levels using four methods. To verify the rationality and stability of obtaining these key factors, the first 15 factors were reintegrated into the prediction model, resulting in almost unchanged accuracy and reduced model iteration time. By improving the influential factors, road traffic accidents can be effectively mitigated.",,CE,Transportation,Traffic accidents; risk prediction; explainability; deep learning,IEEE Access
Journal Article,"S. K., Dwibedy Parhi, S., Patro, S. K.",2025,Managing waste for production of low-carbon concrete mix using uncertainty-aware machine learning model,Elsevier Inc.,279,,,"This study introduces an uncertainty-aware AI-driven optimization framework for designing sustainable concrete mixtures that incorporate waste-derived materials. The primary objectives are to reduce global warming potential (GWP) and promote a circular economy within the construction sector while preserving mechanical characteristics. A comprehensive dataset comprising 3114 unique concrete mix designs was developed from peer-reviewed literature, encompassing a wide range of parameters including cement (102-783 kg/m3), water (80-247 kg/m3), various supplementary cementitious materials (SCMs) (0-715 kg/m3), waste-derived fine aggregates (0-740 kg/m3), coarse aggregates (0-1095 kg/m3), natural coarse (0-1380 kg/m3), and fine aggregates (0-992 kg/m3). The Explainable Boosting Regressor (EBR) demonstrated the highest predictive accuracy, with R2 values of 0.91 for compressive strength and 0.96 for GWP. The Weighted Jackknife + method provides narrow, localized uncertainty bounds, thereby enhancing the model reliability for heterogeneous mixes. Analyses of feature importance and shape functions identified cement, curing age, water, and GGBS as the most influential parameters. High-strength mixes are associated with a greater environmental impact owing to increased cement usage. A multi-objective optimization approach was employed, utilizing EBR as the objective function for compressive strength and GWP, whereas polynomial regression was used for the cost objective function to minimize the GWP and material cost while satisfying user-defined strength constraints. The framework incorporated nonlinear constraints in accordance with IS:10262-2019 standards. A graphical user interface (GUI) was developed for practical deployment. Among the evaluated combinations, the ternary blend of cement, GGBS, and waste glass achieved the highest waste utilization (960 kg/m3), whereas fly ash and metakaolin-based mixes demonstrated a greater avoided GWP while meeting the M35-grade requirements. These findings underscore the potential of interpretable uncertainty-aware AI tools for guiding sustainable concrete design at scale.",,CE,Materials,Sustainable concrete; Waste-derived materials; Explainable artificial intelligence; Global warming potential; Compressive strength prediction; Multi-objective optimization,Environmental Research
Journal Article,"N., Park Park, J., Lee, C.",2025,Conditional Generative Adversarial Network-Based roadway crash risk prediction considering heterogeneity with dynamic data,Elsevier Ltd,92,,217–229,"Introduction: Roadway crash data are very rare and occur randomly, therefore there are several challenges to developing a crash prediction model for real-time traffic safety management. Recently, to resolve the problem of crash data sample size, researchers have conducted studies on crash data augmentation using machine learning techniques for developing safety evaluation models. However, it's important to incorporate the specific characteristics of crash data into augmentation and crash risk assessment, as these characteristics vary depending on spatial and temporal conditions. Method: Therefore, this study developed a real-time crash risk model in three stages. First, crash data were clustered to define heterogeneous crash risk situations and then, key variables were derived by the ensemble and explainable artificial intelligence techniques, Boruta-SHAP. Second, augmentation of each clustered crash data was performed using oversampling techniques including Conditional Generative Adversarial Network (CGAN), which can consider each crash risk cluster's characteristics. Finally, crash risk models were developed and compared with other crash risk models developed by using binary logistic regression model (BLM), Random Forest (RF), extreme gradient boosting (XGBoost), and Support Vector Machine (SVM). Results: The results showed that the CGAN-based XGBoost model has the best performance and the variable of the temporal speed difference at 10-minute intervals and the precipitation variable have a large impact on crash risk prediction. This paper emphasizes that crash risk characteristics must be distinguished in crash risk prediction and provides new insights into addressing the imbalance data issue within crash and non-crash datasets. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211207184&doi=10.1016%2Fj.jsr.2024.12.001&partnerID=40&md5=71b8e146341c3511c4f397d0c41f861b,CE,Transportation,Data augmentation; Traffic safety; Crash risk prediction model; Machine learning; Explainable artificial intelligence,Journal of Safety Research
Journal Article,"S., Rahman Paul, L., Chara, A. H., Mahmuduzzaman, M., Kashem, A., Naim, M., Bhuiyan, R., Malo, S. C.",2025,Beam shear strength prediction of recycled aggregate concrete using explainable artificial intelligence,Springer Nature,,,,"The precise estimation of the shear strength of reinforced concrete (RC) beams constructed with recycled aggregate concrete (RAC) is essential for the secure and sustainable design of structural components. The utilization of construction and demolition waste (CDW) to produce recycled aggregate concrete (RAC) is an attractive approach from both an environmental and budgetary perspective. However, this study proposes a single and novel hybrid machine learning framework to predict the shear strength of RAC beams using a dataset compiled from published experimental studies and validated numerical models. Ensemble learning techniques such as Support Vector Regression (SVR), Gradient Boosted Regression Trees (GBRT), CatBoost, Decision Tree (DT), and Bagging Regressor (BR) were developed to train models using six input features: 28-day compressive strength of concrete (fc), the percentage of recycled coarse aggregate (RCA), the effective depth of the beam cross-section (d), the width of the beam cross-section (b), the percentage of longitudinal reinforcement (rhow), the shear span to effective depth ratio (a/d) and output parameter is the shear strength of the specimen (V<inf>test</inf>). Furthermore, evaluating model performance, we used R2, RMSE, MAPE, and MAE metrics on a robust database that was divided into training (70%) and testing (30%) phases. Results show that the hybrid models outperform standalone algorithms, with the hybrid GBRT model combination achieving the highest prediction accuracy throughout both stages using R2 (0.869, 0.998), RMSE, MAE, and MAPE (20.033, 12.753, and 15.598%), respectively. Additionally, Shapley Additive Explanations (SHAP) analysis was employed to determine significant input characteristics and clarify how they affect the Beam Shear Strength Prediction. The presence of the width of the beam cross-section and the shear span to effective depth ratio contributes the highest positive influence to the outcome. This study demonstrates that hybrid ML approaches can reliably capture nonlinear interactions among RAC beam variables, offering a powerful alternative to empirical formulas. Moreover, a Graphical User Interface (GUI) was developed to enable designers to effectively and economically forecast beam shear strength, and the experimental findings substantially impact the construction industry by facilitating a more accurate and reliable implementation of RAC. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013559890&doi=10.1007%2Fs42107-025-01512-7&partnerID=40&md5=aaaf82a2ef9e38a398bcbe82ebc06c64,CE,Structural,GUI; Machine learning; Recycled aggregate concrete; SHAP; Shear strength,Asian Journal of Civil Engineering
Journal Article,"M., Schiavina Pesaresi, M., Politis, P., Freire, S., Krasnodebska, K., Uhl, J. H., Carioli, A., Corbane, C., Dijkstra, L., Florio, P.",2024,Advances on the global human settlement layer by joint assessment of earth observation and population survey data,Taylor and Francis Ltd.,17,1,,"The Global Human Settlement Layer (GHSL) project fosters an enhanced, public understanding of the human presence on Earth. A decade after its inception in the Digital Earth 2020 vision, GHSL is an established project of the European Commission’s Joint Research Centre and an integral part of the Copernicus Emergency Management Service. The 2023 GHSL edition, a result of rigorous research on Earth Observation data and population censuses, contributes significantly to understanding worldwide human settlements. It introduces new elements like 10-m-resolution, sub-pixel estimation of built-up surfaces, global building height and volume estimates, and a classification of residential and non-residential areas, improving population density grids. This paper evaluates GHSL’s key components, including the Symbolic Machine Learning approach, using novel reference data. These data enable a comparative assessment of GHSL model predictions on the evolution of built-up surface, building heights, and resident population. Empirical evidence suggests that GHSL estimates are the most accurate in the public domain today (e.g. IoU 0.98 water class, 0.92 built-up class, 0.8 non-residential class, 6% MAE for the 100 m built-up surface or 2.27 m MAE for the building height, 83% TAA for resident population). The paper consolidates GHSL’s theoretical foundation and highlights its innovative features for transparent Artificial Intelligence, facilitating international decision-making processes. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202789083&doi=10.1080%2F17538947.2024.2390454&partnerID=40&md5=2f473d8023812b0d6c9247fe3e7da499,CE,GIS / Remote Sensing,GHSL; geospatial XAI; built-up surface; population grids; degree of urbanization; building height; built-up volume; urban land use; settlement morphology; built typology; Sentinel-2; Copernicus,International Journal of Digital Earth
Journal Article,"F., Jamil Qayyum, H., Alsboui, T., Hijjawi, M.",2024,Wildfire risk exploration: leveraging SHAP and TabNet for precise factor analysis,Springer Open,20,1,,"BackgroundUnderstanding the intricacies of wildfire impact across diverse geographical landscapes necessitates a nuanced comprehension of fire dynamics and areas of vulnerability, particularly in regions prone to high wildfire risks. Machine learning (ML) stands as a formidable ally in addressing the complexities associated with predicting and mapping these risks, offering advanced analytical capabilities. Nevertheless, the reliability of such ML approaches is heavily contingent on the integrity of data and the robustness of training protocols. The scientific community has raised concerns about the transparency and interpretability of ML models in the context of wildfire management, recognizing the need for these models to be both accurate and understandable. The often-opaque nature of complex ML algorithms can obscure the rationale behind their outputs, making it imperative to prioritize clarity and interpretability to ensure that model predictions are not only precise but also actionable. Furthermore, a thorough evaluation of model performance must account for multiple critical factors to ensure the utility and dependability of the results in practical wildfire suppression and management strategies.ResultsThis study unveils a sophisticated spatial deep learning framework grounded in TabNet technology, tailored specifically for delineating areas susceptible to wildfires. To elucidate the predictive interplay between the model's outputs and the contributing variables across a spectrum of inputs, we embark on an exhaustive analysis using SHapley Additive exPlanations (SHAP). This approach affords a granular understanding of how individual features sway the model's predictions. Furthermore, the robustness of the predictive model is rigorously validated through 5-fold cross-validation techniques, ensuring the dependability of the findings. The research meticulously investigates the spatial heterogeneity of wildfire susceptibility within the designated study locale, unearthing pivotal insights into the nuanced fabric of fire risk that is distinctly local in nature.ConclusionUtilizing SHapley Additive exPlanations (SHAP) visualizations, this research meticulously identifies key variables, quantifies their importance, and demystifies the decision-making mechanics of the model. Critical factors, including temperature, elevation, the Normalized Difference Vegetation Index (NDVI), aspect, and wind speed, are discerned to have significant sway over the predictions of wildfire susceptibility. The findings of this study accentuate the criticality of transparency in modeling, which facilitates a deeper understanding of wildfire risk factors. By shedding light on the significant predictors within the models, this work enhances our ability to interpret complex predictive models and drives forward the field of wildfire risk management, ultimately contributing to the development of more effective prevention and mitigation strategies. AntecedentesEl entender las complejidades de los impactos de los incendios de vegetacion a traves de los diversos paisajes geograficos, requieren de una detallada comprension de la dinamica del fuego y de las areas de vulnerabilidad, particularmente en regiones propensas y con alto riesgo de incendios. El aprendizaje automatico (Machine Learning o ML, en idioma ingles), aparece como un formidable aliado para abordar las complejidades asociadas con la prediccion y el mapeo de esos riesgos. Sin embargo, la confiabilidad de estos enfoques usando esta tecnica de aprendizaje automatico (ML) es altamente dependiente de la integridad de los datos y de la robustez de los protocolos de entrenamiento. La comunidad cientifica ha sembrado dudas sobre la transparencia e interpretacion de los modelos de ML en el contexto del manejo del fuego, reconociendo la necesidad de que esos modelos sean a su vez precisos y entendibles. La frecuentemente opaca naturaleza de los complejos algoritmos del ML, pueden oscurecer la racionalidad que debe haber por detras de los resultados, haciendose imperativo el priorizar la claridad e interpretacion para asegurar que los modelos de prediccion no solo sean precisos sino tambien procesables. Adicionalmente, una completa evaluacion de la performance del modelo en cuanto a los multiples factores criticos debe tenerse en cuenta para asegurar la utilidad y dependencia de los resultados en estrategias de practicas de supresion y manejo del fuego.ResultadosEste estudio devela un marco espacialmente sofisticado de aprendizaje profundo (deeep learnig) basado en tecnologia de TabNet, disenado especificamente para delinear areas susceptibles a incendios de vegetacion. Para dilucidar las interacciones predictivas entre los resultados del modelo y las variables contributivas a traves de un espectro de entradas, nos embarcamos en un analisis exhaustivo usando SHapley Additive exPlanations (SHAP). Esta aproximacion aborda un entendimiento granular sobre como las caracteristicas individuales influencian las predicciones del modelo. Ademas, la robustez del modelo predictivo fue rigurosamente validado 5 veces a traves de tecnicas de validacion cruzadas, asegurando la dependencia de los resultados. EL trabajo investigo meticulosamente la variabilidad espacial de la susceptibilidad dentro del estudio local, desenterrando ideas fundamentales sobre riesgo de incendios que implican matices distintivos de naturaleza local.ConclusionesUtilizando visualizaciones del SHapley Additive exPlanations (SHAP), esta investigacion identifico meticulosamente variables clave, cuantifico su importancia y desmitifico la mecanica de toma de decisiones del modelo. Los factores criticos, incluyendo temperatura, elevacion, el NDVI (indice Normalizado de Diferencias de Vegetacion), aspecto, y velocidad del viento, fueron descifrados para que tengan una influencia significativa en la prediccion de la susceptibilidad a los incendios. Los resultados de este estudio acentuan la critica a la transparencia de los modelos, lo que facilita un entendimiento mas profundo de los factores criticos del riesgo de incendios. Mediante el esclarecimiento de los predictores significativos del modelo, este trabajo aumenta nuestra habilidad para interpretar modelos predictivos complejos, e impulsa hacia adelante el campo del manejo del riesgo de incendios, contribuyendo en ultima instancia al desarrollo de estrategias de prevencion y mitigacion mas efectivas.",,CE,Environmental,Wildfire; TabNet; Machine learning; Explainable artificial intelligence; Shapely,Fire Ecology
Journal Article,"J. C., Zhao Qian, H. F., Wang, X. X., Wang, T., Liu, B. R., Feng, Z., Xue, C. L.",2024,"Insights from land sparing and land sharing frameworks for land productivity degradation governance in the Yangtze River Delta urban agglomeration, China",John Wiley and Sons Inc,35,14,4240–4256,"Land degradation due to mismanagement is widespread globally and may threaten the achievement of several UN Sustainable Development Goals. Yet the differences in land productivity degradation under various land management patterns (land sparing vs. land sharing) are poorly known. In this research, we used remote sensing data to develop a machine learning model for assessing the risk of land productivity degradation and interpreted the model using state-of-the-art interpretable artificial intelligence techniques. In 2018, the risk level of land productivity degradation in the agricultural production space of the Yangtze River Delta urban agglomeration (YRD) was 0.230. More than half of the area was at low risk (68.19% of the area), mainly in mountainous and hilly areas. The degradation risk of the land sharing management pattern is lower than that of the land sparing pattern, but there are significant differences among provinces/municipalities. The four most influential factors for land productivity degradation in YRD were Normalized Vegetation Difference Index, nighttime light, elevation, and nitrogen deposition, which together explained 72.75% of the degradation risk. This study provides a methodological framework for land degradation governance in emerging urban agglomerations. It strongly recommends that policymakers explore locally appropriate land management patterns based on regional contexts.",,CE,Environmental,influence factors; land degradation; land management pattern; machine learning; urban agglomeration,Land Degradation & Development
Journal Article,"Q. Y., Ren Qiao, C. Y., Chen, S. N., Tundokova, R., Lai, K. Y., Sarkar, C., Zhou, Y. L., Webster, C., Schuldenfrei, E.",2025,Associating COVID-19 prevalence and built environment design: An explainable machine learning approach,Elsevier B.V.,14,2,342–361,"Stay-at-home orders were globally adopted as one of the most important nonpharmaceutical interventions (NPIs) during the recent global pandemic. In a high-rise high-density context of Hong Kong, inter-building airborne transmissions were reported, especially in public housing. The role of residential building design in infection dynamics is under-studied. To unravel how architectural and urban design was linked to airborne virus transmission during the pandemic, we fitted explainable machine learning (EML) models associating COVID-19 prevalence with architectural design controlling for other built environment (BE) factors including socio-demographics, road information, land use, and points of interest (POIs). 284 public housing that underwent restrictiontesting declaration (RTD) during the peak period of the pandemic's fifth wave were our sample. An additional 35 RTD-issued private housing blocks were used for an initial comparison of infection prevalence across public and private housing. Our findings show a significant differential in prevalence over different design forms, with 8-"" and ""L-"" shaped buildings appearing to be more susceptible, with a significantly greater percentage of infections than ""X-"" and ""Y-"" shaped structures. The percentage of vacant land, public residential within a 500-m buffer, and the proportion of children ages under 14 at small tertiary planning unit level (STPU) were the three most influential co-variates in our model. Among specific architectural design features, the number of floors, radial layouts, and building corners were the most significantly associated with COVID-19 prevalence, followed by building average flat (apartment) size and shape factor. The study indicates that public housing residents were more at risk during this wave of the pandemic, which needs further investigation. Using machine learning, we provide insights into how to manage the design of high density neighbourhoods for resilience against airborne disease vectors.""",,CE,Environmental,COVID-19 prevalence; Densely city; Built environment; Architectural design; Explainable machine learning,Journal of Urban Management
Journal Article,"B., Tian Qin, T., Dou, W., Wu, H., Hao, M.",2025,The Association Between the Built Environment and Insufficient Physical Activity Risk Among Older Adults in China: Urban–Rural Differences and Non-Linear Effects,Multidisciplinary Digital Publishing Institute (MDPI),17,9,,"The built environment has been widely recognized as a critical determinant of physical activity among older adults. However, urban–rural disparities and the non-linear effects of environmental features remain underexplored. Using interpretable machine learning (random forest model) on nationwide representative data from 2526 older adults in the China Health and Retirement Longitudinal Study (CHARLS) database, this study identified both common and distinct risk factors for insufficient moderate-to-vigorous physical activity (MVPA) across diverse urban and rural contexts. The results revealed a location-based gradient in physical activity insufficiency: rural areas < suburban areas < central urban areas. Rural older adults faced greater constraints from safety concerns and transportation accessibility limitations. In comparison, urban older adults would benefit from targeted improvements in built environment quality, particularly elevator accessibility and diverse public activity spaces. Furthermore, non-linear relationships were observed between built environment features and physical activity, elucidating the “density paradox”: while moderate urban compactness promoted active behaviors, excessive density (>24,000 persons/km2), perceived overcrowding, and over-proximity to specific facilities (<1 km) were linked to reduced MVPA. These findings underscore the necessity for differentiated policy interventions in urban and rural settings to address the distinct environmental needs of older adults. Meanwhile, in urban planning, it is crucial that we balance spatial compactness and functional diversity within optimal thresholds for creating sustainable and inclusive built environments. Although a compact design may enhance mobility, equal attention must be paid to preventing spatial disorder from over-densification. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004896439&doi=10.3390%2Fsu17094035&partnerID=40&md5=b0f2d8c8736de7c764abc7546203d65e,CE,Environmental,built environment; physical activity; older adults; urban and rural; random forest,Sustainability
Journal Article,"C., Xu Qu, J., Li, W., Shi, S., Liu, B.",2025,Quantifying the nonlinear effects of urban-rural blue-green landscape combination patterns on the trade-off between carbon sinks and surface temperature: An approach based on self-organizing mapping and interpretable machine learning,Elsevier Ltd,130,,,"Global urbanization has exacerbated the heat island effect and carbon emissions, and urban and rural blue-green landscapes are critical for mitigating climate change. Traditional research has overlooked the non-linear mechanisms of the trade-off between carbon sinks and heat island effects, as well as heterogeneous landscape assemblages. Using the Harbin-Changchun Urban Agglomeration in China as an illustration, this investigation integrates a self-organized mapping model and an interpretable machine learning method to establish a characterization framework for blue-green landscape assemblages. The objective is to quantify the response mechanisms and nonlinear thresholds of net primary productivity (NPP) of vegetation, land surface temperature (LST), and their trade-offs. The findings demonstrated that (1) a total of seven types of blue-green landscape combination patterns were identified, with woodland-aggregated landscapes being the dominant type, which could achieve high NPP with low LST. (2) The thresholds of forested land percentage (FP), vegetation cover (FVC), and elevation coefficient of variation (ECV) were the primary factors influencing NPP, while FVC was more influential in regulating the marginal effect of LST. (3) Enhancing carbon sinks and regulating surface temperature can be achieved by maintaining FP within the range of 0.5–0.75. The lowest trade-off between NPP and LST can be achieved by setting FVC to 0.6. (4) It is imperative to prevent the functional attenuation that is caused by inefficient combinations of indicators and extreme threshold intervals, as NPP, LST, and their trade-offs are subject to the comprehensive influence of the interaction of multiple indicators. This study identified fine control thresholds for blue-green landscapes that are based on the characteristics of the landscape assemblage. These thresholds can serve as a spatial decision-making foundation for the mitigation of urban heat islands and the attainment of carbon neutrality standards. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010433946&doi=10.1016%2Fj.scs.2025.106608&partnerID=40&md5=f0a07eb6299ca18585ed7d5055bc777e,CE,Environmental,Blue-green infrastructure; Self-organizing mapping; Interpretable machine learning; Threshold effects; Landscape metrics,Sustainable Cities and Society
Journal Article,"M. A., Zhu Rahman, R.",2025,Characterizing public response to unforeseen cascading fuel shortage: Through the lens of human mobility-based explainable machine learning models,Elsevier Ltd,127,,,"Climate disasters unfold multitudes of effects, from societal and commercial disruptions to fuel and power shortages. These consequences escalate further in cascading disasters, where individuals are more likely to respond unwarrantedly due to the lack of preparation and situational awareness. A noticeable gap exists in comprehending the linkages between public responses to such disasters and socioeconomic and spatial disparities, which are critical to the provision of effective guidance and situational information to those affected. Based on mobile phone data and various socioeconomic, built environment, and geographical variables, this study systematically examines human mobility-based public responses during a cascading fuel shortage crisis. The spatiotemporal analysis uncovered a significant increase in visits to gasoline stations during and after the crisis and a decrease in mean distance traveled at the Census Block Group level. Furthermore, mobility prediction models were constructed using the random forest regression algorithm, which can adequately forecast visits and mean distance traveled to gasoline stations across different crisis stages. The Shapley Additive Explanations analysis reveals how various factors (e.g., educational attainment and distance to the coast) influenced public responses. These findings reinforce the importance of tailored disaster response education and situational awareness to ensure equitable resource access during cascading disasters. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005193966&doi=10.1016%2Fj.scs.2025.106446&partnerID=40&md5=78cb62105aad4d8365a5bc07a5a2cf30,CE,Transportation,Cascading disasters; Fuel shortage crisis; Human mobility; Inequality; Machine learning; Explainable models,Sustainable Cities and Society
Journal Article,"M., Szelag Rajczakowska, M., Habermehl-Cwirzen, K., Hedlund, H., Cwirzen, A.",2023,Interpretable Machine Learning for Prediction of Post-Fire Self-Healing of Concrete,Multidisciplinary Digital Publishing Institute (MDPI),16,3,,"Developing accurate and interpretable models to forecast concrete's self-healing behavior is of interest to material engineers, scientists, and civil engineering contractors. Machine learning (ML) and artificial intelligence are powerful tools that allow constructing high-precision predictions, yet often considered black box"" methods due to their complexity. Those approaches are commonly used for the modeling of mechanical properties of concrete with exceptional accuracy; however, there are few studies dealing with the application of ML for the self-healing of cementitious materials. This paper proposes a pioneering study on the utilization of ML for predicting post-fire self-healing of concrete. A large database is constructed based on the literature studies. Twelve input variables are analyzed: w/c, age of concrete, amount of cement, fine aggregate, coarse aggregate, peak loading temperature, duration of peak loading temperature, cooling regime, duration of cooling, curing regime, duration of curing, and specimen volume. The output of the model is the compressive strength recovery, being one of the self-healing efficiency indicators. Four ML methods are optimized and compared based on their performance error: Support Vector Machines (SVM), Regression Trees (RT), Artificial Neural Networks (ANN), and Ensemble of Regression Trees (ET). Monte Carlo analysis is conducted to verify the stability of the selected model. All ML approaches demonstrate satisfying precision, twice as good as linear regression. The ET model is found to be the most optimal with the highest prediction accuracy and sufficient robustness. Model interpretation is performed using Partial Dependence Plots and Individual Conditional Expectation Plots. Temperature, curing regime, and amounts of aggregates are identified as the most significant predictors.""",,CE,Materials,autogenous self-healing; cementitious materials; high temperature; artificial neural network; ensemble methods; mechanical properties; artificial intelligence,Materials
Journal Article,"J., Seshadri Ramakrishnan, K., Liu, T., Zhang, F., Yu, R., Gou, Z.",2023,Explainable semi-supervised AI for green performance evaluation of airport buildings,Elsevier Ltd,79,,,"Despite the global sustainability trend, airport buildings have received limited attention regarding their environmental impact within the built environment and aviation sectors. With less than 1% of green building certifications worldwide, there is a lack of essential green performance assessment tools, including frameworks, primary datasets, and models that require less human supervision. Previous research has addressed these issues by proposing frameworks, primary datasets, and a supervised method that leverages Classification and Regression Trees. However, these methods still rely on human effort to fix credit scores and assign category weights, necessitating significant supervision from human experts. This study presents a green performance evaluation model for airport buildings using an explainable semi-supervised AI approach to reduce human dependency during the inference of green scores. Principal Component Analysis and Hierarchical Clustering algorithms (agglomerative and divisive) are employed to construct the models. The dataset's dimensionality is reduced based on metrics such as scatter coefficient, Psi Index, variation, and permutation. The cluster numbers are determined using elbow point and silhouette coefficient values, resulting in cluster initializations between 6 and 12. Performance measures, including the rand index, mean absolute error, precision, recall, F1-measure, and accuracy, are evaluated for the models. The AHC and DHC models exhibit a maximum accuracy of 74%, albeit with different cluster numbers. Consequently, models are developed with suitable cluster numbers to derive green rules with improved predictive accuracy. Each agglomerative and divisive hierarchical clustering model yields 12 practical green rules that can assist airport operators and facility managers in enhancing the green performance of their airport buildings. These explainable green rules serve as guidelines to improve airport facilities. Additionally, a web-based green rating tool is developed to demonstrate the proof-of-concept, utilizing the green rules extracted from the agglomerative model. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172190816&doi=10.1016%2Fj.jobe.2023.107788&partnerID=40&md5=ff6a167361054986afd0174329ce803b,CEM,Sustainability (Construction),Green airport; Green building rating tool; Semi-supervised learning; Principal component analysis; Hierarchical clustering; Explainable AI; Performance evaluation,Journal of Building Engineering
Journal Article,"R. S. S., Kulasooriya Ranasinghe, Wkvjb, Perera, U. S., Ekanayake, I. U., Meddage, D. P. P., Mohotti, D., Rathanayake, U.",2024,Eco-friendly mix design of slag-ash-based geopolymer concrete using explainable deep learning,Elsevier B.V,23,,,"Geopolymer concrete is a sustainable and eco-friendly substitute for traditional OPC (Ordinary Portland Cement) based concrete, as it reduces greenhouse gas emissions. With various supplementary cementitious materials, the compressive strength of geopolymer concrete should be accurately predicted. Recent studies have applied deep learning techniques to predict the compressive strength of geopolymer concrete yet its hidden decision-making criteria diminish the end-users' trust in predictions. To bridge this gap, the authors first developed three deep learning models: an artificial neural network (ANN), a deep neural network (DNN), and a 1D convolution neural network (CNN) to predict the compressive strength of slag ash-based geopolymer concrete. The performance indices for accuracy revealed that the DNN model outperforms the other two models. Subsequently, Shapley additive explanations (SHAP) were used to explain the best-performed deep learning model, DNN, and its compressive strength predictions. SHAP exhibited how the importance of each feature and its relationship contributes to the compressive strength prediction of the DNN model. Finally, the authors developed a novel DNN-based open-source software interface to predict the mix design proportions for a given target compressive strength (using inverse modeling technique) for slag ash-based geopolymer concrete. Additionally, the software calculates the Global Warming Potential (kg CO2 equivalent) for each mix design to select the mix designs with low greenhouse emissions.",,CE,Materials,Geopolymer concrete; Compressive strength; Artificial intelligence; Deep learning; Explainability,Results in Engineering
Journal Article,"H. Yan, A. Khan, A. Jamil, B. Abdeldjalil, T. Saidani, N. Y. Rebouh",2025,Deep Learning-Based Spatial Prediction of Landslide Risk in Coastal Areas Using GIS and Multicriteria Decision Making: A DeepLabV3+ Approach,Institute of Electrical and Electronics Engineers Inc.,18,,15222–15235,"Sustainable land-use planning and catastrophe risk reduction depend critically on landslide susceptibility mapping. The complex, nonlinear interconnections of environmental and human elements cause terrain instability and challenge conventional prediction methods. In this work, we offer a DeepLabV3+-based deep learning framework coupled with geographic information systems and multicriteria decision making methods for spatial prediction of landslide risk, over the Dubai coastal and urban region (covering approximately 4000 km2). The approach uses an annotated dataset for semantic segmentation and high-resolution satellite images from the Mohammed Bin Rashid Space Center. On Google Colab with GPU acceleration, the model is trained and verified and then further improved for computational efficiency on a Mac M1 machine. Our results show an overall accuracy of 91.3%, a mean intersection over union of 82.5%, and an F1-score of 88.4%, demonstrating strong classification performance throughout a range of land cover types. The confusion matrix analysis highlights strong segmentation accuracy for water bodies (94.2%) and structures (92.4%). Considerable misclassification between roadways and unpaved terrain results from spectral similarities. Furthermore, the perclass Dice Coefficient analysis confirms that the model can efficiently discriminate intricate topographical patterns. Especially in fast-expanding areas such as Dubai, UAE, it provides a scalable solution for landslide susceptibility mapping, catastrophe risk management, and sustainable urban design. Future work will explore multisensor data fusion, real-time inference, and applying explainable artificial intelligence techniques to enhance model interpretability in dynamic terrain settings.",,CE,GIS / Remote Sensing,Coastal monitoring; DeepLabV3+; geographic information systems (GISs); landslide susceptibility mapping; multisensor data fusion; remote sensing; semantic segmentation; urban resilience classification,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Journal Article,"H. C., Pang Ren, B., Zhao, G., Liu, Y. Y., Zhang, H. P., Liu, S.",2025,Rapid flood simulation and source area identification in urban environments via interpretable deep learning,Elsevier B.V,651,,,"Climate change is driving an increase in extreme weather events, leading to a growing risk of urban flooding. Accurate and rapid forecasting of critical infrastructure (CI) flood variables, along with the identification of flood source areas (FSA), is crucial for effective urban flood risk management. Conventional approaches for identifying FSAs are often impractical for advanced, high-resolution fully distributed hydrodynamic models. This study provides a new approach for FSA identification in complex urban flood models using interpretable deep learning (IDL). Deep Convolutional Neural Networks (DCNNs) and Gradient-weighted Class Activation Mapping (GradCAM) were used to develop a surrogate model for the Distributed Hydrodynamic Model for Urban (DHM Urban), which includes a rain-on-grid overland flow mechanism. The results show that the deep learning model can rapidly and accurately simulate critical flood variables and identify corresponding FSAs at CI locations under various rainfall scenarios. Tests with real rainfall data showed that the MAE and RMSE for the regression simulation of maximum inundation depth were 3.20 cm and 3.77 cm, respectively. With an explained variance of 92.86 % and computation times in the sub-second range, the model demonstrates both high accuracy and efficiency. The model's interpretability ensures that its focus aligns with hydrological expertise, addressing challenges related to variable and discontinuous FSAs. Additionally, the paper highlights the importance of using chaotic rainfall patterns when developing interpretable surrogate models and demonstrates that DCNNs can implicitly encode location information in position-dependent tasks. IDL addresses the limitations of data-driven models by not only predicting outcomes but also explaining the underlying reasons, offering a unified solution for both rapid flood simulation and adaptive FSA identification.",,CE,Water,Interpretable deep learning; Rapid flood simulation; Flood source areas; Random chaotic rainfall; Position-dependent task,Journal of Hydrology
Journal Article,"B., Gobinath Revathi, R., Bala, G. S., Thotakura, T. V., Bonthu, S.",2024,Harnessing explainable Artificial Intelligence (XAI) for enhanced geopolymer concrete mix optimization,Elsevier B.V.,24,,,"Geopolymer concrete (GC) emerges as a sustainable alternative yet faces challenges in achieving optimal resource utilization for strength development. Balancing these aspects is crucial for its large-scale adoption as a sustainable material. The type and dosage of precursors, activator, curing, and mixing conditions influence compressive strength, setting time, and workability. Moreover, multiple experimental trials are required for a desirable geopolymer blend. Even the experimental parameters alone do not meet the design principles concerning sustainable construction. This paper presents a study on the mix design and interpretation of machine learning techniques (MLT) with XAI. To train the model, extensive experimental databases using the shapley additive explanations (SHAP) technique rank input factors that impact the strength aspect. The prediction models' performance was compared using coefficient of determination (R2) and root mean square error (RMSE). SHAP interpretations reveal that temperature, Na to Al ratio, and NaOH molarity are the main factors influencing the compressive strength of GC. Further, these parameters were crucial in developing the dense geopolymer matrix. By integrating XAI into the MLT approach, we have also opened new criteria for understanding the complex relationships between geopolymer concrete potential parameters and their compressive strength. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205322150&doi=10.1016%2Fj.rineng.2024.103036&partnerID=40&md5=80bf9988024bfdf0ccbed7c7c5b34c1a,CE,Materials,Geopolymer; SHAP analysis; Sustainable concrete; Machine learning,Results in Engineering
Journal Article,"A., Peleato Riyadh, N. M.",2025,Exploring spatial and temporal importance of input features and the explainability of machine learning-based modelling of water distribution systems,Elsevier Ltd,14,,,"Ensuring safe drinking water necessitates advanced management and monitoring techniques for water quality in distribution systems. This study leverages machine learning (ML) to model chlorine decay in a water distribution system (WDS) in British Columbia, Canada. A four-layer long short term memory (LSTM) network was trained to predict chlorine concentrations at a reservoir >24,000 m from the treatment plant. Explainable AI (XAI) techniques were applied to the trained network to address critical issues, such as enhancing the transparency and reliability of ML models. Several XAI methods were used to investigate the importance of sensor placement, identify the most significant features, understand feature ranges that result in poor performance, and validate model logic. Results demonstrated that for ML-based WDS control, sensor location is not critical, with high prediction accuracy achieved (mean absolute error <0.025 mg/L) even when exclusively using data from nodes spatially distant from the prediction site. XAI techniques showed the capability of identifying essential features and demonstrated that the behaviour of the ML model conformed with the expectations of chlorine behaviour. Superfluous variables were ranked low in importance, and the model learned fundamental aspects of chemical kinetics, such as temperature dependence and decay rate. Most importantly, the XAI methods applied showed the capability to communicate the reasoning for specific predictions, even at a local or sample-specific level. This study underscores the importance of transparency and trust in ML models, especially as the field transitions towards digital twin and Internet of Things (IoT) technologies, to enhance the effective management of water quality systems. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211002723&doi=10.1016%2Fj.dche.2024.100202&partnerID=40&md5=8872aeea6d8756bdbadc4ec4c580f65e,CE,Water,Explainable AI (XAI); Machine learning; Data-driven technology; Chlorine; Water distribution systems,Digital Chemical Engineering
Journal Article,T. Saeheaw,2025,Interpretable Machine Learning Framework for Non-Destructive Concrete Strength Prediction with Physics-Consistent Feature Analysis,Multidisciplinary Digital Publishing Institute (MDPI),15,15,,"Non-destructive concrete strength prediction faces limitations in validation scope, methodological comparison, and interpretability that constrain deployment in safety-critical construction applications. This study presents a machine learning framework integrating polynomial feature engineering, AdaBoost ensemble regression, and Bayesian optimization to achieve both predictive accuracy and physics-consistent interpretability. Eight state-of-the-art methods were evaluated across 4420 concrete samples, including statistical significance testing, scenario-based assessment, and robustness analysis under measurement uncertainty. The proposed PolyBayes-ABR methodology achieves R2 = 0.9957 (RMSE = 0.643 MPa), showing statistical equivalence to leading ensemble methods, including XGBoost (p = 0.734) and Random Forest (p = 0.888), while outperforming traditional approaches (p < 0.001). Scenario-based validation across four engineering applications confirms robust performance (R2 > 0.93 in all cases). SHAP analysis reveals that polynomial features capture physics-consistent interactions, with the Curing_age × Er interaction achieving dominant importance (SHAP value: 4.2337), aligning with established hydration–microstructure relationships. When accuracy differences fall within measurement uncertainty ranges, the framework provides practical advantages through enhanced uncertainty quantification (±1.260 MPa vs. ±1.338 MPa baseline) and actionable engineering insights for quality control and mix design optimization. This approach addresses the interpretability challenge in concrete engineering applications where both predictive performance and scientific understanding are essential for safe deployment. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013340801&doi=10.3390%2Fbuildings15152601&partnerID=40&md5=2acb3a63e2d181bee130d5d72f8e883f,CE,Materials,concrete compressive strength; machine learning; non-destructive evaluation; model interpretability; ensemble methods; statistical validation,Buildings
Journal Article,"B., Choksuriwong Sahoh, A.",2022,A proof-of-concept and feasibility analysis of using social sensors in the context of causal machine learning-based emergency management,Springer Science and Business Media Deutschland GmbH,13,8,3747–3763,"The goals of emergency management are to restore human safety and security, and to help the authorities understand what causes such events. It requires information that is both highly accurate, and can be generated very quickly. This research addresses these concerns with a machine learning model based on cause-and-effect using a Bayesian belief network. This employs human critical thinking and amplified context to encode the model structures, which contribute towards its imitation of human-intelligent understanding, and the model parameters are fitted using social media data. The results show that our model is a natural fit for a real-world environment required emergency management. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107030733&doi=10.1007%2Fs12652-021-03317-3&partnerID=40&md5=368176a56ace32d3685a4089fe1663b0,CEM,Risk and Uncertainty,Causal bayesian network; Cause-and-effect modeling; Social big data; Counterfactual; Explainable artificial intelligence,Journal of Ambient Intelligence and Humanized Computing
Journal Article,"R. A. A., Al-Areqi Saleh, F., Konyar, M. Z., Kaplan, K., Ongir, S., Ertunç, H. M.",2024,AdvancingTire Safety: Explainable Artificial Intelligence-Powered Foreign Object Defect Detection with Xception Networks and Grad-CAM Interpretation,Multidisciplinary Digital Publishing Institute (MDPI),14,10,,"Automatic detection of tire defects has become an important issue for tire production companies since these defects cause road accidents and loss of human lives. Defects in the inner structure of the tire cannot be detected with the naked eye; thus, a radiographic image of the tire is gathered using X-ray cameras. This image is then examined by a quality control operator, and a decision is made on whether it is a defective tire or not. Among all defect types, the foreign object type is the most common and may occur anywhere in the tire. This study proposes an explainable deep learning model based on Xception and Grad-CAM approaches. This model was fine-tuned and trained on a novel real tire dataset consisting of 2303 defective tires and 49,198 non-defective. The defective tire class was augmented using a custom augmentation technique to solve the imbalance problem of the dataset. Experimental results show that the proposed model detects foreign objects with an accuracy of 99.19%, recall of 98.75%, precision of 99.34%, and f-score of 99.05%. This study provided a clear advantage over similar literature studies. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194425562&doi=10.3390%2Fapp14104267&partnerID=40&md5=e8516c861927ba0a5c4387fd8fe6ab06,CE,Materials,tire defect detection; foreign object detection; deep learning; XAI; tire X-ray; Grad-CAM,Applied Sciences
Journal Article,"K., Shashiprabha Sandamal, S., Muttil, N., Rathnayake, U.",2023,Pavement Roughness Prediction Using Explainable and Supervised Machine Learning Technique for Long-Term Performance,Multidisciplinary Digital Publishing Institute (MDPI),15,12,,"Maintaining and rehabilitating pavement in a timely manner is essential for preserving or improving its condition, with roughness being a critical factor. Accurate prediction of road roughness is a vital component of sustainable transportation because it helps transportation planners to develop cost-effective and sustainable pavement maintenance and rehabilitation strategies. Traditional statistical methods can be less effective for this purpose due to their inherent assumptions, rendering them inaccurate. Therefore, this study employed explainable and supervised machine learning algorithms to predict the International Roughness Index (IRI) of asphalt concrete pavement in Sri Lankan arterial roads from 2013 to 2018. Two predictor variables, pavement age and cumulative traffic volume, were used in this study. Five machine learning models, namely Random Forest (RF), Decision Tree (DT), XGBoost (XGB), Support Vector Machine (SVM), and K-Nearest Neighbor (KNN), were utilized and compared with the statistical model. The study findings revealed that the machine learning algorithms’ predictions were superior to those of the regression model, with a coefficient of determination (R2) of more than 0.75, except for SVM. Moreover, RF provided the best prediction among the five machine learning algorithms due to its extrapolation and global optimization capabilities. Further, SHapley Additive exPlanations (SHAP) analysis showed that both explanatory variables had positive impacts on IRI progression, with pavement age having the most significant effect. Providing accurate explanations for the decision-making processes in black box models using SHAP analysis increases the trust of road users and domain experts in the predictions generated by machine learning models. Furthermore, this study demonstrates that the use of explainable AI-based methods was more effective than traditional regression analysis in IRI prediction. Overall, using this approach, road authorities can plan for timely maintenance to avoid costly and extensive rehabilitation. Therefore, sustainable transportation can be promoted by extending pavement life and reducing frequent reconstruction. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163970149&doi=10.3390%2Fsu15129617&partnerID=40&md5=088358c7b31297eb1140038edea83769,CE,Transportation,explainable AI; international roughness index; pavement performance; supervised machine learning; sustainable transportation,Sustainability
Journal Article,"J. I., Pereda Santos, M., Ahedo, V., Galán, J. M.",2023,Explainable machine learning for project management control,Elsevier Ltd,180,,,"Project control is a crucial phase within project management aimed at ensuring -in an integrated manner- that the project objectives are met according to plan. Earned Value Management -along with its various refinements- is the most popular and widespread method for top-down project control. For project control under uncertainty, Monte Carlo simulation and statistical/machine learning models extend the earned value framework by allowing the analysis of deviations, expected times and costs during project progress. Recent advances in explainable machine learning, in particular attribution methods based on Shapley values, can be used to link project control to activity properties, facilitating the interpretation of interrelations between activity characteristics and control objectives. This work proposes a new methodology that adds an explainability layer based on SHAP -Shapley Additive exPlanations- to different machine learning models fitted to Monte Carlo simulations of the project network during tracking control points. Specifically, our method allows for both prospective and retrospective analyses, which have different utilities: forward analysis helps to identify key relationships between the different tasks and the desired outcomes, thus being useful to make execution/ replanning decisions; and backward analysis serves to identify the causes of project status during project progress. Furthermore, this method is general, model-agnostic and provides quantifiable and easily interpretable information, hence constituting a valuable tool for project control in uncertain environments.",,CEM,Project Management,Project management; Stochastic project control; Earned value management; Shapley values; Explainable machine learning; SHAP,Computers & Industrial Engineering
Journal Article,"S. C., Panagiotakopoulou Sapkota, C., Dahal, D., Beskopylny, A. N., Dahal, S., Asteris, P. G.",2025,Optimizing high-strength concrete compressive strength with explainable machine learning,Springer Science and Business Media B.V.,8,3,,"This study leverages machine learning to enhance the prediction of high-strength concrete (HSC) compressive strength, addressing the limitations of conventional methods, which are often tedious, less reliable, and time-consuming. Extreme Gradient Boosting (XGB) serves as the primary model, with hyperparameter optimization via metaheuristic algorithms such as Cuckoo Search (CSA), Water Strider (WS), Leopard Seal (LS), Harris Hawk (HH), Invasive Weed (IW), and Forest Optimization (FO). A total of 681 data sets were collected from existing literature. The models underwent tenfold cross-validation, with the LS-XGB model achieving an almost ideal performance in testing sets. Other models, including CSA-XGB, WS-XGB, HH-XGB, IW-XGB, and FO-XGB, also demonstrated strong performance, each with R2 > 0.96. For model explainability, Shapley's Additive Explanation (SHAP) analysis has been applied to the best-performing LS-XGB model. The analysis revealed that cement and superplasticizer (SP) are the most crucial features contributing to HSC development, with optimal ranges identified at 600–900 kg/m3 for cement and 8–10 kg/m3 for SP. The study demonstrates on how feature interactions contribute to concrete materials compressive strength, providing better and above all sustainable constructions. Furthermore, the LS-XGB model's optimal performance depicts the strongly nonlinear nature of HSC materials, validated through a set of derived graphs. Additionally, 30 concrete cubes were prepared for experimental validation, and the datasets demonstrated an accuracy of 92% showcasing the ability of models to make well informed decision. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218148806&doi=10.1007%2Fs41939-025-00737-y&partnerID=40&md5=73eeac6eb2efed12037c4abbae7ac3db,CE,Materials,High-strength concrete; Ensemble learning; SHAP analysis; Metaheuristics algorithm; Nature inspired algorithm,"Multiscale and Multidisciplinary Modeling, Experiments and Design"
Journal Article,"M. E. Atik, Z. Duran, D. Z. Seker",2024,Explainable Artificial Intelligence for Machine Learning-Based Photogrammetric Point Cloud Classification,,17,,5834–5846,"Point clouds are one of the most widely used data sources for spatial modeling. Artificial intelligence approaches have become an important tool for understanding and extracting semantic information of point clouds. In particular, the explainability of machine learning approaches for 3-D data has not been sufficiently investigated. Moreover, existing studies are generally limited to object classification issues. This is a pioneer study that addresses the classification of photogrammetric point clouds in terms of explainable artificial intelligence. In this study, the explainability of black-box machine learning models in the context of the classification of photogrammetric point clouds was investigated. Each point in the point cloud is defined using geometric and spectral features. In addition, the effect of selecting the most important of these features on the classification performance of ML models such as Random Forest, XGBoost, and LightGBM was examined. The explainability of ML models was analyzed with Shapley additive explanation (SHAP), an explainable artificial intelligence approach. SHAP analysis was compared with filter-based information gain (IG) and ReliefF methods for feature selection. Using the features selected with SHAP analysis, overall accuracy (OA) of 85.50% in the Ankeny dataset, 91.70% in the Building dataset, and 83.28% in the Cadastre dataset was achieved with LightGBM. The evaluation with XGBoost shows an OA of 85.22% for Ankeny, 91.21% for Building, and 82.47% for Cadastre. The evaluation with RF shows an OA of 83.70% for Ankeny, 89.08% for Building, and 79.36% for Cadastre.",,CE,GIS / Remote Sensing,Classification; explainable artificial intelligence (XAI); feature selection; machine learning; photogrammetry; point cloud,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Journal Article,"K., Noureldin Shabbir, M., Sim, S. H.",2025,"Data-driven model for seismic assessment, design, and retrofit of structures using explainable artificial intelligence",John Wiley and Sons Inc,40,3,281–300,"Retrofitting building designs is crucial given the global aging infrastructure and increased in frequency of natural hazards like earthquakes. While traditional data-driven models are widely used for predicting building conditions, there has been limited exploration of recent artificial intelligence (AI) techniques in structural design. This study introduces a novel explainable AI framework that utilizes data-driven models for assessing, designing, and retrofitting of structures. The framework highlights the key global features of the model and further investigates them locally to adjust the input design parameters. It suggests the necessary changes in these inputs to achieve the desired structural performance. To achieve this, the framework employs interpretability techniques such as feature importance, feature interactions, Shapley Additive exPlanations, local interpretable model-agnostic explanations, partial dependence plot (PDP), and individual conditional expectation to highlight the important features. Additionally, a novel counterfactual) technique is applied for the first time as a design tool in seismic assessment and retrofitting of structures. The effectiveness of this framework is validated on a real benchmark structure through nonlinear time history analysis and natural earthquakes. The results show that the proposed framework is highly effective, especially under design-level earthquake conditions in achieving the necessary change in stiffness and strength of structures to meet the required seismic design objectives across different earthquake scenarios. This framework holds promise for wider adoption and applications in various other structural and civil engineering domains. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204086267&doi=10.1111%2Fmice.13338&partnerID=40&md5=5527a8728442f62c24c0125e605d14a1,CE,Structural,NaN,Computer-Aided Civil and Infrastructure Engineering
Journal Article,"H. M., Iram Shakeel, S., Hill, R., Farid, H. M. A., Brown, P., Rehman, H. U.",2025,An XAI-driven diagnostic framework to investigate the predictive power of building features to enhance EPC ratings in detached houses,Elsevier Ltd,344,,,"Energy performance in detached homes is critical for reducing carbon emissions in United Kingdom. However, understanding the complex factors that affect Energy Performance Certificate (EPC) ratings remains limited. Detached homes face unique challenges due to their larger floor areas and greater environmental exposure. Despite the significance of EPCs in driving energy efficiency, the diagnostic analysis of feature interactions at the class level (A–G) is underexplored, especially in detached homes. This study addresses this gap by employing predictive explainability to provide a detailed, rating class-wise diagnostic analysis of the predictive power of structural and operational features for detached homes. We investigate key factors such as CO2 emissions per floor area, heating costs, window, floor, walls efficiency, and construction age, and explore how these features interact to drive EPC ratings. Our findings show that CO2 emissions and heating costs are the primary drivers of EPC classification, but their impact varies across EPC bands. Detached homes in lower EPC categories (E–G) exhibit heightened sensitivity to high emissions and inefficient heating, while properties in EPC A and B benefit from improved insulation and efficient systems. This study introduces an innovative diagnostic framework that not only identifies key predictive features for each EPC class but also uncovers the synergistic effects of feature combinations. The results provide actionable insights for retrofit strategies and policy interventions, particularly for detached homes, offering a roadmap for improving energy efficiency and advancing sustainable energy practices. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008509216&doi=10.1016%2Fj.enbuild.2025.116022&partnerID=40&md5=d3c12db5679b1b680cb86dc9dd2fd348,CE,Energy,Energy performance certificate; Energy efficiency; Detached homes; Predictive power; Explainability; Machine learning; Buildings features,Energy and Buildings
Journal Article,"H., Ai Shan, L., He, C., Li, K.",2025,Enhancing multi-objective prediction of settlement around foundation pit using explainable machine learning,Springer Science and Business Media Deutschland GmbH,,,,"The settlement prediction around foundation pit is crucial for ensuring the safety and stability of urban construction projects. However, existing studies often face challenges such as limited interpretability of machine learning (ML) models and the inability to perform multi-target predictions for complex geotechnical engineering schemes. To address these issues, this paper investigates the application of explainable machine learning (EML) techniques for the settlement prediction around foundation pit, using measured data from the Yongning Hospital project in Huangyan District as a case study. Two predictive schemes, time series prediction and random sampling prediction, were proposed and validated using classical machine learning models, including multilayer perceptron (MLP), random forest (RF), and extreme gradient boosting (XGBoost). The results indicate that the random sampling prediction scheme, with the RF model as the dominant predictor, achieves high accuracy, with an average root mean square error (RMSE) of 0.110 (mm) and a coefficient of determination (R2) of 0.985. Feature importance ranking and SHapley Additive exPlanations (SHAP) analysis reveal that critical features, such as the top displacement of the enclosing pile, significantly influence settlement predictions, enhancing the model’s interpretability. Furthermore, optimization of input features and output targets reduced the number of input features by 70%, lowering resource expenditure while maintaining acceptable accuracy. This research advances geotechnical engineering practices by promoting the use of EML to enhance the accuracy, transparency, and efficiency of the settlement prediction around foundation pit. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010050663&doi=10.1007%2Fs13349-025-00985-z&partnerID=40&md5=cfff8f35e6fd02a1013631ec6acb595e,CE,Geotechnical,NaN,Journal of Civil Structural Health Monitoring
Journal Article,"N., Paruthi Sharma, S., Kumar Tipu, R. K.",2025,Interpretable GA-PSO-optimised deep learning for multi-objective geopolymer concrete strength prediction,Springer Nature,,,,"The study present an interpretable deep-learning framework, optimized using a hybrid Genetic Algorithm-Particle Swarm Optimization (GA-PSO), to predict and enhance the compressive strength of nano-modified geopolymer concrete (GPC). The framework integrates attention-augmented neural networks with SHAP-based explainability, Monte Carlo dropout uncertainty quantification, and surrogate-assisted multi-objective optimisation to simultaneously maximise strength while minimising cost and embodied CO<inf>2</inf> emissions. A curated dataset comprising 234 experimental GPC mixes–incorporating variables such as precursor type, nano-silica dosage, activator content, and curing conditions—was subjected to advanced preprocessing and polynomial feature engineering. A Binary Grey Wolf Optimiser (BGWO) was used for feature selection. The proposed DeepGA-PSO model outperformed conventional regressors (e.g., SVR, Random Forest, XGBoost) with an R2 of 0.994 and RMSE of 3.86 MPa. Explainability analyses identified curing regime, sodium hydroxide, and nano-silica content as key predictors. Optimisation via NSGA-II yielded Pareto-optimal mix designs suitable for cost-effective and low-carbon construction. A MATLAB-based GUI was developed to facilitate real-time mix design and prediction. This study offers a robust, scalable, and interpretable pipeline for data-driven GPC optimisation and provides a methodological foundation for intelligent infrastructure materials engineering. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011341541&doi=10.1007%2Fs42107-025-01450-4&partnerID=40&md5=deb312b024897e21b2f87c95eb24b775,CE,Materials,Geopolymer concrete; Deep learning; Genetic algorithm; Particle swarm optimisation; SHAP; NSGA-II; Uncertainty quantification; Mix design optimisation; Nano-silica; Sustainable materials,Asian Journal of Civil Engineering
Journal Article,"Y., Pan Shen, Y.",2023,BIM-supported automatic energy performance analysis for green building design using explainable machine learning and multi-objective optimization,Elsevier Ltd,333,,,"Supported by the combination of the advanced BIM technique with intelligent algorithms, this paper develops a systematic framework using explainable machine learning and multi-objective optimization to realize the automatic prediction and optimization of building energy performance towards the sustainable development goal. There are three critical parts incorporated, including the DesignBuilder simulation, BO-LGBM (Bayesian optimization-LightGBM) and an explainable method SHAP (SHapley Additive explanation)-based prediction and explanation of building energy performance, and AGE-MOEA algorithm-based multi-objective optimization (MOO) under sources of uncertainty. It has been verified in a case study for green building design. Results show that: (1) The predictive BO-LGBM model can make a highly precise prediction in agreement with the simulation data, reaching up the R2 larger than 93.4% and MAPE smaller than 2.13%. From the SHAP calculation, features related to the HAVC (Heating Ventilation and Air Conditioning) system tend to contribute more to affecting the prediction results. (2) The AGE-MOEA-based optimization can identify a set of Pareto optimal solutions in simultaneously minimizing the building energy consumption, CO<inf>2</inf> emission, and indoor thermal discomfort degree, arriving at the highest optimization rate of 13.43% under proper adjustment of certain features. (3) In the MOO task, the consideration of model and data uncertainty by prediction intervals and Monte-Carlo simulation can further increase the optimization rate by around 4.0% than the deterministic scenario, resulting in more desired strategies in optimizing the green building performance. In short, this paper enriches the area of green building development. For one thing, it raises the transparency and interpretability of machine learning to make the prediction more convincing. For another, it efficiently determines the passive and active design solutions along with the detailed profile of influential factors for green building design. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146038723&doi=10.1016%2Fj.apenergy.2022.120575&partnerID=40&md5=3151eefd53ac5e844c08ff5b09a4a318,CE,Energy,Building energy performance; Green building design; Ensemble Learning; Model explanation; Multi-objective Optimization,Applied Energy
Journal Article,"S., Baek Shin, K., So, H.",2025,Rapid Prediction of Local Mean Age of Air for Energy-Efficient Ventilation Systems Using Permutation Feature Importance,John Wiley and Sons Inc,2025,1,,"Prediction of local mean age of air (MAA) is a key technology that can enhance the comfort, health, and productivity of indoor residents by adjusting and optimizing the indoor environmental conditions. In this study, we developed a deep neural network (DNN)-based regression model to predict indoor air quality (IAQ) and proposed a permutation feature importance (PFI)-based explainable artificial intelligence (XAI) model to implement efficient ventilation systems in a hospital ward utilizing this regression model. The rapid prediction of the MAA in the space near each patient in the ward, depending on the location of the heating, ventilation, and air conditioning (HVAC) inlets and fluid velocity, were successfully measured through data-driven deep learning model training. Consequently, the proposed MAA prediction model achieved average R-squared values of 0.9506 and 0.9220 for MAA(1) and MAA(2), respectively. In addition, the DNN model demonstrated rapid predictive performance (similar to 0.4 ms/prediction), highlighting the possibility of real-time monitoring compared to conventional methods. Furthermore, the contribution of the location and fluid velocity of the HVAC system to the MAA in the space near the patient was analyzed using PFI. These results support the rapid virtual sensing and recommendation method that has the potential to be applied in future IAQ management, human healthcare, and energy management systems.",,CE,Energy,NaN,International Journal of Energy Research
Journal Article,"M. H. R., Khatun Sobuz, M., Kabbo, M. K. I., Mohamed Sutan, N. M.",2025,An explainable machine learning model for encompassing the mechanical strength of polymer-modified concrete,Springer Nature,26,2,931–954,"Polymer-modified concrete (PMC) is an advanced building material with more excellent durability, tensile strength, adhesion, and lesser susceptibility to chemical degradation. Recent developments in machine learning (ML) have shown that prediction of compressive strength (CS) of PMC key input factors needed to obtain an optimized mix design are among the areas of applicability of ML. This study used eight machine learning models, which are Decision Tree, Support Vector Machine, K-Nearest Neighbors, Bagging Regression, XG-Boost, Ada-Boost, Linear Regression, Gradient Boosting to predict compressive strength and perform SHAP (Shapley additive explanation) analysis. These hybrid predictive PMC models were developed using a wide-ranging dataset of 382 experimental data points compiled from the literature. A SHAP interaction plot was also used to show how each feature affected predictions on the model outputs. As highlighted in the results, hybrid models had significantly higher performance than conventional models, and the XG-Boost and decision tree model had the highest accuracy. In particular, the XG-Boost and decision tree model reached R2 scores of 0.987 for training and 0.577 for testing, proving its remarkable prediction ability for PMC compressive strength. The SHAP analysis confirmed that coarse aggregate, cement, and SCMs had the most significant influence on CS, with all other variables contributing lower values. The Partial Dependence Plots (PDP) analysis allowed a relatively simple interpretation of the contribution of individual inputs to the CS predictions. These results are useful for construction purposes and provide engineers and builders with first-hand knowledge and insight into the importance of individual components on PMC development and performance. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210527989&doi=10.1007%2Fs42107-024-01230-6&partnerID=40&md5=e3f20250d3f8d505acbe477c70b0bfc0,CE,Materials,Compressive strength prediction; Machine learning; Polymer modified concrete; SHAP; PDP analysis,Asian Journal of Civil Engineering
Journal Article,"M. H. R., Aayaz Sobuz, R., Rahman, S. M. A., Shaikh, F. U. A., Kabbo, M. K. I., Khan, M. M. H.",2025,Assessment of hybrid fiber reinforced graphene nano-engineered concrete composites: From experimental testing to explainable machine learning modeling,Elsevier B.V.,36,,1409–1430,"The exceptional characteristics of graphene, relating to its chemical, mechanical, and thermal properties, make it a multifunctional material suitable for a variety of applications in construction materials. This research comprehensively examined the fresh, mechanical, and microstructural properties of graphene nano-engineered hybrid fiber reinforced concrete (GNFRC) incorporating 0-0.06% of graphene and 0-0.5% of hybrid fibers (steel, glass, and polypropylene). The experimental results demonstrate that the optimal concrete mix containing 0.06% graphene, 0.5 vol% polypropylene, and 0.5 vol% steel fibers exhibited a maximum of 22%, 36%, and 14% increase in compressive strength, tensile strength, and elastic modulus relative to the control at 90 days. Additionally, the scanning electron microscopy coupled with energy-dispersive x-ray spectroscopy images of the concrete specimens indicated the economic development of amorphous intermediate hydration products is the main reason for the growth of long-term mechanical properties of the concrete mixes. This study also investigates three machine learning techniques (k-Nearest Neighbors, polynomial regression and XGBoost) to predict the compressive strength of GNFRC utilizing datasets comprising 438 different mix proportions of GNFRC. Among them, the XGBoost model showed highest accuracy with an R2 of 0.994. The use of SHapley Additive exPlanations (SHAP) and Partial Dependency Plot (PDP) analyses identified the most influential mix elements as cement and curing age on compressive strength. The findings ensure the potential of graphene and hybrid fibers to develop durable, high-performance concrete composites suitable for sustainable building applications.",,CE,Materials,Graphene; Hybrid fiber reinforced concrete; Fresh properties; Mechanical properties; Microstructure,Journal of Materials Research and Technology
Journal Article,"C., Zhao Song, T., Xu, L.",2025,Physics-Informed Explainable AI and SMOTE-GPC for the Classification of Surrounding Rock Mass in Tunneling,American Society of Civil Engineers (ASCE),11,2,,"The classification of surrounding rock mass is essential for characterizing rock properties and geological conditions in tunneling engineering. While numerous empirical rock mass classification systems have been proposed (e.g., rock mass rating system, rock structure rating system), they tend to heavily rely on engineers' experience, which is unfavorable for tunnel construction, particularly in deep-buried and ultralong tunnels. Alternatively, machine learning, i.e., artificial intelligence (AI), methods estimate the classification of the surrounding rock mass using certain readily available rock indices (e.g., volumetric joint count). However, most machine learning models are considered black box models, leading to unexplainable predictions. In addition, employing all measurements of readily available rock indices as input may lead to excessive model complexity and a reduction in generalization performance. In this case, a Gaussian process classification (GPC) approach combined with the synthetic minority oversampling technique (SMOTE), Bayesian framework, and SHapley Additive exPlanations is proposed in this study for the probabilistic classification of the surrounding rock mass and selection of the optimal GPC model based on imbalanced and sparse measurement data. It is worth noting that the proposed method can also provide physics-informed explanations for the prediction and model class selection results and determine the significant input variables for each grade of the surrounding rock mass. A real-life example is employed to illustrate and validate the proposed approach. The results show that the F1 score of the optimal GPC model reaches 0.93, which is comparable with those of the GPC model with all input variables. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002006422&doi=10.1061%2FAJRUA6.RUENG-1519&partnerID=40&md5=1c9456c5da6e7a7244e263c48a8e210f,CE,Geotechnical,Data-driven methods; Imbalanced data; Sparse measurements; Bayesian model selection; SHapley Additive explanations (SHAP),"ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems, Part A: Civil Engineering"
Journal Article,"R. M., Raeisinezhad Spangler, M., Cole, D. G.",2024,"Explainable, Deep Reinforcement Learning-Based Decision Making for Operations and Maintenance",Taylor & Francis Group (Informa UK Limited),210,12,2331–2345,"This paper presents research that integrates condition monitoring and prognostics with decision making for nuclear power plant operations and maintenance aimed at reducing lifetime maintenance and repair costs. Additionally, a focal point of this research is to make the decisions explainable to operators, improving the trustworthiness of the decisions from what can be considered a black box model. In this work, we develop and evaluate an explainable, online asset management methodology to help reduce lifetime maintenance and repair costs. Using the latest advancements in condition monitoring, inventory management, deep reinforcement learning, and explainable artificial intelligence methods, we create a predictive maintenance methodology that can optimize the maintenance and spare part management of a repairable nuclear power plant system.To demonstrate these methods, preliminary studies were conducted on a representative maintenance system undergoing a stochastic degradation process that requires repairs or replacement to continue operation. Using deep reinforcement learning, we were able to reduce maintenance spending by approximately 50% compared to optimized, time-based maintenance strategies for the chosen system. A key component of our methodology is the integration of Shapley values to quantify the contribution of various factors to the decision-making process. This addition enhances the explainability and trustworthiness of our decisions, providing operators with transparent and understandable insights into the rationale behind maintenance strategies. The robustness and resiliency of our decision policy against observation noise were also thoroughly evaluated, demonstrating its effectiveness in uncertain operational environments.",,CE,Energy,Deep reinforcement learning; operations and maintenance; decision making; explainability; uncertainty,Nuclear Technology
Journal Article,"R., Yussif Taiwo, A. M., Ben Seghier, M. E. A., Zayed, T.",2024,Explainable ensemble models for predicting wall thickness loss of water pipes,Ain Shams University,15,4,,"Water Distribution Networks (WDNs) are susceptible to pipe failures with significant consequences. Predicting wall-thickness loss in pipes is vital for proactive maintenance and asset management. This study develops optimized, explainable machine learning models for this purpose. Data from four WDNs located in Canada and the USA are collected and preprocessed. Decision Tree, Random Forest (RF), XGBoost, LightGBM, and CatBoost are employed, with optimized hyperparameters via Tree-Structured Parzen Estimator. The proposed framework performance is assessed using dissimilarity-based and similarity-based metrics. Hyperparameter optimization substantially enhances predictive performance such that the mean absolute error of RF improved by 20.51%. Based on the evaluation metrics, the Copeland algorithm was employed to rank the models, and CatBoost emerged as the best-performing model with a Copeland score of 4, followed by XGBoost and RF. The Taylor Diagram offers a visual representation of the linear proportionality between observed and predicted values across various models, with CatBoost and XGBoost showing strong alignment. SHAP analysis identifies age, diameter, and length as key contributors. The optimized models proactively identify potential pipe failures, enhancing maintenance and WDN management. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184675398&doi=10.1016%2Fj.asej.2024.102630&partnerID=40&md5=06342e3ad5cf8b558719db77cfd04ea3,CE,Water,Water pipelines; Wall thickness; Machine learning; Ensemble learning; SHAP,Ain Shams Engineering Journal
Journal Article,"R., van der Werf Taormina, J. A.",2024,Interpretable Sewer Defect Detection with Large Multimodal Models †,Multidisciplinary Digital Publishing Institute (MDPI),69,1,,"Large Multimodal Models are emerging general AI models capable of processing and analyzing diverse data streams, including text, imagery, and sequential data. This paper explores the possibility of exploiting multimodality to develop more interpretable AI-based predictive tools for the water sector, with a first application for sewer defect detection from CCTV imagery. To this aim, we test the zero-shot generalization performance of three generalist large language-vision models for binary sewer defect detection on a subset of the SewerML dataset. We compared the LMMs against a state-of-the-art unimodal Deep Learning approach which has been trained and validated on >1 million SewerML images. Unsurprisingly, the chosen benchmark showcases the best performances, with an overall F1 Score of 0.80. Nonetheless, OpenAI GPT4-V demonstrates relatively good performances with an overall F1 Score of 0.61, displaying equal or better results than the benchmark for some defect classes. Furthermore, GPT4-V often provides text descriptions aligned with the provided prediction, accurately describing the rationale behind a certain decision. Similarly, GPT4-V displays interesting emerging behaviors for trustworthiness, such as refusing to classify images that are too blurred or unclear. Despite the significantly lower performance from the open-source models CogVLM and LLaVA, some preliminary successes suggest good potential for enhancement through fine-tuning, agentic workflows, or retrieval-augmented generation. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218110338&doi=10.3390%2Fengproc2024069158&partnerID=40&md5=21858b92fff209910fdf38c80f187f21,CE,Water,artificial intelligence; asset management; generative AI; sewer defect classification,Engineering Proceedings
Journal Article,"R., Recio-Garcí­a Tariq, J. A., Cetina-Quiñones, A. J., Orozco-del-Castillo, M. G., Bassam, A.",2025,Explainable artificial intelligence twin for metaheuristic optimization: Double-skin facade with energy storage in buildings,Oxford University Press,12,3,16–35,"The double facade solar chimney along with energy storage components is a sustainable building technology that harnesses the power of the sun to regulate indoor temperatures. Extensive research has been conducted on the theoretical simulation of such systems. The novelty of this work is to explore the potential of explainable artificial intelligence in improving the design and optimization of the double-skin solar chimney. The need for this research is owing to the high computational limitations of the physical model of such systems, thus the application of explainable artificial intelligence based upon artificial neural network can address this research gap. The paper solved a validated physical model and demonstrates the suitability of artificial neural network twins as computational-efficient subrogates that can be later used by a multiobjective optimization function to find the optimal design values for the facades of double-skin buildings. The results of the comparison between the physical and the artificial neural network model show the practical advantage of utilizing the digital twin model without compromising accuracy. The results have indicated that the artificial neural network can achieve a high coefficient of determination ranging from 0.921 to 0.999 on different performance indicators that implies a high goodness of fit. Accordingly, the optimization study based upon a nonsorting genetic algorithm (NSGA-II) has indicated a high ventilation rate of 2.86 1/h and an efficiency of 37.21%. The insights of this work have reflected that exploring the societal implications of sustainable building technologies such as the double facade solar chimney through educational initiatives can cultivate a new generation of society who not only understand the technical aspects but also appreciate the broader social and environmental contexts of their work, eventually to have future buildings with integrated passive systems. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000135696&doi=10.1093%2Fjcde%2Fqwaf015&partnerID=40&md5=8d5a8aa1afa06c8c403039d16a42c556,CE,Energy,explainable artificial intelligence; metaheuristic optimization; solar chimney; energy and buildings,Journal of Computational Design and Engineering
Journal Article,"R., Azari Torlapati, H., Shokouhi, P.",2023,Classification of Impact Echo Signals Using Explainable Deep Learning and Transfer Learning Approaches,SAGE Publications Ltd,2677,9,464–477,"Impact echo (IE) is one of the most frequently used nondestructive evaluation (NDE) techniques for detecting subsurface defects such as delamination, honeycombing, and voids in concrete structures. In the conventional analysis of IE data, the time-domain signal is transformed into the frequency domain and the frequency content is used to estimate the presence and nature of the defect. Machine learning (ML) has been recently applied to the IE signal classification problem. However, because of the scarcity of labeled IE datasets, most existing work relies on relatively small training and test datasets without addressing the generalizability and transferability of the developed models. In this paper, we compare two approaches for automatic classification of IE signals: clustering based on expert-crafted features and deep learning (DL) from automatically extracted features. Next, we use the knowledge gained from a DL model trained on concrete specimens with available ground truth to make predictions about defects in a different specimen with completely different construction and characteristics (transfer learning). Finally, we examine our DL model to gain insights into the model working (explainability) and highlight the attributions that are significant in classifying a particular IE signal. Our findings demonstrate the utility of ML and DL for IE signal classification, but also highlight the need for high-quality labeled datasets for advancing ML and DL in NDE data analysis. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169419726&doi=10.1177%2F03611981231159404&partnerID=40&md5=919f1ea2d57920ef258b45c5c186381e,CE,Materials,impact echo; machine learning; deep learning; signal classification; concrete,Transportation Research Record
Journal Article,"M. N., Ye Uddin, J. H., Deng, B. Y., Li, L. Z., Yu, K. Q.",2023,Interpretable machine learning for predicting the strength of 3D printed fiber-reinforced concrete (3DP-FRC),Elsevier Ltd,72,,,"This study aims to provide an effective and accurate machine learning approach to predict the compressive strength (CS) and flexural strength (FS) of 3D printed fiber reinforced concrete (3DPFRC). Six types of ML models were utilized in this study: random forest (RF), support vector machine (SVM), extreme gradient boosting (XGBoost), light gradient boosting machine (LightGBM), categorical gradient boosting (Catboost), and natural gradient boosting (NGBoost). The CS and FS data is collected from recent published papers and split into training set and testing set. The hyperparameter optimization techniques are applied to optimize the ML model parameters using a grid search strategy paired with the 5-fold cross-validation. In the testing set, XGBoost, LightGBM, Catboost, and NGBoost achieve high accuracy (R2 = 0.98, 0.98, 0.98, and 0.96, respectively) on CS prediction, which is better than that of RF and SVM (R2 = 0.90 and 0.92, respectively). High accuracy on FS prediction is also obtained in XGBoost, LightGBM, CatBoost, and NGBoost (R2 = 0.94, 0.93, 0.92, and 0.90, respectively). Furthermore, the relative importance of input variables' contribution to the mechanical performance of 3DP-FRC is disclosed via Shapley additive explanations (SHAP) analysis. The SHAP analysis identifies that water/binder ratio and ordinary Portland cement content are the most influential parameters for CS, while the loading direction and fiber volume fraction are the most significant parameters for FS. The ML models incorporated with SHAP analysis disclose the relationship between the input variables and mechanical performance of 3DP-FRC and could provide valuable information for the performance-based design of the mix proportion of 3DP-FRC.",,CE,Materials,3D concrete printing; Fiber reinforced concrete; Machine learning; Compressive strength; Flexural strength,Journal of Building Engineering
Journal Article,"W., Bin Inqiad Ullah, W., Ayub, B., Khan, M. S., Javed, M. F.",2025,An explainable machine learning (XML) approach to determine strength of glass powder concrete,Elsevier Ltd,45,,,"Glass powder concrete (GPC) holds the potential to reduce the damaging impact of construction industry on the natural environment by cutting down the amounts of cement and natural aggregates used in concrete. However, it requires having reliable ways for predicting key properties of GPC such as compressive strength (CS). The laboratory determination of GPC strength comes with various resource and time limitations due to which this study focused on using advanced machine learning (ML) techniques for predicting CS. Four algorithms including Multi Expression Programming (MEP), AdaBoost, HistGBR, and Extreme Gradient Boosting (XGB) were used for this purpose on a database retrieved from literature published internationally. The mixture composition of GPC like glass powder content, water-to-cement ratio, sand content along with concrete age etc. were used as predictors while CS was the only outcome. The algorithm's performance was validated by several techniques including k-fold cross validation, scatter plots, and residual distribution. Different error evaluation metrics were also utilized which revealed that XGB had the superior accuracy compared to its counterparts with a testing coefficient of determination (R2) value equal to 0.993. In contrast, MEP depicted its outcome as an equation which XGB and all the other algorithms could not provide. Moreover, interpretable ML techniques like shapely and Individual Conditional Expectation (ICE) analysis were employed for investigation of the significance and relationships between inputs and the predicted outcome. Furthermore, a graphical user interface has been developed which will enable the professionals in the civil engineering industry to utilize the findings of this study seamlessly. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000195959&doi=10.1016%2Fj.mtcomm.2025.112181&partnerID=40&md5=582c2cdec99b96460b9a06014431c29e,CE,Materials,Compressive strength; Machine learning; Glass powder concrete; Explainable machine learning; Prediction,Materials Today Communications
Journal Article,"M., Yildirim Ulucan, G., Alataş, B., Alyamaç, K. E.",2023,A new intelligent sunflower optimization based explainable artificial intelligence approach for early-age concrete compressive strength classification and mixture design of RAC,John Wiley and Sons Inc,24,6,7400–7418,"This study aims to develop a new artificial intelligence model that can produce explainable rules to predict the mix design and early-age concrete compressive strength classes of recycled aggregate concrete (RAC). Unlike other black-box machine learning methods and rule-based algorithms, the study relies on a metaheuristic mechanism for explainability. This metaheuristic mechanism is not used for a traditional parameter optimization, but to automatically extract interpretable and interpretable rules from the experimental data. In the study, 30 series of RACs are produced, and the samples’ 1- and 3-day early-age concrete compressive strength values are determined. Concretes produced using these strength values are classified. The labels defined for each concrete class are Class A (C 8/10), Class B (C 12/15), Class C (C 16/20), and Class D (C 20/25). The proposed intelligent classification model that consists of rule set automatically produces interpretable and comprehensible rules from data to determine the early-age concrete compressive strength class and RAC mix amounts. In addition, the proposed method eliminates the black-box disadvantages of classical machine learning methods with its explainability and interpretability feature. The sunflower optimization algorithm is adapted as the metaheuristic mechanism and a special fitness function and representative solution form are developed for automatic extraction of high-quality comprehensible rules by simultaneously optimizing many different metrics. This paper is the first interpretable and comprehensible artificial intelligence model attempt used for early-age compressive strength classification and mixture design of recycled aggregate concrete by balancing and optimizing both the accuracy and explainability. Proposed explainable intelligent classification model is tested against both well-known state-of-the-art machine learning algorithms and standard rule-based methods on the produced real data. Promising results in terms of accuracy, precision, recall are obtained along with the explainability feature. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162204209&doi=10.1002%2Fsuco.202300138&partnerID=40&md5=be92f2e81b922745e97efda768ba5892,CE,Materials,concrete mix design; construction and demolition waste; earthquake; explainable artificial intelligence; intelligent optimization,Structural Concrete
Journal Article,"M., Yildirim Ulucan, G., Alatas, B., Alyamac, K. E.",2024,Modelling and evaluation of mechanical performance and environmental impacts of sustainable concretes using a multi-objective optimization based innovative interpretable artificial intelligence method,Elsevier Ltd,372,,,"This study focuses on modelling sustainable concretes' mechanical and environmental properties with interpretable artificial intelligence-based automated rule extraction, management of waste materials, and meeting future prospects. In this context, 24 sustainable concrete series containing fly ash and recycled aggregates were produced. Compressive strength tests were performed on these specimens at 7, 28, and 90 days, and their mechanical properties were evaluated. Concrete classes (Class A, B, C, D) were determined using the compressive strength values obtained for each test day. The results of each concrete class were analyzed using a unique interpretable multi-objective rule extraction model, and the range values of the materials used were determined. The applied multi-objective rule extraction method is used for the first time in the literature, and its most important novelty is that, unlike other black-box artificial intelligence methods, it can also enable the creation of sustainable concrete recipes. After the range values of the materials used were found by automatic rule extraction, environmental impact assessments were performed. Among the impact categories, energy consumption and global warming potential were considered. The energy consumption results for Rule 4 were calculated as 814.8-1467.1 MJ, respectively, and a reduction of approximately 44.5% was observed. Similarly, global warming potentials for Rule 3 were obtained as 187.0-267.3 kg m3, respectively, with a reduction of about 30%. The limitations and future prospects of the study have been extensively investigated. The importance of adopting explainable/interpretable artificial intelligence-based approaches within the scope of sustainable development and circular economy goals to develop social infrastructure and buildings with low carbon emissions that are feasible in terms of mechanical and environmental properties is highlighted. Multi-Objective Optimization Based Innovative Interpretable Artificial Intelligence Method, used for the first time in mechanical and environmental modelling of sustainable concretes, can make significant contributions to the literature and future studies.",,CE,Materials,Explainable and interpretable artificial intelligence; Construction and demolition waste; Life cycle assessment; Waste management and recycling; Natural resource management; Sustainable development,Journal of Environmental Management
Journal Article,"N., Kreibich Veigel, H., Cominola, A.",2023,Interpretable Machine Learning Reveals Potential to Overcome Reactive Flood Adaptation in the Continental US,,11,9,,"Floods cause average annual losses of more than US$30 billion in the US and are estimated to significantly increase due to global change. Flood resilience, which currently differs strongly between socio-economic groups, needs to be substantially improved by proactive adaptive measures, such as timely purchase of flood insurance. Yet, knowledge about the state and uptake of private adaptation and its drivers is so far scarce and fragmented. Based on interpretable machine learning and large insurance and socio-economic open data sets covering the whole continental US we reveal that flood insurance purchase is characterized by reactive behavior after severe flood events. However, we observe that the Community Rating System helps overcome this behavior by effectively fostering proactive insurance purchase, irrespective of socio-economic backgrounds in the communities. Thus, we recommend developing additional targeted measures to help overcome existing inequalities, for example, by providing special incentives to the most vulnerable and exposed communities. Flood resilience of individuals and communities can be improved by bottom-up strategies, such as insurance purchase, or top-down measures like the US National Flood Insurance Program's Community Rating System (CRS). Our interpretable machine learning approach shows that flood insurances are mostly purchased reactively, after the occurrence of a flood event. Yet, reactive behaviors are ill-suited as more extreme events are expected under future climate, also in areas that were not previously flooded. The CRS counteracts this behavior by fostering proactive adaptation across a widespread range of socio-economic backgrounds. Future risk management including the CRS should support and motivate individuals' proactive adaptation with a particular focus on highly vulnerable social groups to overcome existing inequalities in flood risk. Flood insurance purchase in the US is dominated by reactive behavior after severe floodsThe Community Rating System (CRS) fosters proactive insurance adoption irrespective of socio-economic backgroundThe CRS should further balance existing inequalities by targeting specific population segments",,CE,Water,NaN,Earth's Future
Journal Article,"S., Salami Wahab, B. A., Alateah, A. H., Al-Tholaia, M. M. H., Alahmari, T. S.",2024,"Exploring the interrelationships between composition, rheology, and compressive strength of self-compacting concrete: An exploration of explainable boosting algorithms",Elsevier B.V.,20,,,"This study introduces a novel methodology for enhancing the compressive strength of selfcompacting concrete (SCC) via the use of the Explainable Boosting Machine (EBM), a sophisticated and interpretable machine learning algorithm. It presents a data -driven model that aims to accurately predict the strength of SCC by considering the intricate interactions among its various elements. Additionally, the model provides insights into the variables that influence SCC ' s compressive strength. By using EBM in conjunction with XGBoost and CatBoost algorithms, this study conducts a comparative examination of predictive abilities using datasets related to composition and rheology. The findings reveal that CatBoost has greater predictive performance using rheology dataset, as shown by an R 2 value of 0.977. Conversely, XGBoost exhibits a higher predictive capability using the composition dataset, as indicated by an R 2 value of 0.947. The EBM can provide comprehensive explanations at both global and local levels. It effectively identifies the key factors that have a significant influence on compressive strength. These factors include the coarse aggregate content, cement content, water content, viscosity, and V -funnel flow time. The study findings provide more evidence to support the notion that including rheological data into the model leads to a notable improvement in its accuracy. This outcome further confirms the existence of a direct correlation between rheological properties and compressive strength. The explanatory insights provided by EBM give practical instructions for customising SCC mixes to attain desired strengths. This facilitates quality control and enables personalised concrete design in the field of construction. This study highlights the potential of interpretable machine learning algorithms in improving the predictive modelling of SCC features. This advancement may lead to the development of more durable, efficient, and customised building materials.",,CE,Materials,Self-compacting concrete; Compressive strength; Machine learning; Ensemble algorithms; Explainable boosting machine,Case Studies in Construction Materials
Journal Article,"T. G., Alam Wakjira, M. S.",2024,Performance-based seismic design of Ultra-High-Performance Concrete (UHPC) bridge columns with design example – Powered by explainable machine learning model,Elsevier Ltd,314,,,"Ultra-high-performance concrete (UHPC) has rapidly become a material of choice in modern construction owing to its favorable properties, such as superior strength, toughness, and durability. Yet, accurate quantification of damage states using appropriate engineering demand parameters (EDPs) remains a significant challenge in the performance-based seismic design (PBSD) of UHPC columns. This gap is especially evident in the scarcity of research focused on the PBSD of UHPC columns. Therefore, this study aims to develop an explainable machine learning (ML) powered predictive model for the drift ratio limit states of UHPC bridge columns across four damage states. To achieve this, a fiber-based numerical model was developed and then validated against large-scale experimental results of UHPC columns tested under reverse cyclic loading. The Latin hypercube sampling method is employed to generate a comprehensive database of UHPC bridge columns (10,000 UHPC columns), considering uncertainties across a spectrum of design factors, including column geometry, material characteristics, reinforcement ratio, and axial load ratio. The drift ratios at the initiation of four damage states are determined using the numerical model. Subsequently, the power of the ML algorithm is leveraged to introduce drift ratio limit states for UHPC columns that capture the onset of four different damage states. The efficacy of the developed model is assessed using a range of statistical metrics, including the composite fitness score, D<inf>X%</inf> metrics, coefficient of determination, root mean squared error (RMSE), normalized RMSE, and mean absolute error, which demonstrated high predictive accuracy of the drift ratio limit states on both the training and unseen or test datasets. Furthermore, a model-agnostic Shapley Additive exPlanation (SHAP) is used to explain the output of the model and examine the influence of various design factors on drift ratio across the damage states. Notably, the axial load ratio and aspect ratio are found to be major factors in determining the drift ratios across the damage states. The developed model is deployed into a software tool in order to enable its practical application. Additionally, the implementation of the developed model in the context of PBSD is illustrated through a design example. Furthermore, the study proposed capacity uncertainty parameters to aid the fragility assessment of UHPC columns. Finally, the proposed model, along with capacity uncertainty parameters, is used for the fragility assessment of UHPC columns reinforced with varying steel bar grades. The results demonstrate the less vulnerability of UHPC columns reinforced with higher bar grade, yet the effect of bar grade lessens at severe damage levels. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195100319&doi=10.1016%2Fj.engstruct.2024.118346&partnerID=40&md5=49151bcba0ef357ffc8bee418c0b95b5,CE,Structural,UHPC; Performance-based seismic design (PBSD); Limit states;Fragility analysis; Machine learning;Explainable machine learning,Engineering Structures
Journal Article,"M., Sajjad Waleed, M.",2025,Advancing flood susceptibility prediction: A comparative assessment and scalability analysis of machine learning algorithms via artificial intelligence in high-risk regions of Pakistan,John Wiley and Sons Inc,18,1,,"Flood susceptibility mapping (FSM) is crucial for effective flood risk management, particularly in flood-prone regions like Pakistan. This study addresses the need for accurate and scalable FSM by systematically evaluating the performance of 14 machine learning (ML) models in high-risk areas of Pakistan. The novelty lies in the comprehensive comparison of these models and the use of explainable artificial intelligence (XAI) techniques. We employed XAI to identify significant conditioning factors for flood susceptibility at both the model training and prediction stages. The models were assessed for both accuracy and scalability, with specific focus on computational efficiency. Our findings indicate that LGBM and XGBoost are the top performers in terms of accuracy, with XGBoost also excelling in scalability, achieving a prediction time of ~18 s compared to LGBM's 22 s and random forest's 31 s. The evaluation framework presented is applicable to other flood-prone regions and highlights that LGBM is superior for accuracy-focused applications, while XGBoost is optimal for scenarios with computational constraints. The findings of this study can assist in accurate FSM in different regions and can also assist in scaling up the analysis to a larger geographical region which could assist in better decision-making and informed policy production for flood risk management. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210100876&doi=10.1111%2Fjfr3.13047&partnerID=40&md5=53bcc5ab8a808a67ef2cd70b3ef358f9,CE,Water,flood risk; flood susceptibility mapping; floods; machine learning; remote sensing,Journal of Flood Risk Management
Journal Article,"D., Thunéll Wang, S., Lindberg, U., Jiang, L. L., Trygg, J., Tysklind, M., Souihi, N.",2021,A machine learning framework to improve effluent quality control in wastewater treatment plants,Elsevier B.V.,784,,,"Due to the intrinsic complexity of wastewater treatment plant (WWTP) processes, it is always challenging to respond promptly and appropriately to the dynamic process conditions in order to ensure the quality of the effluent, especially when operational cost is a major concern. Machine Learning (ML) methods have therefore been used to model WWTP processes in order to avoid various shortcomings of conventional mechanistic models. However, to the best of the authors' knowledge, no ML applications have focused on investigating how operational factors can affect effluent quality. Additionally, the time lags between process steps have always been neglected, making it difficult to explain the relationships between operational factors and effluent quality. Therefore, this paper presents a novel ML-based framework designed to improve effluent quality control in WWTPs by clarifying the relationships between operational variables and effluent parameters. The framework consists of Random Forest (RF) models, Deep Neural Network (DNN) models, Variable Importance Measure (VIM) analyses, and Partial Dependence Plot (PDP) analyses, and uses a novel approach to account for the impact of time lags between processes. Details of the framework are provided along with a demonstration of its practical applicability based on a case study of the lima WWTP in Sweden involving a large number of samples (105763) representing the full scale of the plant's operations. Two effluent parameters, Total Suspended Solids in effluent (TSSe) and Phosphate in effluent (PO4(e)), and thirty-two operational variables are studied. RF models are developed, validated using DNN models as references, and shown to be suitable for VIM and PDP analyses. VIM identifies the variables that most strongly influence TSSe and PO4(e), while PDP elucidates their specific effects on TSSe, and PO4(e). The major findings are: (1) Influent temperature is the most influential variable for both TSSe and PO4(e), but it affects them in different ways; (2) PO4(e) depends strongly on the TSS in aeration basins - higher TSS concentrations in aeration basins generally promote PO4 removal, but excess TSS can have negative effects; (3) In general, the impact of TSS in aeration basins on TSSe and PO4(e) increases with the distances of the basin from the merging outlet, so more attention should be paid to the TSS concentration in the third or fourth aeration basins than the first and second ones; (4) Returning excessive amounts of sludge through the second return sludge pipe should be avoided because of its adverse impact on TSSe removal. These results could support the development of more advanced control strategies to increase control precision and reduce running costs in the Umea WWTP and other similarly configured WWTPs. The framework could also be applied to other parameters in WWTPs and industrial processes in general if sufficient high-resolution data are available. (C) 2021 The Authors. Published by Elsevier B.V.",,CE,Environmental,Wastewater treatment; Big data; Interpretable AI; Effluent quality; Process analytics,Science of The Total Environment
Journal Article,"G., Wang Wang, S., Li, W., Yang, H.",2025,What Determines Carbon Emissions of Multimodal Travel? Insights from Interpretable Machine Learning on Mobility Trajectory Data,Multidisciplinary Digital Publishing Institute (MDPI),17,15,,"Understanding the carbon emissions of multimodal travel—comprising walking, metro, bus, cycling, and ride-hailing—is essential for promoting sustainable urban mobility. However, most existing studies focus on single-mode travel, while underlying spatiotemporal and behavioral determinants remain insufficiently explored due to the lack of fine-grained data and interpretable analytical frameworks. This study proposes a novel integration of high-frequency, real-world mobility trajectory data with interpretable machine learning to systematically identify the key drivers of carbon emissions at the individual trip level. Firstly, multimodal travel chains are reconstructed using continuous GPS trajectory data collected in Beijing. Secondly, a model based on Calculate Emissions from Road Transport (COPERT) is developed to quantify trip-level CO<inf>2</inf> emissions. Thirdly, four interpretable machine learning models based on gradient boosting—XGBoost, GBDT, LightGBM, and CatBoost—are trained using transportation and built environment features to model the relationship between CO<inf>2</inf> emissions and a set of explanatory variables; finally, Shapley Additive exPlanations (SHAP) and partial dependence plots (PDPs) are used to interpret the model outputs, revealing key determinants and their non-linear interaction effects. The results show that transportation-related features account for 75.1% of the explained variance in emissions, with bus usage being the most influential single factor (contributing 22.6%). Built environment features explain the remaining 24.9%. The PDP analysis reveals that substantial emission reductions occur only when the shares of bus, metro, and cycling surpass threshold levels of approximately 40%, 40%, and 30%, respectively. Additionally, travel carbon emissions are minimized when trip origins and destinations are located within a 10 to 11 km radius of the central business district (CBD). This study advances the field by establishing a scalable, interpretable, and behaviorally grounded framework to assess carbon emissions from multimodal travel, providing actionable insights for low-carbon transport planning and policy design. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013130441&doi=10.3390%2Fsu17156983&partnerID=40&md5=591d20d7ed3a6ab51a8e921821cd129c,CE,Transportation,carbon emissions; multimodal travel; mobility trajectory data; interpretable machine learning,Sustainability
Journal Article,"H., Li Wang, J., Mao, P.",2025,Identifying factors influencing energy saving cognition-behavior gaps in shared residential spaces using interpretable machine learning: A dormitory case study,Elsevier Ltd,100,,,"Residential buildings are important contributors to building energy consumption. Occupants' energy-saving behaviors and cognition have significant impacts on building energy saving, which are often influenced by factors such as group and environment, especially in shared residential spaces. Several studies have examined factors influencing energy-saving behaviors, highlighting a significant gap between these behaviors and individuals' cognition regarding energy conservation. However, why such a gap exists have not been well explored. This research aims to identify and assess factors influencing occupants' energy-saving cognition-behavior gaps in shared residential spaces, using university dormitories as a case study. By comparing five interpretable machine learning algorithms, Random Forest was found to be the optimal algorithm. The contribution of different factors to the model was explained through SHapley Additive exPlanations (SHAP). Results show that compared with individual dimension factors, interaction dimension factors such as willingness to responding to cultural measures had a more important influence on energy-saving cognition-behavior gaps. Factors exhibited the importance variability across four groups of occupants. Management strategies designed to bridge energy-saving cognition-behavior gaps for each occupant group were proposed accordingly. This research innovatively proposed a new perspective of separating energy-saving cognition and behavior to explore influencing factors of occupants' energy conservation. Furthermore, this research provided valuable insights for building managers to develop tailored energy-saving strategies aligned with occupants’ cognitive and behavioural differences. By addressing these variations, managers can implement targeted interventions to enhance energy efficiency, optimize facility operations, and foster greater occupant engagement - key to advancing sustainability in the built environment. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214486638&doi=10.1016%2Fj.jobe.2025.111809&partnerID=40&md5=fe229a5f5f5d627dc04d26f49c245d30,CEM,Sustainability (Construction),Shared residential spaces; Building energy saving; Cognition-behavior gaps; Influencing factors; Interpretable machine learning,Journal of Building Engineering
Journal Article,"K., Zhang Wang, L., Fu, X.",2023,Time series prediction of tunnel boring machine (TBM) performance during excavation using causal explainable artificial intelligence (CX-AI),Elsevier B.V.,147,,,"Since early warning is significant to ensure high-quality tunneling boring machine (TBM) construction, a real-time prediction method based on TBM data is proposed. To solve the “black box” problem of prediction by artificial intelligence (AI) methods, the causal explainable gated recurrent unit (CX-GRU) is developed to achieve real-time prediction for TBM parameters. The approach is implemented in a tunnel construction project in Singapore and the results indicate that CX-GRU performs well with the R square score are 0.9140 and 0.9184 in real-time prediction for thrust force and soil pressure. The causal discovery component can increase the computational efficiency of model training by 8.8% on average. According to the SHAP analysis of prediction results, the thrust force is more sensitive to the input TBM features, while the soil pressure is more sensitive to historical data. The CX-GRU is more reliable and efficient when applied to TBM projects than traditional methods. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146048562&doi=10.1016%2Fj.autcon.2022.104730&partnerID=40&md5=e539438f388e549fa5f177f0b802fafc,CE,Geotechnical,TBM parameters; Real-time prediction; Causal discovery; Explainable AI; Time series,Automation in Construction
Journal Article,"S., Hasan Wang, M., Lu, M.",2024,Global Sensitivity Analysis Methodology for Construction Simulation Models: Multiple Linear Regressions versus Multilayer Perceptions,American Society of Civil Engineers (ASCE),150,5,,"In this research, the multilayer perceptron (MLP), also known as error backpropagation neural networks, is made transparent and explainable by contrasting with the commonly applied multiple linear regression (MLR). A novel MLP-based method for performing global sensitivity analysis is formalized to tackle complicated, nonexplainable simulation models or artificial intelligence (AI) models, which were developed to support critical decisions in construction engineering. The sensitivity analysis results serve as further evidence to validate the decision support models and lend new insights into the problems under investigation. The proposed new method was applied in two case studies in construction engineering, they are: precast viaduct installation cycles and concrete strength development. In both applications, the results of sensitivity analysis were represented in straightforward forms and effectively cross-checked with the existing knowledge of the problem domain or the experiences of construction practitioners. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187551697&doi=10.1061%2FJCEMD4.COENG-14059&partnerID=40&md5=f5d437f35f83b2bf17c7e0c3ed12e22d,CEM,Productivity / Workforce,Productivity model; Sensitivity analysis; Multilayer perception; Multiple linear regression (MLR); Explainable artificial intelligence (AI),Journal of Construction Engineering and Management
Journal Article,"S., Zeng Wang, X., Huang, Y., Li, X.",2024,"Analysing urban local cold air dynamics and climate functional zones using interpretable machine learning: A case study of Tianhe district, Guangzhou",Elsevier Ltd,114,,,"Deterioration of the thermal environment in built-up areas poses a serious threat to human health, comfort, and urban infrastructure, while also increasing energy consumption and carbon emissions. This underscores the need to optimize wind environments as a key mitigation strategy for urban areas. This paper analyzed the effects of human activities and natural factors on local cold air in Tianhe District, Guangzhou, from the perspective of local ventilation systems. The KLAM_21 (Kaltluft Abfluss Modell) was used to simulate local cold air flow and delineate climate functional zones. A random forest model, interpreted with the SHapley Additive exPlanation (SHAP) method, assessed the impact of various factors on local cold air dynamics. The study found that: (1) The northern mountainous area is a crucial cold source; (2) Some open spaces in the built environment fail to function as effective local cold air corridors; (3) High-intensity urban development hinders local cold air transmission; (4) Water bodies are more effective than green spaces in collecting and transmitting local cold air. This study provided technical methods for identifying climate functional zones and understanding local cold air dynamics, as well as theoretical support for the construction of local ventilation systems in urban areas. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201290189&doi=10.1016%2Fj.scs.2024.105731&partnerID=40&md5=f5c0481fdb535b1f91d4d27335f71694,CE,Environmental,Climate heat mitigation; Local cold air; Urban ventilation; KLAM_21; Random forest; SHAP,Sustainable Cities and Society
Journal Article,"S. Q., Liu Wang, H. Y., Cai, W. H., Huang, X., Sun, Q. C.",2025,Australian nationwide assessment of social vulnerability in two decades through its linkage to the built environment,Elsevier Ltd,163,,,"Evaluating social vulnerability and identifying at-risk populations are foundational to hazard risk mitigation and sustainable human settlement, yet these tasks entail distinct challenges that vary across urban-rural contexts and temporal scales. Despite Australia's escalating exposure to natural disasters, longitudinal, nationwide metrics of social vulnerability remain absent, with prior research constrained to localized or temporally fragmented analyses. Grounded in Cutter's social vulnerability framework, this study develops fine-grained, longitudinal indices of social vulnerability for urban and rural Australia across five census cycles (2001-2021). Spatial inequities, temporal trends, and associations between social vulnerability and 30 built environment indicators were analyzed using interpretable machine learning techniques. Key findings reveal that while the geographic distribution of the most vulnerable areas has remained stable over time, socioeconomic inequities of such areas have diminished whereas spatial clustering intensified. Resilient regions are characterized by enhanced access to public amenities, housing diversity, and commercial density-factors that collectively attenuate vulnerability. Furthermore, upgraded built environments attract socioeconomically advantaged populations, amplifying aggregated resilience. To our knowledge, this represents the first comprehensive, longitudinal, and spatially explicit national assessment of social vulnerability in Australia over two decades. The results elucidate critical trends in vulnerability and their interplay with built environment dynamics, offering actionable insights for place-based hazard mitigation, urban-rural sustainability, and infrastructure resilience. By bridging data gaps, this work addresses Australia's pressing need for evidence-driven policy and spatially targeted interventions to counter intensifying natural hazards and prioritize the resilience of human settlement.",,CE,Environmental,Social vulnerability; Spatial pattern; Inequity; Urban-rural disparity; Built environment; Australia,Habitat International
Journal Article,"W. C., Huang Wang, S. C., Wang, H. P., Cao, M. T.",2025,Measuring building information modeling user satisfaction by using active interpretable machine learning,Elsevier Ltd,183,,,"Accurately predicting building information modeling (BIM) user satisfaction (US) is essential for proactively addressing implementation challenges, ensuring effective adoption, and maximizing return on investment in BIM technologies in construction projects. Accordingly, this study developed advanced, interpretable boosting ensemble models to predict BIM US by integrating the forensic-based investigation (FBI) algorithm with gradient boosting machine, light gradient boosting machine, adaptive boosting (AdaBoost), extreme gradient boosting, and random forest algorithms. To validate the proposed models and establish a dataset, a comprehensive survey was conducted on 70 construction projects in Taiwan that used BIM technologies to support design work. Subsequently, the synthetic minority oversampling technique (SMOTE) was integrated into the proposed models to address the data imbalance problem. The results indicated that among all models, the FBI-AdaBoost-SMOTE model exhibited the highest performance, achieving accuracy, precision, recall, and F1 scores of 88.6 %, 90.6 %, 88.6 %, and 87.8 %, respectively. The FBI-AdaBoost model based on Shapley additive explanations identified contextual analysis and visualization, project scale, and cost estimates as key determinants of BIM US. Overall, this study presents an advanced machine learning framework for predicting BIM US and identifying key influencing factors for BIM US. It also provides actionable insights for stakeholders to enhance BIM implementation and user experience. In addition, this study highlights the potential of predictive modeling for optimizing the adoption of BIM in the architecture, engineering, and construction industry. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011495612&doi=10.1016%2Fj.asoc.2025.113663&partnerID=40&md5=616e50f6abf4b7a8630c89985952bd02,CEM,"Digital Construction (BIM, AI, Sensors, Robotics)",Building information modeling; Construction project; Satisfaction level; Machine learning; Boosting ensemble method,Applied Soft Computing
Journal Article,"Y., Zhang Wang, Z., Wang, Z., Wang, C., Cheng, C.",2024,Interpretable machine learning-based text classification method for construction quality defect reports,Elsevier Ltd,89,,,"Efficient identification and remediation of construction defects are critical for ensuring the quality and success of engineering projects. However, the complexity of construction environments poses challenges to this endeavor. Current research predominantly relies on statistical and causal analyses of defect detection reports, yet these methods are time-consuming and error-prone due to the unstructured nature of such reports. To address this, machine learning techniques have been applied to classify defect texts rapidly and accurately. However, existing studies primarily focus on model performance enhancement, neglecting interpretability and the effect of imbalanced data. This study introduces RF-SMOTE, an oversampling technique based on Random Forest (RF), to address the limitations of traditional methods like SMOTE. Comparative analyses demonstrated the efficacy of RF-SMOTE in mitigating imbalanced data effects. Further, the application of SHAP-based interpretability methods in construction management decision-making was explored, filling gaps in existing research. Contributions include providing interpretable machine learning solutions, discussing the effect of imbalanced data, and proposing SHAP-based application scenarios. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190594924&doi=10.1016%2Fj.jobe.2024.109330&partnerID=40&md5=b97c1f3ad183bf08cd0a6446031340a7,CEM,Project Management,Machine learning; Construction defects; Text classification; SHAP; SMOTE,Journal of Building Engineering
Journal Article,"Z., Zhou Wang, R., Rui, J., Yu, Y.",2025,Revealing the impact of urban spatial morphology on land surface temperature in plain and plateau cities using explainable machine learning,Elsevier Ltd,118,,,"Rapid urbanization has intensified urban heat island (UHI) effects, highlighting the need to understand UHI drivers to improve local thermal environments. While previous research has shown Urban spatial morphology significantly influences land surface temperature (LST), the mechanisms and characteristics of this impact across different geographic conditions remain unclear. Based on this, we selected the main urban areas of Chengdu and Lhasa as examples, using machine learning models and Shapley additive explanation (SHAP) method to reveal the linear and nonlinear relationships between Urban spatial morphology and LST from a morphological perspective. The results show that: (1) The built environment has the most significant impact on LST in plain cities, while the morphology of green space more strongly regulates LST in plateau cities. (2) Building height and density of core both reflect a role in reducing LST in plateau cities. (3) The interaction mechanisms of building density and building height features show the same trend in both plain and plateau cities. However, density of branch between 0.1 and 0.2 reduces LST in plain cities, while densities below 0.1 are more effective in reducing LST in plateau cities. Our results can provide refined and differentiated references for urban planners dedicated to mitigating UHI. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212140910&doi=10.1016%2Fj.scs.2024.106046&partnerID=40&md5=20bae2d93fcc6d47cb991a7f4ad9d846,CE,Environmental,Urban heat island; Land surface temperature; Urban spatial morphology; Machine learning; Shapley additive explanation; Plain and plateau cities,Sustainable Cities and Society
Journal Article,"X. K., Wang Wei, G. J., Grosser, P. F., Schmalz, B.",2024,Using explainable artificial intelligence (XAI) methods to understand the nonlinear relationship between the Three Gorges Dam and downstream flood,Elsevier B.V.,53,,,"Study Region: In the Yangtze River basin of China. Study focus: The emerging Explainable Artificial Intelligence (XAI) methods provide us an opportunity to understand the nonlinear relationship that the Deep Learning(DL) model learned inside. The construction of the Three Gorges Dam (TGD) has successfully minimized the likelihood of flooding in the Yangtze River basin. The XAI methods can help us to know the nonlinear relationship behind it. We apply the Long Short Term Memory (LSTM) network, in conjunction with two XAI methods, SHapley Additive exPlanation (SHAP) and Expected Gradient (EG), to do our work.In our DL model, we use YiChang (YC) station runoff,Precipitation (Pre) and vapour pressure deficit (VPD) data from the middle and lower river basin as input, while the output of the model generates runoff data at the DaTong (DT) station, XAI methods enable us to calculate the significance of each input feature is for generating the output feature in a DL model. In this study, we examine the difference in importance scores between the Before Three Gorges Dam (BTGD) period and the After Three Gorges Dam (ATGD) period. New Hydrological Insights for the Region: In the BTGD period, YC runoff was the primary contributor to flooding at the DT station. However, in the ATGD period, the largest contribution to flooding in the middle and lower Yangtze River basin has shifted from YC runoff to the the middle and lower reaches of precipitation. Our results suggest that the XAI can show the nonlinear relationship between the TGD and downstream flood clearly and the TGD can effectively mitigate flooding in the middle and lower river basins by regulating runoff from the upper river basin. The work shows the potential of XAI to explain the nonlinear relationship in the hydrology field.",,CE,Water,Nonlinear relationship; Runoff prediction; LSTM; Flood; XAI; TGD,Journal of Hydrology: Regional Studies
Journal Article,"Y., Ji Wei, R., Li, Q., Song, Z.",2023,Mechanical Performance Prediction Model of Steel Bridge Deck Pavement System Based on XGBoost,Multidisciplinary Digital Publishing Institute (MDPI),13,21,,"Steel bridges are widely used in bridge engineering. In the structural design of steel bridge deck pavement systems, engineers focus on obtaining mechanical properties by calculating design parameters and are keen to establish a quick and accurate solution method. Because of the complex knowledge system involved in the numerical calculation method, it is difficult for the general engineering designer to master it. Researchers have started using artificial intelligence algorithms to solve problems in civil engineering. This study developed an XGBoost-based mechanical performance prediction model for steel bridge deck pavement systems. First, numerical simulation tests are conducted at unfavorable load locations using a finite element model to establish a dataset. Then, an XGBoost model is built using this dataset, and its parameters are optimized and compared with traditional machine learning models. Finally, an explanatory analysis of the model is performed using SHAP, an interpretable machine learning framework. The results indicate that the developed XGBoost model accurately predicts the mechanical properties of steel bridge deck pavement systems. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192375639&doi=10.3390%2Fapp132112048&partnerID=40&md5=096c8430539d28a60be4cf5c9c32b563,CE,Structural,steel bridge deck pavement; artificial intelligence; ensemble learning; mechanical property prediction; feature importance,Applied Sciences
Journal Article,"F., Zhang Wen, Y., Du, P., Zhang, Z., Zhang, B., Zhang, Y.",2024,Factors Influencing the Usage Frequency of Community Elderly Care Facilities and Their Functional Spaces: A Multilevel Based Study,Multidisciplinary Digital Publishing Institute (MDPI),14,6,,"The construction of community elderly care facilities (CECF) is pivotal for promoting healthy aging and “aging in place” for older people. This study focuses on the low utilization rates of community elderly care facilities in the Dongcheng and Xicheng Districts, core areas of Beijing. The explainable machine learning method is used to analyze data across three dimensions: the elderly’s individual attributes, characteristics of the community elderly care station (CECS), and features of the built environment around CECS and subdistrict, to identify the important factors that influence the usage frequency of overall CECS and its different functional spaces, and also the correlation between factors and usage frequency of CECS. It shows that the most important factors are the features of CSCF, including the degree of space acceptance and satisfaction with services provided, which influence the usage frequency of nine functional spaces (R2 ≥ 0.68) and overall (R2 = 0.56). In addition, older people’s individual factors, such as age and physical condition, significantly influence the usage of specific spaces such as rehabilitation therapy rooms and assistive bathing rooms. The influence of built environment characteristics is relatively low, with factors such as the density of bus stations and housing prices within the subdistrict and the mean distance from CECF to the nearest subway stations being more important. These findings provide a reference for the construction of indoor environments, management of service quality, and optimal site selection for future community elderly care facilities. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197857202&doi=10.3390%2Fbuildings14061827&partnerID=40&md5=5ebb1d9ea5075bbfc9b0495b220b622a,CE,Environmental,community elderly care facilities (CECF); community elderly care station (CECS); indoor space acceptance; machine learning; quality of service; usage frequency,Buildings
Journal Article,"B. Y., Alahakoon Wickramasuriya, Y., Krishantha, B. R. G. A., Alawatugoda, J., Ekanayake, I. U.",2025,Prediction of the strength characteristics of basalt fibre reinforced concrete using explainable machine learning models,Springer Nature,7,8,,"Adding basalt fibre to concrete has become a promising way to improve its strength under different loading conditions. Predicting the strength of basalt fibre reinforced concrete (BFRC) is more complex than for conventional concrete. Traditional methods rely on iterative experiments to understand the underlying behaviour. This study applies machine learning (ML) models, paired with explainable artificial intelligence (XAI), to predict the compressive, flexural, and tensile strengths of BFRC efficiently and transparently. Three datasets, each with 267 samples, were used for model development. Each sample included 10 features: cement, fly ash, silica ash, coarse aggregate, fine aggregate, water, superplasticiser, fibre diameter, fibre length, and fibre content. Three ML models, random forest (RF), support vector machine (SVM), and decision tree (DT), were evaluated. RF achieved the highest accuracy for compressive (R2 = 0.926, MSE = 9.4, MAE = 2.3) and flexural strength (R2 = 0.882, MSE = 0.5, MAE = 0.5), while DT performed best for tensile strength (R2 = 0.935, MSE = 0.2, MAE = 0.3). Shapley additive explanation (SHAP)-based explainable artificial intelligence (XAI) methods revealed key predictors: fine aggregate for compressive strength, fibre diameter for tensile strength, and silica ash for flexural strength. Also, the authors developed a web-based online application to predict the strength (compressive, flexural and tensile) to improve the accessibility and usability of the findings. This approach offers a practical alternative to experimental testing and contributes to a better understanding of BFRC behaviour. It holds strong potential for adoption in the construction industry. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012771207&doi=10.1007%2Fs42452-025-07528-7&partnerID=40&md5=ae0b0203eb4d039bd47b30213feb9bae,CE,Materials,Basalt fibre reinforcement concrete; Machine learning; Mechanical properties; Random forest; Explainable artificial intelligence,Discover Applied Sciences
Journal Article,"A., Wang Wu, Z.",2025,Exploring urban building carbon sinks: A SHAP-driven machine learning approach,Elsevier Ltd,126,,,"Cement-based materials in urban buildings can absorb atmospheric carbon dioxide through carbonation, presenting substantial potential for urban carbon sequestration. Accurately estimating and interpreting the carbon sink capacity of urban buildings is essential for effective urban carbon management. In this study, we develop an interpretable machine learning framework that integrates a multilayer perceptron neural network with Shapley additive explanation methods and a causal forest model to quantify building carbon sinks in the central urban area of Guiyang, China. The results indicate that this region's total carbon sequestered by buildings is approximately 1.822 million metric tons. Residential buildings account for about 68.2 % of the total carbon sink among different building types. The Shapley additive explanations analysis identifies building volume, footprint area, and perimeter as the most influential predictive variables. However, the causal forest analysis further reveals that building height and slope have the most potent direct causal effects, highlighting key structural factors influencing carbon sequestration. The significant discrepancy between predictive importance and causal contribution underscores the need to incorporate causal reasoning into urban carbon management strategies. These findings provide a robust methodological foundation for accurately assessing building-related carbon sinks and offer critical insights for sustainable urban planning and carbon neutrality strategies. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004260554&doi=10.1016%2Fj.scs.2025.106428&partnerID=40&md5=fbccbb82a90eb0175b97c7479598e877,CE,Environmental,Building carbon sinks; Machine learning; SHAP analysis; Causal forests,Sustainable Cities and Society
Journal Article,"R., Fujita Wu, Y., Soga, K.",2020,Integrating domain knowledge with deep learning models: An interpretable AI system for automatic work progress identification of NATM tunnels,Elsevier Ltd,105,,,"Finding a reliable and cost-effective approach to monitor the activities of the New Austrian Tunneling Method (NATM) tunnel construction automatically is a challenging yet important task. This study presents an interpretable artificial intelligence (AI) framework that automatically identifies NATM construction works using low-cost site surveillance images. The framework adopts the Bayesian statistics to combine the prior NATM construction knowledge with the visual evidence extracted by deep learning (DL) based computer vision models. The analysis results of Site CCTV surveillance videos of four NATM tunneling projects are presented to demonstrate its ability (i) to label NATM work cycles from the work timeline, (ii) to identify NATM work categories inside each work cycle, and (iii) to estimate the degree of plan-work deviation at the construction cycle level. The proposed framework yields promising results on a real NATM tunneling project. © 2020 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089917025&doi=10.1016%2Fj.tust.2020.103558&partnerID=40&md5=9f870703aafab9a1d66c0b2cebf3dafc,CEM,"Digital Construction (BIM, AI, Sensors, Robotics)",Artificial intelligence; NATM project monitoring; Deep learning; Computer vision,Tunnelling and Underground Space Technology
Journal Article,"Z. Y., Lyu Xing, G., Yao, Y., Liu, Z., Zhang, X. D.",2025,"Fine-grained analysis and mapping of urban flood susceptibility with interpretable machine learning: A case study of Hefei, China",Elsevier B.V.,60,,,"Study region: Built-up area of Hefei City, China. Study focus: Climate change has increased frequency of extreme rainfall events. Mapping the urban flood susceptibility and exploring the impact factors can enhance urban resilience. Existing methods often treat cities as uniform entities, making it challenging to capture the complexity of these localized characteristics. This paper proposes a novel approach combining interpretable machine learning and spatial autocorrelation. An ensemble learning model assesses susceptibility by incorporating terrain, urban construction, and precipitation factors. An improved spatial weight matrix is proposed to perform spatial autocorrelation for revealing spatial distribution of flood susceptibility, and the local factors are explained by LIME to provide a fine-grained analysis of different regions. New hydrological insights for the region: (1)NDVI is the most influential factor emphasizing the importance of green spaces in urban flood regulation. (2)Micro-topography significantly affects urban flood susceptibility, and normalizing DSM based on micro-watersheds provides an accurate representation. (3)High flood susceptibility in Hefei, as revealed by spatial autocorrelation analysis, follows patterns similar to built-up areas and is influenced by major roads. Based on this, LIME analysis reveals distinct regional impact factors, such as NDVI, land use, distance to water bodies, and road density, supporting targeted flood management strategies. These findings can provide valuable insights for flood prevention and urban planning, contributing to the overall resilience of the urban environment.",,CE,Water,Urban flood; Flood susceptibility mapping; Machine learning; Interpretability; Spatial weight matrix,Journal of Hydrology: Regional Studies
Journal Article,"C. G., Xiong Xu, W., Zhang, S. M., Shi, H. L., Wu, S. C., Bao, S. J., Xiao, T. Q.",2025,"Research on the Nonlinear Relationship Between Carbon Emissions from Residential Land and the Built Environment: A Case Study of Susong County, Anhui Province Using the XGBoost-SHAP Model",Multidisciplinary Digital Publishing Institute (MDPI),14,3,,"Residential land is the basic unit of urban-scale carbon emissions (CEs). Quantifying and predicting CEs from residential land are conducive to achieving urban carbon neutrality. This study took 84 residential communities in Susong County, Anhui Province as its research object, exploring the nonlinear relationship between the urban built environment and CEs from residential land. By identifying CEs from residential land through building electricity consumption, 14 built environment indicators, including land area (LA), floor area ratio (FAR), greening ratio (GA), building density (BD), gross floor area (GFA), land use mix rate (Phh), and permanent population density (PPD), were selected to establish an interpretable machine learning (ML) model based on the XGBoost-SHAP attribution analysis framework. The research results show that, first, the goodness of fit of the XGBoost model reached 91.9%, and its prediction accuracy was better than that of gradient boosting decision tree (GBDT), random forest (RF), the Adaboost model, and the traditional logistic model. Second, compared with other ML models, the XGBoost-SHAP model explained the influencing factors of CEs from residential land more clearly. The SHAP attribution analysis results indicate that BD, FAR, and Phh were the most important factors affecting CEs. Third, there was a significant nonlinear relationship and threshold effect between built environment characteristic variables and CEs from residential land. Fourth, there was an interaction between different dimensions of environmental factors, and BD, FAR, and Phh played a dominant role in the interaction. Reducing FAR is considered to be an effective CE reduction strategy. This research provides practical suggestions for urban planners on reducing CEs from residential land, which has important policy implications and practical significance.",,CE,Environmental,residential land; machine learning; XGBoost model; SHAP algorithm; carbon emissions; built environment factors; nonlinear relationship,Land
Journal Article,"G. Y., Zhou Xu, G. X., Althoey, F., Hadidi, H. M., Alaskar, A., Hassan, A. M., Farooq, F.",2023,Evaluation of properties of bio-composite with interpretable machine learning approaches: optimization and hyper tuning,Elsevier B.V.,25,,1421–1446,"Hemp bio-composite (HBC) is a sustainable material that can be considered as a carbon negative"" or ""better-than-zero-carbon"" because it absorbs more carbon from its surrounding than it releases during its manufacturing and application on the field. Because of their eco-friendliness, these bio-composites not only improve structure thermal efficiency but also encourage sustainable construction. However, due to its heterogeneous nature, the majority of the published research has experimentally investigated the characteristics of bio-composites. While the mathematical models remain still a challenge for academics. Therefore, three machine learning methods known as deep neural network (DNN), gene expression programming (GEP), and optimizable Gaussian process regressor (OGPR) are used to build up prediction models for compressive strength (CS) and thermal conductivity (TC) of hemp bio-composite. A total of 86 and 159 experimental values for TC and CS, respectively, were collected from the literature. To formulate the models, the ten most dominant input variables were chosen. To evaluate the performance of the models, various statistical metrics were employed. Based on statistical indicators, the increasing order of prediction capabilities of the established models is DNN>OGPR>GEP. The value of the correlation coefficient (R) for CS models is 0.9893 (GEP), 0.9997 (DNN), and 0.9995 (OGPR). While that for TC models are 0.9847 (GEP), 0.9999 (DNN), and 0.9999 (OGPR). The values of the performance index (r) and objective function (OF) are approaching 0. Thus, indicating the outburst performance of the developed models. The comparison of machine learning (ML) models with traditional regression models further confirms the accuracy of the developed models. Furthermore, contour maps analysis was conducted to get an insight into the ranges of input variables that were employed by global researchers to gain higher strength and thermal properties. For the interpretation of the models, Shapley Additive explanations (SHAP) analysis was carried out. The result revealed that the influencing trend of input variables is in close agreement with experimental studies. Thus, practi-tioners and designers to avoid hectic experimental tests to save time and cost can utilize these ML-based developed models in field projects. However, it is recommended to use more updated and extensive data to further validate the models and utilize other ML al-gorithms and post hoc interpretation techniques for the model's explanations.(c) 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).""",,CE,Materials,Hemp bio-composite; Thermal conductivity; Compressive strength; Machine learning; Deep neural network; Gene expression programming; Optimizable Gaussian process regressor,Journal of Materials Research and Technology
Journal Article,"W., Gu Xu, X.",2025,Spatiotemporal impacts of purpose-specific human mobility on air pollution: Evidence from taxi trajectories and interpretable machine learning,Elsevier Ltd,126,,,"Human mobility exerts significant influences on urban air pollution. Regrettably, most existing studies treated mobility as a homogeneous entity, neglecting that its effects may vary by travel purposes due to distinct spatiotemporal patterns. To address this gap, this study utilizes a trip purpose inference algorithm to classify mobility based on Beijing's three-month taxi trajectory data and examines its impact on air pollution using interpretable XGBoost-SHAP models. The correlational analysis indicates the substantial contribution of wind, temperature, and precipitation to air pollution. Human mobility's contribution is less significant than the abovementioned natural environments but greater than built environments, such as building density and height. In the long term, the negative correlation between work- and home-purpose mobility and pollution challenges the assumption that more mobility always increases pollution. Based on the case study in Beijing, this research eventually proposed possible practical implications and suggestions for sustainable urban planning and management, including promoting mixed-use development and work-residence integration, creating urban wind corridors and open green spaces, and adopting low-emission transportation while avoiding blanket traffic restrictions. This study uses interpretable machine learning models to clarify complex variable relationships, while future research could explore causality to better understand the underlying mechanisms. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004412972&doi=10.1016%2Fj.scs.2025.106411&partnerID=40&md5=eb1f418727297e7a8120bcfa548ced4b,CE,Environmental,Human mobility; Air pollution; Taxi trajectory data; Trip purpose inference; XGBoost; SHAP,Sustainable Cities and Society
Journal Article,"Y., Yan Xu, X., Liu, X., Zhao, X.",2021,Identifying key factors associated with ridesplitting adoption rate and modeling their nonlinear relationships,Elsevier Ltd,144,,170–188,"Ridesharing is critical for promoting transportation sustainability. As a new form of ridesharing services, ridesplitting has rarely been studied. Based on the Chicago ridesourcing trip data, this study explores how ridesplitting adoption rate (i.e., the proportion of ridesourcing trips with ridesharing authorization) varies across space and what factors are associated with these variations. We find large variations in ridesplitting adoption rates across neighborhoods (Census Tracts) and across origin–destination (Census-Tract-to-Census-Tract) pairs. Particularly, the ridesplitting adoption rate is low for airport rides. We further apply a random forest model to explore which factors are key predictors of ridesplitting adoption rate across O-D pairs and to explore their nonlinear associations. The results suggest that the socioeconomic and demographic variables collectively contribute to 68.60% of the predictive power of the model, but travel-cost variables and built-environment-related factors are also important. The most important variables associated with ridesplitting adoption are ethnic composition, median household income, education level, trip distance, and neighborhood density. We further examine the nonlinear association between neighborhood ridesplitting adoption rate and several key variables such as the percentage of white population, median household income, and neighborhood Walk Score. The revealed nonlinear patterns can help transportation professionals identify neighborhoods with the greatest potential to promote ridesplitting. © 2021 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099206476&doi=10.1016%2Fj.tra.2020.12.005&partnerID=40&md5=fa7ace01b80805b20c50a40d67dbcb1e,CE,Transportation,Ridesplitting; Random forest; Nonlinearity; Variable importance; Interpretable machine learning; Ridesharing,Transportation Research Part A: Policy and Practice
Journal Article,"Y., Zhang Xu, D., Lin, J., Peng, Q., Lei, X., Jin, T., Wang, J., Yuan, R.",2024,Prediction of phytoplankton biomass and identification of key influencing factors using interpretable machine learning models,Elsevier B.V.,158,,,"The water quality of the Middle Route of the South-to-North Water Diversion Project (MRP) of China is related to the health and safety of about 8500w people. In recent years, multiple occurrences of abnormal algal proliferation in water conveyance channels have posed potential risks to the water quality of diverted water. To clarify the growth situation of phytoplankton and evaluate the response relationships between water environmental parameters and phytoplankton in the MRP, in this study, a long-term monitoring dataset was collected from seven monitoring sites from May 2015 to December 2020 to statistically analyse the spatiotemporal characteristics of planktonic algal cell density (ACD). Based on this, four machine learning models, including multiple linear regression (MLR), support vector regression (SVR), random forest (RF), and extreme gradient boosting (XBGoost), were constructed to predict the ACD, and the SHapely Additive exPlanations (SHAP) method was used to interpret the best prediction model for identifying the response relationship between water environmental parameters and ACD. The results showed that (1) the seven monitoring sites could be divided into two significant groups (i.e., Group I and Group II) by hierarchical clustering analysis (HCA), among which the ACD of Group II was higher than that of Group I. (2) the performance of four prediction models using different evaluation metrics indicated that the RF model surpassed in prediction accuracy compared with the other three models in predicting ACD variations in Group I and Group II. (3) SHAP analysis revealed that the key factors affecting ACD variations in Group I were water temperature (WT), water depth (WD), permanganate index (PI), flow rate (Q) and dissolved oxygen (DO), and the ACD were inhibited when WT was below 23 °C, WD exceeded 6.5 m, and Q was between 100 and 250 m3/s. Additionally, WT, PI, and DO were the most important predictors of ACD in Group II, and ACD were inhibited when WT was below 24 °C, PI was lower than 2.4 mg/L and DO was higher than 9 mg/L. This research provides a theoretical basis and reference for water quality management and algal ecological control of water transfer projects with high spatial heterogeneity. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183575690&doi=10.1016%2Fj.ecolind.2023.111320&partnerID=40&md5=70dfb9183e5d452831649d49e3b41284,CE,Water,South-to-North Water Diversion Project; Abnormal algal proliferation; Planktonic algal cell density prediction; Machine learning; SHapely Additive exPlanations,Ecological Indicators
Journal Article,"D. S., Xue Yang, D. X., Xu, H., Duan, W. H.",2024,Low-cycle fatigue design for reinforced high-strength concrete under high compressive stress,Elsevier Ltd,20,,,"As engineering endeavors push the boundaries of material and design capabilities, the significance of understanding and mitigating fatigue in construction materials becomes paramount. This study specifically investigates the low-cycle fatigue performance of reinforced high-strength concrete (RHSC). Using rigorous data collection, we established a clear link between interpretable machine learning analysis and the fatigue properties of RHSC. A trained model was developed, yielding a straightforward formula tailored to low-cycle fatigue design considerations for RHSC. This model stands as a testament to the potential for marrying traditional engineering practices with advanced statistical techniques. Our results emphasize that, when appropriately applied, regression analysis can be instrumental in enhancing the durability and longevity of RHSC structures exposed to dynamic loadings. This research not only underscores the pivotal role of statistical methods in fatigue design but also champions the broader adoption of such techniques in evolving engineering landscapes.",,CE,Materials,High-strength concrete; Fatigue failure; Low-cycle fatigue; Machine learning,Case Studies in Construction Materials
Journal Article,"Y., Hou Yang, K., Sun, H., Guo, L., Zhe, Y.",2024,Prediction Model and Knowledge Discovery for Roof Stress in Mined-Out Areas Integrating 3D Scanning Image Features,Multidisciplinary Digital Publishing Institute (MDPI),14,23,,"The accurate prediction of roof stress in mined-out areas is crucial for ensuring mine safety. However, existing study methods often overlook the increasingly available image data and fail to balance the model predictive capability with interpretability. To address these issues, this study innovatively integrates 3D laser scanning image features into the prediction of roof stress in mined-out areas. Image features are extracted using pre-trained deep-learning models and combined with traditional geological parameters to construct multiple machine-learning models for prediction. The experimental results demonstrate that models incorporating image features significantly outperform traditional models that rely solely on geological parameters in terms of prediction accuracy, interpretability, and complexity, revealing the critical role of image features in stress prediction. Furthermore, the use of SHapley Additive exPlanations (SHAP) to interpret the random forest model uncovers new domain knowledge, such as the relationship between spatial patterns and stress concentration. This study theoretically validates the effectiveness of image data and effectively balances the predictive capability and interpretability of the model, facilitating knowledge discovery in the field. On a practical level, it also provides guidance for mine safety management. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211940626&doi=10.3390%2Fapp142311180&partnerID=40&md5=94d1f9c13088212b5719b18c589d23ec,CE,Geotechnical,mined-out area; roof stress prediction; 3D laser scanning images; interpretable machine learning,Applied Sciences
Journal Article,"Jiawei, Jian Yao, Yixin, Shen, Yanting, Wen, Wen, Huang, Chenyu, Wang, Jinyu, Fu, Jiayan, Yu, Zhongqi, Zhang, Yecheng",2025,"Decoding the Spatial Heterogeneity of Bike-Sharing Impacts: Machine Learning Model of Meteorology, Epidemic, and Urban Factors",American Society of Civil Engineers,151,2,04025009,"Previous studies on the factors affecting bike-sharing travel (BST) have not considered spatial differences, leading to insufficient understanding of the complex impacts of variables in different geographical locations. This study aims to reveal the differential spatial impacts of meteorological conditions, epidemics, and urban spatial variables on BST. Firstly, New York was selected as the study area, and the period from 2020 to 2021 was chosen for the study. Secondly, a high-precision urban information data set, including meteorological, epidemic, and urban spatial variables, was constructed using weighted Thiessen polygons as the segmentation method. Finally, machine learning was conducted, and the XGBoost ensemble learning algorithm, which yielded the best training results, was chosen for interpretable analysis. This examined the nonlinear correlations and spatial benefits of each variable with BST. The results show that (1) the impact of average temperature on shared bicycle travel is most significant among all factors, accounting for 26.15% of the total impact; (2) there is significant spatial heterogeneity in the influence of factors, and office closeness is negatively correlated with BST, contributing positively in the west and negatively in the east; (3) the southern part of Manhattan is significantly affected by meteorological (?SHAP value? = 484.18) and urban spatial sector (?SHAP value? = 122.65), while the central part of Manhattan is most significantly influenced by epidemic variables (?SHAP value? = 469.27). In summary, this study takes New York as an example to analyze the nonlinear effects and spatial benefits of meteorology, epidemics, and urban space on shared bicycle travel. Based on this, more targeted and effective urban traffic intervention strategies are provided for different regions of the city. This study provides valuable insights into how different factors influence bike-sharing travel in New York City, offering practical implications for urban planners and policymakers. Firstly, we found that temperature has the most significant impact on bike-sharing, highlighting the necessity for climate-conscious dynamic urban planning. Secondly, our research reveals that the effects of meteorological, epidemic, and urban space factors vary greatly across different regions. The study identifies that the areas most sensitive to these factors are in Midtown and Lower Manhattan, which should be prioritized for interventions. Additionally, southern Manhattan is significantly affected by meteorology and urban space factors, while central Manhattan is more influenced by the number of confirmed COVID-19 cases. This underscores the importance of targeted policies. In southern Manhattan, measures such as improving the microclimate through street shading and increasing the accessibility of public and commercial spaces can encourage bike-sharing. Conversely, in central Manhattan, monitoring epidemic trends and managing the number of cases is crucial. By considering the spatial differences in factors affecting bike-sharing travel, cities can develop more effective, region-specific transportation strategies, ultimately enhancing urban resilience and reducing the impact of epidemics.",https://doi.org/10.1061/JUPDDM.UPENG-5192,CE,Transportation,Meteorology; Epidemics; Urban space; Interpretable analysis; Spatial benefits,Journal of Urban Planning and Development
Journal Article,"S., Tu Yi, W., Zhao, T., Li, X., Zhang, Y., Li, D., Rodriguez, J., Sun, Y.",2025,Uncovering nonlinear urban factors of homelessness: Evidence from New York City using interpretable machine learning,SAGE Publications Ltd,,,,"Urban homelessness is a complex issue rooted in structural inequalities and spatial disparities, significantly affecting urban life and well-being. Existing research often relies on survey-based or linear regression methods, which are limited in scope, coverage, and their ability to capture nonlinear associations. This study addresses these gaps by combining homeless incident reports from New York City’s 311 service with multi-source urban big data and employing a Light Gradient Boosting Machine (LightGBM) model alongside SHapley Additive Explanations (SHAP). Through a census tract-level analysis, we examine how socioeconomic, built environment, transportation, and urban landscape factors relate to homelessness incidence. Our findings show that (1) the importance of predictive factors varies across location types, for instance, information, and communication POIs are most predictive in commercial areas, while felony crime and median income dominate in residential zones; (2) socioeconomic and built environment features are consistently more important than transportation and visual landscape indicators; and (3) many factors exhibit nonlinear relationships and threshold effects, such as sharp increases in homelessness beyond a median rent of $1800 or Gini index of 0.53. These findings offer new insights into the spatial distribution and drivers of homelessness and underscore the value of interpretable machine learning in urban analytics. By identifying key environmental thresholds, this study provides evidence-based guidance for spatially targeted urban interventions, such as prioritizing support services in high-risk areas and designing inclusive public spaces that can help mitigate homelessness and promote more sustainable and equitable cities. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004932184&doi=10.1177%2F23998083251342406&partnerID=40&md5=baddfc22c57aa803afc89e586083981f,CE,Environmental,Homelessness; urban big data; street view imagery; interpretable machine learning; SHapley additive explanations,Urban Analytics and City Science
Journal Article,"G. M., Huang Yin, Z., Fu, C., Ren, S. L., Bao, Y., Ma, X. L.",2024,"Examining active travel behavior through explainable machine learning: Insights from Beijing, China",Elsevier Ltd,127,,,"Active travel, namely walking and cycling, is an eco-friendly and socially beneficial mode of sustainable transportation. However, existing research on active travel relies on limited survey data and generalized linear models. To fill the gap, our study integrates large-scale big trip data and data-driven machine learning to simultaneously predict active travel flow and probability. We employ SHapley Additive exPlanation to analyze the nonlinear effects of various characteristics (e.g., travel, socioeconomic, infrastructure, environment) on active travel. Gradient Boosting Decision Tree performs best for both prediction tasks. The overall importance of travel distance is over 50% to the model. Features like crow-fly distance, housing price, point-of-interest density, subway proximity, building area/road density, and urban greenery exhibit pronounced nonlinear effects. Local interpretability analysis reveals the determinants of specific trips, facilitating targeted optimization implications. Our study reveals the drivers and nonlinearities of active travel behavior and aids sustainable transportation planning.",,CE,Transportation,Urban transportation; Active travel; Active mobility; Walking and cycling; Explainable machine learning; SHAP; Geospatial big data,Transportation Research Part D: Transport and Environment
Journal Article,"X., Mu Yin, Z., Cui, Q., Sun, T.",2025,Interpretable and spatio-distributed settlement related multimode process monitoring for Metro tunnels excavated by TBM,Elsevier Ltd,66,,,"Monitoring ground settlement states during tunnel boring machine (TBM) excavation in metro tunnels is crucial for construction quality and disaster prevention. However, current studies mainly focus on anomaly detection and alarm of settlement measurements, with less attention to monitoring underlying excavation influencing parameters. Moreover, these parameters commonly vary following the geological types and TBM operating states, which means if the monitor method fails to distinguish the difference in geological types and operating modes may trigger false alarms. To address above issues, an index-related operating parameters anomaly detection framework with full consideration of the multi-point distribution intrinsic property for settlement is proposed. This framework realizes adaptive monitoring across different geological types and operating modes and provides interpretability analysis for detection results through Explainable Artificial Intelligence (XAI). Specifically, the properties of spatio-distributed settlement are firstly represented and utilized to enhance monitoring capability. Then, geological types are identified, and working condition division rules are deduced from variable effect parameters. Next, dual monitoring statistics and control limits are derived by latent variables and reconstruction errors of Variational Autoencoder (VAE) to monitor the settlement-related parameters of main mode in the most common geological type. The improved instance transfer strategy K-TrA, enhancing source-target domain similarity and reducing iteration time, is elaborated to adapt to variable working modes. Meanwhile, parameter transfer based fine-tuning strategy is adopted to monitor the main mode in other geological types. Afterward, SHapley Additive exPlanations(SHAP) is utilized to further analyze the contributions of various parameters to detection results. Finally, the performance of proposed method is verified on a real-case. Ablation and comparative experiments confirm its superiority in predictive accuracy and anomaly detection rate. Thus, our work can offer new insights for settlement monitoring and valuable guidance to field engineers. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005518732&doi=10.1016%2Fj.aei.2025.103464&partnerID=40&md5=35376fd84ad4bbf47aca149ed1983a78,CE,Geotechnical,Multi-points distributed settlement; Settlement-related monitoring; Multimode process; Interpretability analysis; TBM excavation for metro tunnels,Advanced Engineering Informatics
Journal Article,"Gunwoo, Liu Yong, Meiyin, Lee, SangHyun",2024,Explainable Image Captioning to Identify Ergonomic Problems and Solutions for Construction Workers,American Society of Civil Engineers,38,4,04024022,"The high occurrence of work-related musculoskeletal disorders (WMSDs) in construction remains a pressing concern, causing numerous nonfatal injuries. Preventing WMSDs necessitates the implementation of an ergonomic process, encompassing the identification of ergonomic problems and corresponding solutions. Finding ergonomic problems and solutions within active construction sites requires significant efforts from personnel possessing ergonomics expertise. However, ergonomic experts and training programs are often lacking in construction. To address this issue, the authors applied deep learning (DL)?based explainable image captioning to identify ergonomic problems and their corresponding solutions from images that are prevalent in construction sites. To this end, the authors proposed a vision-language model (VLM) capable of identifying ergonomic problems and their solutions, aided by data augmentation. The bilingual evaluation understudy (BLEU) score was used to measure the similarity between ergonomic problems and solutions identified by the proposed VLM and those specified in an ergonomic guideline. Testing with 222 real-site images, the proposed VLM achieved the highest BLEU-4 score, 0.796, compared with the traditional convolutional neural network-long short-term memory and a state-of-the-art VLM, the bootstrapping language-image pretraining. In addition, the authors developed an explainability module, visualizing which specific areas of the images the proposed VLM focuses on when identifying ergonomic problems and the important words for identifying ergonomic solutions. The highest BLEU score and the visual explanations demonstrate the potential and credibility of the proposed VLM in identifying ergonomic problems and their solutions. The proposed VLM and explainability module greatly contribute to implementing the ergonomic process in construction, identifying ergonomic problems and their solutions only with site images. To prevent WMSDs, the National Institute of Occupational Safety and Health (NIOSH) recommends implementing an ergonomic process, which encompasses ergonomic problem identification, ergonomic risk assessment, and ergonomic solution identification. The current practice on sites relies on the intermittent implementation of manual ergonomic processes, and thus often falls short in protecting workers against WMSDs due to rapidly changing site conditions and the lack of on-site ergonomic expertise. Addressing this, many automated tools have been developed for ergonomic risk assessment, but none for ergonomic problem and solution identification. Therefore, with these assessment tools, we aim to streamline the recommended ergonomic process in an automated manner. To this end, we propose a deep learning?based explainable image captioning model for automated ergonomic problem and solution identification. Utilizing an ordinary camera (e.g., smartphones and site surveillance cameras), safety managers can easily identify ergonomic problems, assess risk levels, and identify corresponding solutions. Additionally, our model provides justification for its identification by visualizing the reason behind the identified ergonomic problems and solutions. With such an easily accessible and trustworthy model, the on-site ergonomic process can be streamlined, potentially reducing workers? WMSDs.",https://doi.org/10.1061/JCCEE5.CPENG-5744,CEM,Safety,NaN,Journal of Computing in Civil Engineering
Journal Article,"M., Noureldin Yossef, M., Alqabbany, A.",2024,Explainable artificial intelligence framework for FRP composites design,Elsevier Ltd,341,,,"Fiber-reinforced polymer (FRP) materials are integral to various industries, from automotive and aerospace to infrastructure and construction. While FRP composite design guidelines have been established, the process of obtaining the desired strength of an FRP composite demands considerable time and resources. Despite recent advancements in Machine Learning (ML) models which are commonly used as predictive models, the inherent 'black box' nature of those models poses challenges in understanding the relationship between input design parameters and output strength of the composite. Moreover, these models do not provide tools to facilitate the designing process of the composite. The current study introduces an explainable Artificial Intelligence (XAI) framework that will provide understanding for the input–output relationships of the model through SHapley Additive exPlanations (SHAP) and Partial Dependence Plots (PDPs). In addition, the framework provides for the first time a designing approach for adjusting the important design parameters to obtain the desired composite strength by the designer through utilizing an explainability technique called Counterfactual (CF). The framework is evaluated through the design of a 14-ply composite, successfully identifying critical design parameters, and specifying necessary adjustments to meet strength requirements. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193621586&doi=10.1016%2Fj.compstruct.2024.118190&partnerID=40&md5=8631cdc77c745d220de32de99e63fed2,CE,Materials,Composite design; FRP; Explainable artificial intelligence; Machine Learning; Counterfactual; Casual AI; SHapley Additive exPlanations; Partial Dependence Plots,Composite Structures
Journal Article,"S., Pamungmoon Youwai, A.",2025,Developing an explainable artificial intelligent (XAI) model for predicting pile driving vibrations in Bangkok’s subsoil,Springer Science and Business Media Deutschland GmbH,37,18,12881–12902,"This study presents an explainable artificial intelligence model for predicting pile driving vibrations in Bangkok’s soft clay subsoil. A deep neural network was developed using 1,018 real-world pile driving measurements, encompassing various pile and hammer characteristics, sensor locations, and vibration measurement axes. The model achieved a mean absolute error of 0.276, outperforming traditional empirical methods and other machine learning approaches. SHapley Additive exPlanations analysis was employed to provide both global and local explanations of the model’s predictions. Global explanations revealed complex relationships between input features and peak particle velocity, with distance from the pile driving location emerging as the most influential factor. Local explanations enabled interpretation of individual predictions, allowing for targeted optimization of pile driving parameters. Nonlinear relationships and threshold effects were observed, providing new insights into vibration propagation in soft clay. A web-based application was developed to facilitate adoption by practicing engineers, offering both predictive capabilities and explanations for each prediction. This research contributes to geotechnical engineering by offering a more accurate, nuanced, and interpretable approach to predicting pile driving vibrations, with implications for optimizing construction practices and mitigating environmental impacts in urban areas. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003184414&doi=10.1007%2Fs00521-025-11203-8&partnerID=40&md5=fb3bdc25eb5cf37d34538c703554a233,CE,Geotechnical,Neural Network; Piles & Piling; Peak Particle Velocity,Neural Computing and Applications
Journal Article,"X. J., Ma Yu, J., Jiang, F. F.",2025,Revealing the impacts of the built environment factors on pedestrian-weighted air pollutant concentration using automated and interpretable machine learning,Elsevier Ltd,387,,,"Urban air pollution poses significant health risks, especially to pedestrians due to their proximity to pollutants and lack of physical protection. Understanding the influence of built environment factors is essential to mitigate this pollution and safeguard pedestrian health. However, most existing literature focus primarily on pollutant sources and dispersion dynamics, paying less attention to the factors that affect the extent of pedestrian exposure to pollutants. Additionally, while machine learning has gained traction in urban studies, challenges remain in model optimization and interpretability, leading to limited transparency and reduced clarity in environment strategy development. To address these gaps, this study proposes a methodological framework to measure pedestrian-weighted air pollutant concentrations (PWAPC) and analyze the complex effects of the built environment. The objectives include (1) integrating air pollution and pedestrian volume data to quantify PWAPC levels, and (2) employing automated machine learning (AutoML) and interpretable machine learning (IML) to model PWAPC and evaluate key built environment impacts. A case study on PM2.5 concentrations in Central London demonstrates the efficiency of AutoML in algorithm selection and hyperparameter optimization. Using IML, critical factors such as points of interest (POIs), traffic infrastructure, diversion ratios, betweenness centrality, street canyon effects, and urban greenness are identified. The analysis also reveals non-linear relationships between these factors and PWAPC. This study provides actionable insights for urban planning and environmental management.",,CE,Environmental,Automated machine learning; Built environment; Interpretable machine learning; Pedestrian-weighed air pollutant concentration,Journal of Environmental Management
Journal Article,"Y., Chen Yu, Y., Liao, W., Wang, Z., Zhang, S., Kang, Y., Lu, X.",2025,Intelligent generation and interpretability analysis of shear wall structure design by learning from multidimensional to high-dimensional features,Elsevier Ltd,325,,,"The intelligent design of shear wall structures is a critical aspect of smart construction, with a high demand for research and applications. Accurately predicting the shear wall ratio (i.e., the shear wall area-to-floor area ratio) during cost estimation and rapidly generating shear wall layouts during early design is essential. However, the unclear influences of numerous design feature parameters hinder the enhancement of generative AI design. This affects both the prediction of shear wall ratios from multidimensional features and the generation of shear wall layouts from high-dimensional features. Therefore, a method for generating key structural design features using machine learning (ML) and generative adversarial networks (GANs), along with model interpretation, is proposed in this study. Existing shear wall design data are collected, and features such as the architectural plan geometry, seismic design conditions, and shear wall ratios are extracted to establish a dataset. Key shear wall ratio parameters are predicted using an ML model with multidimensional design features as inputs, and interpretability analysis is conducted using Shapley Additive Explanations (SHAP). Concurrently, a GAN model is built to generate shear wall designs using fused image-text high-dimensional features, and the influence patterns of design features are explained through sensitivity analysis. The analysis results indicate that the prediction accuracy is effectively enhanced by ML-based multidimensional feature learning, shear wall designs are effectively generated by GAN-based high-dimensional feature learning, and seismic design intensity and structural height are revealed as significant factors through interpretability analysis. Furthermore, when high-dimensional feature inputs are available, the generation of comprehensive features should be prioritized for shear wall structural designs. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211970583&doi=10.1016%2Fj.engstruct.2024.119472&partnerID=40&md5=36d25bdd627b3632c52aee774f34111d,CE,Structural,Intelligent structural design; Interpretable machine learning; Generative adversarial networks; Multi- and high-dimensional feature analysis; Shear wall structure,Engineering Structures
Journal Article,"Y. X., Yu Yu, Z. P., Shi, X. H., Wan, R., Wang, B. W., Zhang, J. X.",2025,Towards Transparent Urban Perception: A Concept-Driven Framework with Visual Foundation Models,Multidisciplinary Digital Publishing Institute (MDPI),14,8,,"Understanding urban visual perception is crucial for modeling how individuals cognitively and emotionally interact with the built environment. However, traditional survey-based approaches are limited in scalability and often fail to generalize across diverse urban contexts. In this study, we introduce the UP-CBM, a transparent framework that leverages visual foundation models (VFMs) and concept-based reasoning to address these challenges. The UP-CBM automatically constructs a task-specific vocabulary of perceptual concepts using GPT-4o and processes urban scene images through a multi-scale visual prompting pipeline. This pipeline generates CLIP-based similarity maps that facilitate the learning of an interpretable bottleneck layer, effectively linking visual features with human perceptual judgments. Our framework not only achieves higher predictive accuracy but also offers enhanced interpretability, enabling transparent reasoning about urban perception. Experiments on two benchmark datasets-Place Pulse 2.0 (achieving improvements of +0.041 in comparison accuracy and +0.029 in R2) and VRVWPR (+0.018 in classification accuracy)-demonstrate the effectiveness and generalizability of our approach. These results underscore the potential of integrating VFMs with structured concept-driven pipelines for more explainable urban visual analytics.",,CE,GIS / Remote Sensing,deep learning; urban perception; visual concept; explainable AI; visual prompt,ISPRS International Journal of Geo-Information
Journal Article,H. Yue,2024,Investigating the influence of streetscape environmental characteristics on pedestrian crashes at intersections using street view images and explainable machine learning,Elsevier Ltd,205,,,"Examining the relationship between streetscape features and road traffic accidents is pivotal for enhancing roadway safety. While previous studies have primarily focused on the influence of street design characteristics, sociodemographic features, and land use features on crash occurrence, the impact of streetscape features on pedestrian crashes has not been thoroughly investigated. Furthermore, while machine learning models demonstrate high accuracy in prediction and are increasingly utilized in traffic safety research, understanding the prediction results poses challenges. To address these gaps, this study extracts streetscape environment characteristics from street view images (SVIs) using a combination of semantic segmentation and object detection deep learning networks. These characteristics are then incorporated into the eXtreme Gradient Boosting (XGBoost) algorithm, along with a set of control variables, to model the occurrence of pedestrian crashes at intersections. Subsequently, the SHapley Additive exPlanations (SHAP) method is integrated with XGBoost to establish an interpretable framework for exploring the association between pedestrian crash occurrence and the surrounding streetscape built environment. The results are interpreted from global, local, and regional perspectives. The findings indicate that, from a global perspective, traffic volume and commercial land use are significant contributors to pedestrian–vehicle collisions at intersections, while road, person, and vehicle elements extracted from SVIs are associated with higher risks of pedestrian crash onset. At a local level, the XGBoost-SHAP framework enables quantification of features’ local contributions for individual intersections, revealing spatial heterogeneity in factors influencing pedestrian crashes. From a regional perspective, similar intersections can be grouped to define geographical regions, facilitating the formulation of spatially responsive strategies for distinct regions to reduce traffic accidents. This approach can potentially enhance the quality and accuracy of local policy making. These findings underscore the underlying relationship between streetscape-level environmental characteristics and vehicle–pedestrian crashes. The integration of SVIs and deep learning techniques offers a visually descriptive portrayal of the streetscape environment at locations where traffic crashes occur at eye level. The proposed framework not only achieves excellent prediction performance but also enhances understanding of traffic crash occurrences, offering guidance for optimizing traffic accident prevention and treatment programs. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197101517&doi=10.1016%2Fj.aap.2024.107693&partnerID=40&md5=329b7a1f2504f8f66180c81e49e33f63,CE,Transportation,Pedestrian crash; Intersection; Streetscape environment; Street view images; XGBoost; SHAP,Accident Analysis & Prevention
Journal Article,"Y. W., Wang Zang, H. M., Liu, Z. Z., Huang, J.",2024,Flood risk assessment of coastal cities based on GCW_ISODATA and explainable artificial intelligence methods,Elsevier Ltd,115,,,"Scientific and accurate assessment of risk influencing factors are crucial for flood risk management. This paper aims to propose a new comprehensive framework for flood risk assessment in coastal cities. Firstly, considering the flood characteristics of coastal cities and the impact of floods on urban spatial structure, a flood risk assessment indicator system for coastal cities was established. Secondly, combining game combination weighting and Iterative self-organizing data analysis technique algorithm (GCW_ISODATA) for flood risk assessment. Finally, based on the explainable machine learning techniques, the sensitivity of indicators to flood risk was analyzed. The results indicated that coastal floods are more destructive than rainfall and river floods. Moreover, the indicator weighting and threshold division have a direct impact on the rationality of flood risk, GCW_ISODATA method performs well in Accuracy, F1 score, and AUC, especially with the highest AUC among all methods. Entropy weight method and GCW are significantly superior to Analytic Hierarchy Process (AHP) method, and ISODATA method usually performs better than the K-Means and Natural Break method. Furthermore, the sensitivity of indicators to flood risk reveals that differences in economic, social, and environmental characteristics across regions affect the actual impact of these indicators, leading to the sensitivity of the same indicator to flood risk varies significantly across different regions. It is expected that the framework proposed in this study can be used to explore flood risk impact on other coastal cities.",,CE,Water,Flood risk assessment; Coastal cities; Game combination weighting method; Explainable artificial intelligence method; Sensitivity analysis,International Journal of Disaster Risk Reduction
Journal Article,"J., Fang Zhan, W., Love, P. E. D., Luo, H.",2024,Explainable Artificial Intelligence: Counterfactual Explanations for Risk-Based Decision-Making in Construction,Institute of Electrical and Electronics Engineers Inc.,71,,10667–10685,"Artificial intelligence (AI) approaches, such as deep learning models, are increasingly used to determine risks in construction. However, the black-box nature of AI models makes their inner workings difficult to understand and interpret. Deploying explainable artificial intelligence (XAI) can help explain why and how the output of AI models is generated. This article addresses the following research question: How can we accurately identify the critical factors influencing tunnel-induced ground settlement and provide counterfactual explanations to support risk-based decision-making? We apply an XAI approach using counterfactual explanations to help understand decision-making surrounding risks when considering control ground settlement. Our approach consists of a: 1) construction of Kernel principal components analysis-based deep neural network (DNN) model; 2) generation of counterfactual explanations; 3) analysis of risk prediction and assessment factors' importance, necessity, and sufficiency. We apply our approach to the San-yang road tunnel project in Wuhan, China. The results demonstrate that the KPCA-DNN model better predicted ground settlement based on high-dimensional input features than the baseline model (i.e., AdaBoost and RandomForest). The bubble chamber pressure→ cutter-head speed→ equipment inclination is also identified as the primary risk path. Our findings indicate that using counterfactual explanations enables transparency and trust in AI-based risk models to be acquired. Moreover, our approach can help site managers, engineers, and tunnel-boring machine operators understand how to manage better and mitigate the risk of ground settlement. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183958144&doi=10.1109%2FTEM.2023.3325951&partnerID=40&md5=a2b76a9362d9ea0c839fab3da462c56d,CEM,Risk and Uncertainty,Counterfactual explanations (CFE); decisionmaking; explainable artificial intelligence (XAI); risk; tunneling,IEEE Transactions on Engineering Management
Journal Article,"E., Zhang Zhang, E.",2025,Gas pipeline leakage detection based on multiple multimodal deep feature selections and optimized deep forest classifier,,13,,,"Gas pipeline leaks contribute to one-third of methane emissions annually, posing environmental damage and safety risks. However, accurate and timely detection of the leak presents several challenges, including remote and inaccessible environments, low accuracy and efficiency, and high hardware and labor costs. To address these challenges, we propose a gas pipeline leakage detection architecture based on multiple multimodal deep feature selections and the optimized Improved Deep Forest Classifier (IDFC). First, the multimodal data, thermal images and gas sensor data, are pre-processed. Then a deep feature pool is constructed using the selected Convolutional Neural Network (CNN) models, including AlexNet, ResNet-50, MobileNet, VggNet, and EfficientNet, as well as their inner layers. Aided by the heatmaps created using Gradient-weighted Class Activation Mapping (Grad-CAM), a visualization-based primary feature selection is applied to determine the best features to form an initial CNN pool. The output of the flattened features from this CNN pool is then fed into the IDFC for classification. Hyperparameters of the base learners are then selected for an explainable and enhanced diversity tree-structured deep forest classifier, using the selected multimodal features as inputs. Finally, the Accuracy-Size Comprehensive Indicator (ASCI) is introduced for the secondary feature selection and the optimized deep forest classifier construction, which balances the model accuracy and size and reduces hardware resource requirements. Using the simulated testing dataset created for this research, our architecture demonstrated superior accuracy (98.9%) and deployability with its lower model size (115 MB). This lightweight AI architecture is successfully deployed on a soft robotic system for real-time gas leak detection. Our proposed architecture is also extensible to other environmental hazard detection situations, such as liquid leaks in the pipelines.",,CE,Environmental,gas pipeline leakage detection; multimodal modeling; deep feature selection; explainable artificial intelligence; tree-based deep learning; accuracy size comprehensive indicator; soft robotics,Frontiers in Environmental Science
Journal Article,"F., Wang Zhang, C., Liu, J., Zou, X., Sneed, L. H., Bao, Y., Zhang, L.",2023,Prediction of FRP-concrete interfacial bond strength based on machine learning,Elsevier Ltd,274,,,"Externally bonding fiber reinforced polymer (FRP) to concrete structures is an effective way to enhance the mechanical performance of concrete structures. Many equations have been proposed to predict the interfacial bond strength for FRP-concrete structures but have limited accuracy due to the complexity of the bond behavior. This study proposes to formulate the FRP-concrete interfacial bond strength based on machine learning (ML) methods, which have emerged as a promising alternative to achieve high prediction accuracy in high-dimension problems. To this end, a database containing 1,375 FRP-concrete direct shear test specimens that failed due to interfacial debonding was established. The database was improved using an unsupervised isolation forest that identified and eliminated anomalous data, and was then used to train six ML models, namely artificial neural networks (ANN), support vector machine, decision tree, gradient boosting decision tree, random forest, and XGboost algorithms, to predict the FRP-concrete interfacial bond strength. The ML predictive models showed higher accuracy than 16 existing equations in the literature. The XGBoost model showed the highest accuracy, and its coefficient of variation was 54% lower than the existing equation with the highest accuracy among those considered. The ANN model was used to perform a parametric study on the influencing parameters, and a new equation was generated to predict the interfacial bond strength, considering the key influencing parameters. The equation enables interpretation of the ML models. The study combines ML models and traditional physical models to achieve a novel, interpretable ML method for predicting FRP-concrete interfacial bond strength. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141250348&doi=10.1016%2Fj.engstruct.2022.115156&partnerID=40&md5=8cb1b2de398bd762be6b2b6919759b11,CE,Materials,Bond strength; extreme gradient boosting (XGBoost); fiber reinforced polymer (FRP); Isolation forest; Interpretable machine learning (ML); Random forest (RF),Engineering Structures
Journal Article,"H. A., Hewage Zhang, K., Bakhtavar, E., Sun, Q. Q., Sadiq, R.",2025,Robust building energy retrofit evaluation under uncertainty: An interpretable machine learning approach,Elsevier Ltd,345,,,"Improving the energy efficiency and thermal comfort of existing residential buildings is essential for sustainable urban development. However, uncertainties in occupant behaviors and building constructions pose challenges to optimizing retrofit strategies. This study presents an integrated approach combining physics-based energy simulation, interpretable machine learning, and multi-objective optimization to quantify these uncertainties and identify optimal retrofit strategies. Latin Hypercube Sampling was used to generate representative variability in occupant and construction parameters, while Extreme Gradient Boosting (XGBoost) served as a surrogate model to reduce the computational burden of detailed simulations. Shapley Additive exPlanations (SHAP) and standardized regression coefficients were applied to enhance model interpretability and identify key features influencing energy and comfort performance. The approach was applied to representative Canadian singledetached houses across four climate zones and three HVAC systems: natural gas furnaces, air source heat pumps, and ground source heat pumps. XGBoost achieved R2 values above 0.85 for energy consumption and 0.95 for discomfort hours in most scenarios. Heating setpoint temperature, airtightness, and equipment power density emerged as dominant factors, with their relative importance varying by HVAC types and climate conditions. Passive retrofits were more impactful in colder zones, while behavioral adjustments were more effective in milder climates. The surrogate-based model reduced computation time by 89.7%. Pareto optimal solutions demonstrated up to 48% energy savings and 32% discomfort hour reductions. The proposed method offers actionable insights for policymakers and practitioners to implement targeted, efficient, and adaptive retrofit strategies under uncertainty.",,CE,Energy,Energy savings; Interpretable machine learning; Robust optimization; Occupant behaviors; Construction uncertainties,Energy Conversion and Management
Journal Article,"J. Y., Ma Zhang, X. L., Zhang, J. L., Sun, D. L., Zhou, X. Z., Mi, C. L., Wen, H. J.",2023,Insights into geospatial heterogeneity of landslide susceptibility based on the SHAP-XGBoost model,Elsevier Ltd,332,,,"The spatial heterogeneity of landslide influencing factors is the main reason for the poor generalizability of the susceptibility evaluation model. This study aimed to construct a comprehensive explanatory framework for landslide susceptibility evaluation models based on the SHAP (SHapley Additive explanation)-XGBoost (eXtreme Gradient Boosting) algorithm, analyze the regional characteristics and spatial heterogeneity of landslide influ-encing factors, and discuss the heterogeneity of the generalizability of the models under different landscapes. Firstly, we selected different regions in typical mountainous hilly region and constructed a geospatial database containing 12 landslide influencing factors such as elevation, annual average rainfall, slope, lithology, and NDVI through field surveys, satellite images, and a literature review. Subsequently, the landslide susceptibility eval-uation model was constructed based on the XGBoost algorithm and spatial database, and the prediction results of the landslide susceptibility evaluation model were explained based on regional topography, geology, and hy-drology using the SHAP algorithm. Finally, the model was generalized and applied to regions with both similar and very different topography, geology, meteorology, and vegetation, to explore the spatial heterogeneity of the generalizability of the model. The following conclusions were drawn: the spatial distribution of landslides is heterogeneous and complex, and the contribution of each influencing factor on the occurrence of landslides has obvious regional characteristics and spatial heterogeneity. The generalizability of the landslide susceptibility evaluation model is spatially heterogeneous and has better generalizability to regions with similar regional characteristics. Further explanation of the XGBoost landslide susceptibility evaluation model using the SHAP method allows quantitative analysis of the differences in how much various factors contribute to disasters due to spatial heterogeneity, from the perspective of global and local evaluation units. In summary, the integrated explanatory framework based on the SHAP-XGBoost model can quantify the contribution of influencing factors on landslide occurrence at both global and local levels, which is conducive to the construction and improvement of the influencing factor system of landslide susceptibility in different regions. It can also provide a reference for predicting potential landslide hazard-prone areas and for Explainable Artificial Intelligence (XAI) research.",,CE,Geotechnical,Geospatial heterogeneity; Landslide susceptibility; Explainability; Influencing factors; Generalizability,Journal of Environmental Management
Journal Article,"K., Song Zhang, Q., Ma, H., Qiu, W., Li, M., Kim, I.",2025,Synergistic role of audio-visual perceptions in promoting bikeshare for active travel,Elsevier Ltd,145,,,"This study examines factors beyond macroscale built environments (BE) influencing cycling behavior, integrating both microscale visual and auditory perceptions, especially soundscapes, while their impacts on cycling have not been explicitly investigated. Leveraging massive bikeshare trip data in Shenzhen, China, this study employs spatial explainable machine learning to uncover the associations among these three dimensions and cycling behavior using a multi-scale analytical framework. The results show that: (1) Macroscale BE remains dominant, while micro-scale soundscapes outweigh visual perceptions in explaining cycling usage. (2) Both perceived streetscape and soundscape indicators exhibit significant nonlinear effects on cycling behavior. (3) Synergistic interactions between visual streetscape and soundscape are observed, where maintaining audio-visual consistency (e.g., high perceived safety with high natural sounds perception) can positively affect cycling more substantially. Our findings encourage planners to prioritize strategic micro-level modifications targeting perceived street visual and auditory aspects over conventional macroscale BE interventions as cost-effective alternatives. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005497820&doi=10.1016%2Fj.trd.2025.104806&partnerID=40&md5=897fc25d8d5bd13cf37c8a5833491dfa,CE,Transportation,Bikeshare; Urban perception; Soundscape; Streetscape; Nonlinear; Spatial analysis,Transportation Research Part D: Transport and Environment
Journal Article,"K., Zheng Zhang, S.",2025,"An Interpretable Deep Learning Approach Integrating PatchTST, Quantile Regression, and SHAP for Dam Displacement Interval Prediction",Multidisciplinary Digital Publishing Institute (MDPI),17,11,,"Accurate prediction of dam displacement is essential for structural safety and risk management. To comprehensively address the accuracy-uncertainty-interpretability"" trilemma in dam displacement prediction, this study proposes a deep learning framework that integrates Patch Time Series Transformer (PatchTST), Sand Cat Swarm Optimization (SCSO), Quantile Regression (QR), and SHapley Additive exPlanations (SHAP). The proposed framework first employs PatchTST to capture the nonlinear temporal dependencies between multiple monitoring factors and dam displacement, while SCSO is utilized to adaptively optimize key hyperparameters, enabling the construction of a high-precision point prediction model. On this basis, QR is introduced to model the distributional uncertainty of displacement responses and to generate confidence-based prediction intervals, facilitating the evaluation of displacement anomalies. Furthermore, SHAP is incorporated to quantify the marginal contribution of each input factor to the model outputs, thereby enhancing interpretability and aligning model behavior with physical domain knowledge. The framework is validated using multi-year monitoring data from a double-curvature arch dam located in Southwest China. Comparative experiments demonstrate that the proposed model outperforms five well-established machine learning methods and the traditional linear regression method in terms of point prediction accuracy, reliability of interval estimation, and false alarm rate, exhibiting strong generalization and robustness. The SHAP-based analysis further reveals that water pressure variations and seasonal temperature cycles are the dominant factors influencing radial displacement, consistent with known structural deformation mechanisms. These findings affirm the physical consistency and engineering applicability of the proposed framework, offering a deployable and trustworthy solution for intelligent dam health monitoring and uncertainty-aware forecasting in safety-critical infrastructures.""",,CE,Water,displacement interval prediction; patch time series transformer; sand cat swarm optimization; quantile regression; shapley additive explanations; uncertainty quantization,Water
Journal Article,"K. H., Tamakloe Zhang, R., Cao, M. Q., Kim, I.",2024,Exploring fatal/severe pedestrian injury crash frequency at school zone crash hotspots: using interpretable machine learning to assess the micro-level street environment,Elsevier Ltd,121,,,"Several countries have implemented designated school zones and installed traffic calming measures to enhance the safety of vulnerable pedestrians near schools. While macro-level built environment attributes (e.g., land use) have been widely acknowledged in relation to the role they play in urban traffic safety, the effects of micro-level streetscape characteristics on crash frequency have not been investigated to any significant extent. Moreover, the associations between these environmental features and crashes in school zones remains largely unknown. To address this issue, we first identified school zone-related crash hotspot using spatiotemporal hotspot mining on a comprehensive dataset of 20,484 pedestrian-vehicle crashes between 2017 and 2021 in Seoul, South Korea. Streetscape characteristics were analysed using street view imagery and advanced computer vision techniques to extract and classify pixel-wise visual elements. Preliminary findings reveal spatiotemporal variations in fatal and severe injury (FSI) crashes, with school zones in central commercial and industrial areas emerging as persistent crash hotspots that have remained statistically significant hotspots for 90 % of the study period. Further impact analysis using interpretable machine learning helped to uncover the non-linear relationships between both micro and macro environmental features and FSI frequency. Lower levels of street enclosure and walkability were associated with a higher frequency of FSI crashes, while increased openness and imageability were also correlated with more FSI incidents. Additionally, street greenery was found to reduce FSI crashes once it reached a certain threshold. Our findings extend existing knowledge of how the built environment and streetscape design influence pedestrian safety in school zones, paving the way for more targeted interventions to plan safer pedestrian environments around schools.",,CE,Transportation,Street environment; School zone; Spatiotemporal; Pedestrian crash; Interpretable machine learning,Journal of Transport Geography
Journal Article,"L., Qi Zhang, Z. Z., Yang, X., Jiang, L.",2025,Intelligent Optimization Pathway and Impact Mechanism of Age-Friendly Neighborhood Spatial Environment Driven by NSGA-II and XGBoost,Multidisciplinary Digital Publishing Institute (MDPI),15,3,,"A comfortable outdoor environment, like its indoor counterpart, can significantly enhance the quality of life and improve the physical and mental health of elderly populations. Urban spatial morphology is one of the key factors influencing outdoor environmental performance. To explore the interactions between urban spatial morphology and the outdoor environment for the elderly, this study utilized parametric tools to establish a performance-driven workflow based on a morphology generation-performance evaluation-morphology optimization"" framework. Using survey data from 340 elderly neighborhoods in Beijing, a parametric urban morphology generation model was constructed. The following three optimization objectives were set: maximizing the winter pedestrian Universal Thermal Climate Index (UTCI), minimizing the summer pedestrian UTCI, and maximizing sunlight hours. Multi-objective optimization was conducted using a genetic algorithm, generating a ""morphology-performance"" dataset. Subsequently, the XGBoost (eXtreme Gradient Boosting) and SHAP (Shapley Additive Explanations) explainable machine learning algorithms were applied to uncover the nonlinear relationships among variables. The results indicate that optimizing spatial morphology significantly enhances environmental performance. For the summer elderly UTCI, the contributing morphological indicators include the Shape Coefficient (SC), Standard Deviation of Building Area (SA), and Deviation of Building Volume (SV), while the inhibitory indicators include the average building height (AH), Average Building Volume (AV), Mean Building Area (MA), and floor-area ratio (FAR). For the winter elderly UTCI, the contributing indicators include the AH, Volume-Area Ratio (VAR), and FAR, while the inhibitory indicators include the SC and porosity (PO). The morphological indicators contributing to sunlight hours are not clearly identified in the model, but the inhibitory indicators for sunlight hours include the AH, MA, and FAR. This study identifies the morphological indicators influencing environmental performance and provides early-stage design strategies for age-friendly neighborhood layouts, reducing the cost of later-stage environmental performance optimization.""",,CE,Environmental,age-friendly neighborhood morphology; outdoor environmental performance; multi-objective optimization; XGBoost,Applied Sciences
Journal Article,"P., Jin Zhang, T., Wang, Y., Guo, H.",2025,Exploring the Dynamic Evolution and Drivers of the Coupled Coordination Relationship of Carbon Emission Efficiency and Economic Benefits in Construction Land Development,Multidisciplinary Digital Publishing Institute (MDPI),15,5,,"In the pursuit of sustainable urban development, construction land development (CLD) not only carries the important mission of promoting economic growth but also needs to actively respond to the environmental requirements of reducing carbon emissions. However, there is a tension and balance between these two objectives. This study explores the evolution characteristics and influencing mechanisms of the synergistic relationship between carbon emission efficiency and economic benefits of CLD based on the undesirable slacks-based measurement, coupling coordination degree (CCD) model, Tapio decoupling model, spatial convergence model, and interpretable machine learning techniques. The main conclusions are as follows: (1) The CCR between CEE and economic benefits of CLD in China shows the characteristic of “improvement-stability-local decline”, and it is higher in the eastern region than in the central and western regions. (2) The decoupling of carbon emission efficiency (CEE) and economic benefits of CLD between 2003 and 2023 shows diverse trends in different provinces and time scales in China. (3) China’s eastern region are consistent with σ-convergence and β-convergence, and the gap in the level of inter-regional coupling co-ordination has narrowed. On the contrary, the central and western regions do not pass the σ and β-convergence tests, and the regional equilibrium needs to be improved. (4) In descending order of influence on CCR, they are ownership structure, urban construction land per capita, energy consumption per unit of gross domestic product, energy structure, industrial structure, and foreign trade investment intensity. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000511354&doi=10.3390%2Fbuildings15050759&partnerID=40&md5=f303cd8c69d6146eb1b1333e34583a3c,CEM,Sustainability (Construction),carbon emission efficiency (CEE) of construction land development (CLD); economic benefits; coupled coordination relationship (CCR); spatial convergence; interpretable machine learning,Buildings
Journal Article,"T. L., Wang Zhang, L., Zhang, Y. Z., Hu, Y. K., Zhang, W. Z.",2024,Assessing the nonlinear impact of green space exposure on psychological stress perception using machine learning and street view images,,12,,,"Introduction Urban green space (GS) exposure is recognized as a nature-based strategy for addressing urban challenges. However, the stress relieving effects and mechanisms of GS exposure are yet to be fully explored. The development of machine learning and street view images offers a method for large-scale measurement and precise empirical analysis.Methods This study focuses on the central area of Shanghai, examining the complex effects of GS exposure on psychological stress perception. By constructing a multidimensional psychological stress perception scale and integrating machine learning algorithms with extensive street view images data, we successfully developed a framework for measuring urban stress perception. Using the scores from the psychological stress perception scale provided by volunteers as labeled data, we predicted the psychological stress perception in Shanghai's central urban area through the Support Vector Machine (SVM) algorithm. Additionally, this study employed the interpretable machine learning model eXtreme Gradient Boosting (XGBoost) algorithm to reveal the nonlinear relationship between GS exposure and residents' psychological stress.Results Results indicate that the GS exposure in central Shanghai is generally low, with significant spatial heterogeneity. GS exposure has a positive impact on reducing residents' psychological stress. However, this effect has a threshold; when GS exposure exceeds 0.35, its impact on stress perception gradually diminishes.Discussion We recommend combining the threshold of stress perception with GS exposure to identify urban spaces, thereby guiding precise strategies for enhancing GS. This research not only demonstrates the complex mitigating effect of GS exposure on psychological stress perception but also emphasizes the importance of considering the dose-effect"" of it in urban planning and construction. Based on open-source data, the framework and methods developed in this study have the potential to be applied in different urban environments, thus providing more comprehensive support for future urban planning.""",,CE,Environmental,urban greening; street view; human perception; health planning; sustainable environment,Frontiers in Public Health
Journal Article,"X., Zhao Zhang, X., Bian, Y., Huang, J., Yin, L.",2024,"Interactive effects analysis of road, traffic, and weather characteristics on shared e-bike speeding risk: A data-driven approach",Elsevier Ltd,207,,,"As electric bikes (e-bikes) rapidly develop in China, their traffic safety issues are becoming increasingly prominent. Accurately detecting risky riding behaviors and conducting mechanism analysis on the multiple risk factors are crucial in formulating and implementing precise management policies. The emergence of shared e-bikes and the advancements in interpretable machine learning present new opportunities for accurately analyzing the determinants of risky riding behaviors. The primary objective of this study is to examine and analyze the risk factors related to speeding behavior to aid urban management agencies in crafting necessary management policies. This study utilizes a large-scale dataset of shared e-bike trajectory data to establish a framework for detecting speeding behavior. Subsequently, the extreme gradient boosting (XGBoost) model is employed to identify the level of speeding risk by leveraging its excellent identification ability. Moreover, based on measuring the degree of interaction among road, traffic, and weather characteristics, the investigation of the complex interactive effects of these risk factors on high-risk speeding is conducted using bivariate partial dependence plots (PDP) by its superior parsing ability. Feature importance analysis results indicate that the top five ranked variables that significantly affect the identified results of speed risk levels are land use density, rainfall, road level, curbside parking density, and bike lane width. The interaction analysis results indicate that higher levels of road and bike lane width correspond to an increased possibility of high-risk speeding among riders. Land use density, curbside parking density, and rainfall display a nonlinear effect on high-risk speeding. Introducing road level, bike lane width, and time interval could change the patterns of nonlinear effects in land use density, curbside parking density, and rainfall. Finally, several policy recommendations are proposed to improve e-bike traffic safety by utilizing the extracted feature values associated with a higher probability of high-risk speeding. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202356107&doi=10.1016%2Fj.aap.2024.107755&partnerID=40&md5=c4b4d69bd50f57fdf7893ab90ff01102,CE,Transportation,Shared e-bike; Speeding behavior; Interpretable machine learning; Risk factors; Policy recommendations,Accident Analysis & Prevention
Journal Article,"X. J., Zhou Zhang, Z. Z., Xu, Y. M., Zhao, X. L.",2024,Analyzing spatial heterogeneity of ridesourcing usage determinants using explainable machine learning,Elsevier Ltd,114,,,"There is a pressing need to study spatial heterogeneity of ridesourcing usage determinants to develop bettertargeted transportation and land use policies. This study incorporates spatial information (i.e., the geographic coordinates of census tracts) into the machine learning model and leverages state-of-the-art explainable machine learning techniques to analyze census-tract-to-census-tract ridesourcing usage, identify the key factors that shape the usage, and explore their nonlinear associations across different spatial contexts. Specifically, we analyze the spatial heterogeneity of ridesourcing travel in Chicago based on three spatial contexts, including downtown, neighborhood and airport. The results reveal that built environment variables collectively contribute to the largest importance for the downtown and airport context, while socioeconomic and demographic variables are the strongest predictors for the neighborhood context. Travel cost, the number of commuters and transit supply variables have evident nonlinear associations with ridesourcing usage, and these associations show strong differences across these three spatial contexts. Moreover, incorporating geographic coordinates is shown to be useful in improving model's capability to capture spatial information and thus enhance its predictive performance. These findings provide transportation professionals with location-based insights to better plan and manage ridesourcing services in Chicago.",,CE,Transportation,Explainable machine learning; Nonlinear relationships; Ridesourcing usage; Spatial heterogeneity,Journal of Transport Geography
Journal Article,"Y. S., Ren Zhang, W. J., Chen, Y. C., Mi, Y. T., Lei, J. Y., Sun, L. C.",2024,Predicting the compressive strength of high-performance concrete using an interpretable machine learning model,,14,1,,"Reliable predictions of concrete strength can reduce construction time and labor costs, providing strong support for building construction quality inspection. To enhance the accuracy of concrete strength prediction, this paper introduces an interpretable framework for machine learning (ML) models to predict the compressive strength of high-performance concrete (HPC). By leveraging information from a concrete dataset, an additional six features were added as inputs in the training process of the random forest (RF), AdaBoost, XGBoost and LightGBM models, and the optimal hyperparameters of the models were determined using 5-fold cross-validation and random search methods. Four interpretable ML models for predicting the compressive strength of HPC, including the RF, AdaBoost, XGBoost and LightGBM models, which combine feature derivation and random search, were constructed. In addition, the SHapley Additive exPlanations (SHAP) method was applied to analyze the effects of the input features on the prediction results of the LightGBM model, which combines feature derivation and random search. The results showed that input features such as age, water/cement ratio, slag, and water were the key influences for predicting the compressive strength of HPC. Input features such as the superplastic/cement ratio, slag/cement ratio, and ash/cement ratio had nonsignificant impacts on the predicted compressive strength.",,CE,Materials,Machine learning; Proportional features; Random search; Compressive strength prediction; SHapley Additive exPlanations,Scientific Reports
Journal Article,"Z., Hu Zhang, Q., Yin, J.",2025,Maritime-Accident-Induced Environmental Pollution and Economic Loss Analysis Using an Interpretable Data-Driven Method,Multidisciplinary Digital Publishing Institute (MDPI),17,7,,"In this study, we developed an interpretable machine learning (ML) framework to predict marine pollution and economic losses from accident risk factors. A triple-feature selection process identified key predictors, followed by a comparative analysis of eight ML models. Random forest outperformed other models in forecasting environmental and property damage. The interpretable model was established based on the SHAP value framework, which revealed that onboard personnel count, vessel dimensions (length), and accident/ship types account for the risk factors with the most severe consequences, with environmental conditions like wind speed and air temperature contributing secondary effects. The methodology enables two critical applications: (1) environmental agencies can proactively assess accident impact through the identified risk triggers, optimizing emergency response planning, and (2) insurance providers gain data-driven risk evaluation metrics to refine premium calculations. By quantifying how human/technical factors, including crew members and vessel specifications, dominate over natural variables in accident effects, this data-driven approach provides actionable insights for maritime safety management and financial risk mitigation, achieving high prediction accuracy while maintaining model transparency through Shapley value explanations. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002308156&doi=10.3390%2Fsu17073023&partnerID=40&md5=b9431109ede62c80af217fd7a292a494,CE,Environmental,maritime safety; risk factor; interpretable machine learning; environmental and economic-loss analysis,Sustainability
Journal Article,"Z., Yang Zhang, J., Shi, Y., Yang, H., Chen, D., Zhang, C., Zhang, Z., Zhang, X.",2025,"Installation of pocket parks in mountainous cities: A case study on the nonlinear effect of the built environment on pocket park vitality in Chongqing, China",Elsevier B.V.,173,,,"Pocket parks are regarded as valuable tools for providing green activity spaces for citizens in high-density mountainous cities. However, existing research on the pocket park vitality has hardly paid attention to mountainous cities. Due to the unique topographical conditions of mountainous cities, the built environment factors influencing the pocket parks vitality may be more complex, new indicators such as sky view factors, average slope and building enclosure have been taken into account. This study takes Chongqing as a case study of a mountainous city. It utilizes location-based service data to measure pocket park vitality. An interpretable machine learning model that integrates eXtreme Gradient Boosting (XGBoost) and Shapley Additive exPlanations (SHAP) is employed to explore the nonlinear effects and interactions of built environment factors on pocket park vitality. Results indicate that: (1) Sky view factors and residential population are significant factors affecting pocket park vitality in mountainous cities. (2) Different built environment elements in mountainous cities have optimal threshold effects on pocket park vitality. For example, optimal park areas of less than 0.08 ha, vegetation coverage rates below 0.57 are built environment intervals that can significantly enhance pocket park vitality. (3) The spatial distribution map of SHAP values and the interaction analysis reveal the spatial effects and interrelationships of built environment factors. For example, the interaction between Residential Population and Floor Area Ratio exemplifies the complex mechanism of the synergistic impact of multiple factors. We further explain specific pocket parks in combination with local contributions and put forward precise optimization strategies based on the park attributes and characteristics of the surrounding environment. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001960918&doi=10.1016%2Fj.ecolind.2025.113417&partnerID=40&md5=b4792c051b1b44d56e2b077883709e4f,CE,Environmental,Pocket Park Vitality; Built Environment (BE) factors; Interpretable Machine Learning; eXtreme Gradient Boosting (XGBoost) model; Shapley Additive exPlanations (SHAP),Ecological Indicators
Journal Article,"C. P., Jia Zhao, M. M., Wang, Z. M., Mao, D. H., Wang, Y. Q.",2023,Toward a better understanding of coastal salt marsh mapping: A case from China using dual-temporal images,Elsevier Inc.,295,,,"Coastal salt marshes suffering from anthropogenic coastal development and sea level rise have attracted much attention because of their capacity for carbon sequestration and global climate change mitigation. Accurate mapping of coastal salt marshes is always the first step for their protection, management, and restoration. The inherent complexities of vegetation, dynamics of tides, and anthropogenic disturbances pose challenges for remote sensing-based approaches. Existing studies have utilized phenology information and various black-box algorithms to reduce misclassifications. However, the approaches with dual-temporal images containing phenology information have suffered from inefficiency; the misclassifications have been objectively postprocessed rather than considered in the method design, and the tacit knowledge of the trained black-box models has not been revealed. To address the above issues, we proposed a new approach with solid improvements in dual-temporal image construction, misclassification processing, and tacit knowledge analysis, including an efficient method to synthesize dual-temporal images based on the common divisor of the green-up season or senescence season resulting from latitudinal gradients in narrow coastal areas of China, a detailed classification scheme by interpretation of iteratively collected samples, and a key decision rule approximating the trained model. We applied the approach to Sentinel-1/2 images and DEM data, thus deriving a 10-m resolution coastal salt marsh map of China with an overall accuracy of 92.5%. A qualitative comparison showed that the map produced in this study was fitted well with actual salt marsh distributions, resulting in improved accuracy when compared to recently generated maps. The most important contribution is that the overall nature of the trained model observed from the training samples was approximated by a four-feature decision rule following the principle of explainable artificial intelligence, i.e., B8senescence/B4senescence < 2.06 & B4green/B8green < 0.78 & B12green/B11green < 0.72 & elevation < 2.13, and thus established a new potential classification mechanism for dual-temporal black-box algorithms, i.e., the water signal was covered up by flourishing vegetation, but the situation changed when vegetation withered. This study not only generates an accurate coastal salt marsh map at a national scale but also provides a classification mechanism for dual-temporal image-based coastal salt marsh identification and mapping.",,CE,GIS / Remote Sensing,Salt marshes; Tacit knowledge; Decision rules; Explainable artificial intelligence; Phenology,Remote Sensing of Environment
Journal Article,"J. Y., Jia Zhao, B. Y., Wu, J., Wu, X. L.",2025,Study of Spatial and Temporal Characteristics and Influencing Factors of Net Carbon Emissions in Hubei Province Based on Interpretable Machine Learning,Multidisciplinary Digital Publishing Institute (MDPI),14,6,,"Carbon emissions from global warming pose significant threats to both regional ecology and sustainable development. Understanding the factors affecting emissions is critical to developing effective carbon neutral strategies. This study constructed a precise 1 km resolution net carbon emissions map of Hubei Province, China (2000-2020), and compared the ten distinct machine learning models to identify the most effective model for revealing the relationship between carbon emissions and their influencing factors. The random forest regressor (RFR) demonstrates optimal performance, achieving root mean square error (RMSE) and mean absolute error (MAE) values that are nearly 10 times lower on average than the other models. The results are interpreted using Shapley additive explanation (SHAP), revealing dynamic factor impacts. Our findings include the following. (1) Between 2000 and 2020, net carbon emissions in Hubei increased threefold, with emissions from construction land rising by approximately 7.5 times over the past two decades. Woodland, a major carbon sink, experienced a downward trend. (2) Six key factors are population, the normalized difference vegetation index (NDVI), road density, PM2.5, the degree of urbanization, and the industrial scale, with only the NDVI reducing emissions. (3) Net carbon emissions displayed significant spatial differences and aggregation and are mainly concentrated in the central urban areas of Hubei Province. Overall, this study evaluates various regression models and identifies the primary factors influencing net carbon emissions. The net carbon emission map we have developed can visually identify and locate high-emission hotspots and vulnerable carbon sink areas, thereby providing a direct basis for provincial land use planning.",,CE,Environmental,machine learning; net carbon emissions; Shapley additive explanation (SHAP),Land
Journal Article,R. Zhao,2025,Explainable AI and Industrial Internet of Things (IIoT)-Based Energy-Saving Design: Application for the Electrical Industry,Institute of Electrical and Electronics Engineers Inc.,11,1,76–88,"The ever-increasing need for energy has caused the field of industrial electrical engineering to place a greater emphasis on designs that save energy. This is done with the intention of lowering carbon emissions as well as overall consumption. Transformers, which are essential parts of power supply and distribution systems, may be designed with an energy-efficient layout, which is one of the significant aspects of this phenomenon. This study makes use of Explainable AI (XAI) to investigate power loss in transformers, describe technical factors, determine the number of transformers, and optimize economic functioning using XAI algorithms. In order to attain superior results than those obtained via the use of conventional methods, the energy-saving design approach that was created in this research makes use of IoT sensors and XAI. Experiments that were carried out on a five-star hotel in the province of Jilin revealed a 9.8 and 43.5% decrease in complete power loss for two substations, respectively, when compared to the original design. This resulted in considerable cost reductions in yearly and entire life loss energy expenses. The research provides conclusive evidence that XAI is an efficient tool for generating cost savings and energy efficiency for industrial electrical applications.",,CE,Energy,NaN,"IEEE Systems, Man, and Cybernetics Magazine"
Journal Article,"W., Feng Zhao, S., Liu, J., Sun, B.",2023,An explainable intelligent algorithm for the multiple performance prediction of cement-based grouting materials,Elsevier Ltd,366,,,"Cement-based grouting material, distinguished by excellent fluidity and high strength, is widely used in the field of construction reinforcement, and anchoring of restraints. Owing to the high-performance requirements and complicated influencing factors of grouting material, the design of the mixing proportion has been challenging. This study proposes a machine learning (ML) based algorithmic framework integrating prediction, interpretation, and automatic hyperparameter tuning to identify the complex potential relationships between the mixing proportion parameters on the compressive strength and fluidity of cement-based grouting materials. The 442 compressive strength data and 217 fluidity data derived from both published literatures and laboratory experiments were collected to build a dataset for demonstrating the predictive performance of the ML models. The results indicated that the hyperparameter tuning technique via Bayesian Optimisation (BO) can significantly improve the time efficiency compared to grid search, reducing time consumption from 8000 s to 197 s with comparable accuracy. The optimal prediction results were obtained based on the XGBoost model with R2 = 0.93, RMSE = 7.37 for compressive strength, and R2 = 0.92, RMSE = 16.33 for fluidity. The SHapley Additive exPlainations (SHAP) is introduced to interpret the evaluation results and the influence of the various mix factors on grouting material from both global(model) and local(instance) perspectives. The suggested model can be seen as a function of influential input variables that help engineers conduct a rapid assessment and then in turn to optimize the design of the mixture proportion. © 2022 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144822015&doi=10.1016%2Fj.conbuildmat.2022.130146&partnerID=40&md5=c9df7c98f19935122412349467caddf5,CE,Materials,Explainable machine learning; Cement-based grouting material; SHAP explanation; Automatic hyperparameter tuning,Construction and Building Materials
Journal Article,"X., Zhang Zhao, L., Zhu, G., Cheng, C., He, J., Traoré, S., Singh, V. P.",2023,Exploring interpretable and non-interpretable machine learning models for estimating winter wheat evapotranspiration using particle swarm optimization with limited climatic data,Elsevier Ltd,212,,,"Accurate estimation of crop evapotranspiration (ETc) is crucial for improving the water use efficiency and designing and operating irrigation systems. To accurately calculate winter wheat ETc with limited meteorological data, the present study proposed two interpretable machine learning (ML) models (random forest (RF) and extreme gradient boosting (XGBoost)) as well as non-interpretable ML models (support vector machine (SVM) and deep neural network (DNN)) based on the particle swarm optimization (PSO) algorithm using observed winter wheat ETc data during the period from 2007 to 2013 at Luan Cheng Agro-ecosystem Experimental Station. Mean absolute error (MAE), root mean square error (RMSE), Nash-Sutcliffe efficiency (NSE), coefficient of determination (R2), and global performance indicator (GPI) were used to assess the performance of models. This demonstrated that the ML models based on the crop coefficient (Kc) and solar radiation (Rn) were accurate and offered a workaround for calculating winter wheat ETc in the absence of meteorological data. In four ML models, the ninth input combination, consisting of Kc, Rn, daily air maximum temperature (Tmax), daily air minimum temperature (Tmin), sunshine hours (n), and wind speed with a height of 2 m (U<inf>2</inf>), produced the best estimate of ETc. Among them, the PSO-based SVM (PSO-SVM) model obtained the best results for estimating ETc with MAE, RMSE, NSE, R2, and GPI values of 0.389 mm·d−1, 0.562 mm·d−1 0.910, 0.911, and 0.975, respectively, showing the advantages of the non-interpretable ML model in ETc forecasting. Accurate descriptions of actual hydrological and climatic processes were given by local interpretable model-agnostic explanations (LIME). The inflection points of daily climatic parameters (Tmin, Tmax, Rn, n) related to ETc were determined to be 3.80 °C, 5.50 °C, 1.62 MJ·m−2·d−1, 1.37 h, respectively. This work has potential to overcome the difficulty of measuring winter wheat ETc properly due to the lack of meteorological data and accomplish appropriate water management to conserve water and increase water productivity. © 2023 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168008233&doi=10.1016%2Fj.compag.2023.108140&partnerID=40&md5=40af9e784fd3861f70677aa4f27022bc,CE,Water,Winter wheat; Crop evapotranspiration; Particle swarm optimization algorithm; Machine learning; Interpretability,Computers and Electronics in Agriculture
Journal Article,"X. Y., Hong Zhao, M. Y., Wu, B.",2023,Chemistry-informed multi-objective mix design optimization of self-compacting concrete incorporating recycled aggregates,Elsevier Ltd,19,,,"One of the primary objectives of modern concrete advancements is to strike a delicate balance among factors such as improved mechanical performance, appropriate economic viability, and reduced carbon emissions. Given this backdrop, self-compacting concrete incorporating recycled aggregates (referred to herein as RASCC) has gained attention in the quest for sustainable construction materials. Nevertheless, concurrently attaining the above objectives is not an easy feat for this particular type of concrete. In this study, a machine learning model based on the XGBoost algorithm was developed using 368 sample data points to predict the compressive strength of RASCC. To enhance the model's accuracy, the chemical composition of RASCC's binding materials, rather than the quantities of constituent materials, was chosen as part of input parameters, giving rise to the term chemistry-informed"" for this model. The model's interpretability was comprehensively examined using the So library. Then, in conjunction with the explainable machine learning model, the NSGA-II algorithm was leveraged to establish a RASCC auxiliary design system, enabling triple-objective optimization (i.e., strength, cost, and carbon emissions). The findings indicated that the XGBoost-based model achieved superior accuracy in predicting RASCC's strength as compared to an existing neural network-based model. Additionally, using compound contents as inputs imbued the model with chemical significance, thus further enhancing its accuracy and interpretability. In conclusion, this study presents a plausible and beneficial tool for the efficient, cost-effective, and low-carbon design of RASCC.""",,CE,Materials,Recycled aggregate concrete; Self-compacting concrete; XGBoost; NSGA-II; Mixture proportioning,Case Studies in Construction Materials
Journal Article,"Y., Wang Zhao, M., Wang, J., Zhang, Y., Ren, J., Zhao, H.",2025,Interpretable machine learning model for performance characterization of lightweight concrete and composition design,Elsevier Ltd,45,,,"Concrete is a common building material due to its excellent properties. The engineering performance of concrete depends significantly on its compositions. Characterizing concrete performance and determining its reasonable composition is challenging. This study developed a novel framework to characterize concrete performance and determine the optimal composition based on the simplicial homology global optimization (SHGO) and the eXtreme gradient boosting (XGBoost) algorithm. The XGBoost algorithm captured the complex interrelations between the compositions and the corresponding performance. The SHGO was leveraged to determine the reasonable and optimal composition for improving the performance. The light aggregate concrete illustrated the developed framework. Meanwhile, the framework's performance was investigated and validated using laboratory tests. The relative error between the 28-day and 90-day compressive strength and the test value is less than 4 %, and the relative error of the mass is less than 2 %. Compared with the traditional design method, the 90-day compressive strength increased by 2.6 %, and the mass reduced by 5.5 %. Thus, the XGBoost adequately characterizes the performance of the lightweight, and the determined composition is different from the composition determined by the orthogonal experiments. However, the performance of the concrete improved significantly. It proved that the developed framework outperformed the traditional method. It also has essential guiding significance for the performance characterization and composition design of other civil engineering materials. The developed framework provides an excellent, scientific, and promising civil material characterizing and design tool. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000531757&doi=10.1016%2Fj.mtcomm.2025.112266&partnerID=40&md5=f56475d72b662e993edddfbc1fa85aa6,CE,Materials,Lightweight concrete; Performance; Composition design; Machine learning; Optimization,Materials Today Communications
Journal Article,"D. F., Li Zheng, Y. Y., Yan, C. L., Wu, H., Yamashiki, Y. A., Gao, B. T., Nian, T. K.",2025,"Landslide susceptibility assessment using AutoML-SHAP method in the southern foothills of Changbai Mountain, China",Springer Nature,22,6,1855–1875,"Landslide susceptibility assessment (LSA) is crucial for effective regional geohazard risk management and territorial spatial planning. Recent advancements in machine learning (ML) have enhanced the accuracy of landslide susceptibility modeling. However, ML models face challenges in efficiently identifying potential landslide-prone areas because of the considerable time and resources required for data processing, model selection, and tuning, even for experienced specialists. This study proposes a hybrid approach that combines an automated ML model (AutoML) with the Shapley Additive exPlanations (SHAP) method to automatically predict landslides and provide a comprehensive explanation of the predictions. The AutoML model was employed for the southern foothills of Changbai Mountain, utilizing a dataset of 381 landslides with 16 landslide conditioning factors. The model automatically completed the assessment task, achieving an area under the curve (AUC) value of 0.9. Compared to traditional models, such as logistic regression, eXtreme Gradient Boosting, and two-dimensional convolutional neural network models, the AutoML model demonstrated high efficiency and good accuracy. Furthermore, the SHAP method offered a nuanced understanding of the interplay between model outcomes and conditioning factors from global and local perspectives. The SHAP method identified roads, altitudes, and slopes as key drivers of landslides in the case areas, revealing a non-linear relationship between feature variables and landslide prediction. These findings suggest that hybrid AutoML-SHAP can provide a new method for automatically assessing landslide susceptibility and introduce explainable artificial intelligence models.",,CE,Geotechnical,NaN,Landslides
Journal Article,"D., Lv Zhi, Y., Sun, H., Feng, X., Song, W., Tirachini, A., Antoniou, C.",2025,Understanding factors influencing ride-splitting adoption in Beijing: A comparative analysis with solo ride-hailing,Elsevier Ltd,200,,,"Ride-splitting, a special kind of ride-hailing service, presents significant potential for energy savings and emission reduction. Studying factors that promote ride-splitting can help build sustainable transportation systems. Although many studies have analyzed the impact of the built environment and sociodemographic variables on ride-splitting, there is a lack of consideration of variables specific to ride-hailing systems. This study aims to analyze the complex impact of explanatory variables (including ride-hailing system-specific variables) on ride-splitting, based on an interpretable machine-learning framework. Firstly, the price ratio between shared and solo trips, the distance passengers wait for the driver to pick them up (called passenger waiting distance), and the driver's detour index are extracted from Beijing's data. Then, a machine learning-based framework combining XGBoost and SHAP is constructed. The explained variables are the daily trip numbers of ride-splitting and solo ride-hailing between origin–destination (OD) pairs. The results show that price ratio, passenger waiting distance, and detour index have a greater impact on ride-splitting than solo ride-hailing. Based on SHAP values, a nonlinear threshold-based relationship between individual variables and ride-splitting demand is investigated. Exogenous variables related to the high adoption of ride-splitting include OD pairs having trip durations shorter than 20 min, a zonal per capita GDP below a certain threshold, and being located away from the city center. The interaction effects of multiple variables on ride-splitting, such as distance from the origin/destination to the city center and travel time, are investigated based on the SHAP interaction value. These findings help to adapt specific variables to facilitate the shift from solo trips to shared trips, which is conducive to more sustainable transportation patterns. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013583519&doi=10.1016%2Fj.tra.2025.104625&partnerID=40&md5=80f030805b134a8196c587c913453c25,CE,Transportation,Ride-splitting; Detour index; Price ratio; XGBoost; SHAP; Interaction effect,Transportation Research Part A: Policy and Practice
Journal Article,"H., Qiu Zhou, J., Li, M. J., Lu, H. L., Li, F. F.",2025,Assessment of Large-Scale Reservoirs' Impact on the Local Precipitation,,61,5,,"Reservoir operations have complex and profound impacts on local climate, particularly precipitation. Quantifying this impact is challenging because it requires the reconstruction of natural precipitation prior to reservoir operation. Instead of assuming that the natural variability of the contrast region and the study region is identical, this study develops an interpretable machine learning model to investigate relationships between precipitation-influencing factors and precipitation itself, including both stable components (sum of trend and seasonality from STL decomposition) and random components (residuals after removing trend and seasonality), which is then used to forecast natural precipitation in the absence of reservoir operation. The application in the contrast region verifies the forecast's accuracy, even in mountainous areas. The proposed method is used to analyze the impact of three large-scale reservoirs along the Yangtze River on local precipitation, collectively having a total storage capacity of 17.86 x 109 m3. The results indicate that reservoir operation leads to a 14% increase in the trend and seasonal components of precipitation, which would be underestimated by previous methods. In addition, there is a noticeable shift in the precipitation center toward the reservoir. Further comparisons suggest that reservoir operation shifts the key influencing factors of local precipitation patterns from those characterized by high variability to those characterized by low variability. Changes in soil water retention capacity likely play a significant role in these precipitation changes. We also found a significant positive coupling between soil moisture and precipitation in the study area, which has been a focal point of recent research. These findings provide new insights into the mechanisms through which reservoir construction impacts precipitation.",,CE,Water,reservoir; precipitation; interpretable machine learning model; precipitation centroid; SHAP,Water Resources Research
Journal Article,"R. C., Hu Zhou, X. W., Xi, C. J., He, K., Deng, L., Luo, G.",2025,"Unveiling dominant factors for gully distribution in wildfire-affected areas using explainable AI: A case study of Xiangjiao catchment, Southwest China",Springer Nature,22,8,2765–2792,"Wildfires significantly disrupt the physical and hydrologic conditions of the environment, leading to vegetation loss and altered surface geo-material properties. These complex dynamics promote post-fire gully erosion, yet the key conditioning factors (e.g., topography, hydrology) remain insufficiently understood. This study proposes a novel artificial intelligence (AI) framework that integrates four machine learning (ML) models with Shapley Additive Explanations (SHAP) method, offering a hierarchical perspective from global to local on the dominant factors controlling gully distribution in wildfire-affected areas. In a case study of Xiangjiao catchment burned on March 28, 2020, in Muli County in Sichuan Province of Southwest China, we derived 21 geoenvironmental factors to assess the susceptibility of post-fire gully erosion using logistic regression (LR), support vector machine (SVM), random forest (RF), and convolutional neural network (CNN) models. SHAP-based model interpretation revealed eight key conditioning factors: topographic position index (TPI), topographic wetness index (TWI), distance to stream, mean annual precipitation, differenced normalized burn ratio (dNBR), land use/cover, soil type, and distance to road. Comparative model evaluation demonstrated that reduced-variable models incorporating these dominant factors achieved accuracy comparable to that of the initial-variable models, with AUC values exceeding 0.868 across all ML algorithms. These findings provide critical insights into gully erosion behavior in wildfire-affected areas, supporting the decision-making process behind environmental management and hazard mitigation.",,CE,Geotechnical,Gully erosion susceptibility; Explainable AI; Wildfire; Geo-environmental factor; Machine learning; Information and Computing Sciences; Artificial Intelligence and Image Processing,Journal of Mountain Science
Journal Article,"Y., Fu Zhou, C. Y., Jiang, X. G., Yu, Q., Liu, H. Y.",2024,Who might encounter hard-braking while speeding? Analysis for regular speeders using low-frequency taxi trajectories on arterial roads and explainable AI,Elsevier Ltd,195,,,"Regular speeders are those who commit speeding recidivism during a period. Among their speeding behaviors, some occurring in specific scenarios may cause more hazards to road users. Therefore, there is a need to evaluate the driving risks if the regular speeders have different speeding propensities. This study considers speeding -related hard-braking events (SHEs) as a safety surrogate measure and recognizes the regular speeders who encounter at least one SHEs during the study period as risky individuals. To identify speeding behaviors and hard-braking events from low-frequency GPS trajectories, we compare the average travel speed between pairwise adjacent GPS points to the posted speed limit and examine the speed curve and the corresponding travel distance between these GPS points, respectively. Thereafter, a logistic model, XGBoost, and three 1D Convolutional Neural Networks (CNNs) including AlexNet CNN, Mini-AlexNet CNN, and Simple CNN are respectively developed to recognize the regular speeders who encountered SHEs based on their speeding propensities. The pro-posed Mini-AlexNet CNN achieves a global F1-score of 91% and recall of 90% on the testing data, which are superior to other models. Further, the study uses the Shapley Additive exPlanation (SHAP) framework to visually interpret the contribution of speeding propensities on SHE likelihood. It is found that speeding by 50% or greater for no more than 285 m is the most dangerous kind among all the speeding behaviors. Speeding on roads without bicycle lanes or on roads with roadside parking and excessive accesses increases the probability of encountering SHEs. Based on the analyses, we put forward tailored recommendations that aim to restrict hazard-related speeding behaviors rather than speeding behaviors of all kinds.",,CE,Transportation,Speeding behavior; Regular speeder; Hard-braking; Low-frequency GPS trajectories; Convolution neural network; Explainable AI,Accident Analysis & Prevention
Journal Article,"Yingjie, Chen Zhu, Liying",2025,Framework for Developing Prediction Models for Boundary Conditions of Slabs in Girders Considering Interpretability: An Application for Deck Slabs in Composite Box Girders,American Society of Civil Engineers,30,4,04025010,"The boundary conditions are critical for analyzing the behavior of the deck slabs in girders. However, the boundary conditions of slabs are hard to predict due to complex influencing factors. Recently, machine learning (ML) has been extensively used in structural analysis, while the lack of interpretability prevents their practical application. This paper innovatively proposes a framework that integrates analytical models with interpretable ML techniques, aiming at explainable prediction of bridge deck slab boundary conditions. The rotational and lateral restraint stiffnesses are each divided into two parts. Analytical models tackle analytically solvable structural behaviors, while ML models address complex behaviors, complemented by interpretive approaches to ensure prediction reliability. Ultimately, by integrating the solutions of these two parts, a prediction model for slab boundary conditions can be established. Taking the reinforced concrete deck slabs in steel?concrete composite box girders as an example, analytical models are established for restraint stiffness under simplified conditions. For restraint stiffness considering spatial effect and material nonlinearity, a validated finite-element model is employed for parametric analysis to construct the data set. Subsequently, three ML models are utilized to predict this part of restraint stiffness, incorporating three interpretability approaches to guarantee model reliability. After a comprehensive assessment of both interpretability and accuracy, the optimal ML models are integrated with the analytical models, creating the interpretable prediction models for boundary conditions of deck slabs in composite box girders. Examples and evidence demonstrate that the combination of theoretical analysis and artificial intelligence can effectively improve the reliability of the entire algorithm when considering the spatial effect of the structure as well as the nonlinearity of the material, providing a new perspective and method for calculating boundary conditions.",https://doi.org/10.1061/JBENF2.BEENG-6935,CE,Structural,Machine learning; Interpretability; Prediction model; Boundary conditions; Deck slabs,Journal of Bridge Engineering
Journal Article,"H., Yi Zong, W., Chan, A. P. C., Yang, H., Peng, P., Xiao, B.",2025,Developing a Fatigue Model for Construction Workers: An Interpretable Machine Learning Approach,American Society of Civil Engineers (ASCE),41,4,,"The construction industry is one of the most hazardous sectors worldwide, with extremely high rates of occupational deaths and injuries. Worker fatigue, caused by undertaking physically demanding tasks in awkward working postures over prolonged daily durations, has been recognized as the main cause of these accidents. Additionally, fatigue can lead to reduced work efficiency and increased absenteeism, undermining labor productivity. This study aims to develop an accurate and reliable model to estimate the fatigue levels of construction workers. Field studies were conducted with 156 construction workers at four construction sites in mainland China. A series of physiological, personal, work-related, and environmental factors were measured and monitored to establish an interpretable machine learning model for assessing fatigue levels. The developed interpretable machine learning model exhibited good fitting with high accuracy, evidenced by the random forest model attaining an R2 value of 0.9953 through the 10-fold cross-validation method. Furthermore, this model could transparently reveal the mechanisms underlying the prediction of worker fatigue. Work duration, work session (i.e., morning session, afternoon session), environmental parameters (i.e., air temperature, humidity, wind velocity, and radiation), and worker age were identified as key factors affecting the fatigue of construction workers. The developed fatigue model can prevent excessive fatigue among construction workers, and the model interpretation results may benefit the industry by making solid guidelines and practice notes to alleviate worker fatigue. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004639182&doi=10.1061%2FJMENEA.MEENG-6662&partnerID=40&md5=3ac08608412852d953cceb4d95648aac,CEM,Safety,Construction workers; Fatigue assessment; Occupational health and safety; Work stress; Interpretable machine learning,Journal of Management in Engineering
Journal Article,"L., Wang Zou, Z., Guo, R., Zhao, L.",2025,Decoding urban transportation: Trade-offs in mode choices using big data,Elsevier Ltd,143,,,"Understanding the determinants of urban transportation mode choice is crucial for developing efficient, sustainable, and green transit systems in metropolitan areas. This study proposes a comprehensive framework that integrates big data and machine learning techniques, leveraging a large dataset comprising over ten million trips to investigate the factors influencing transportation mode choice under clear competition during peak hours. We examine how travel attributes, land use, network centrality, and demographics shape the choices of subway, bus, taxi, and bike-share users. Employing oversampling and interpretable techniques, our analysis reveals that travel attributes significantly influence transportation mode choices, especially for public transportation. Additionally, the marginal effects of the features on mode choice are efficiently captured. Interaction effects between travel time and cost further highlight the complex trade-offs travelers make under different choice probabilities. The findings underscore the importance of integrating diverse data sources for a holistic understanding of urban transportation dynamics. © 2025 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002315388&doi=10.1016%2Fj.trd.2025.104756&partnerID=40&md5=76a5951e5d8e4d0e330d79c867f580ef,CE,Transportation,Travel mode choice; Travel behavior; Built environment; Big data; Interpretable machine learning,Transportation Research Part D: Transport and Environment
Journal Article,"Y., Xie Zou, W., Lou, S., Huang, Y., Xia, D., Yang, X., Feng, C.",2024,Prediction model establishment for residential community occupancy considering urban environment,Elsevier Ltd,96,,,"The accurate prediction of occupancy rates within the residential sector is pivotal for urban and energy planning and management. This study aims at addressing the often-neglected influence of regional variations within urban settings on occupancy rates by employing multi-source urban data and explainable machine learning algorithms. Utilizing mobile signaling data from January and July 2022 in Guangzhou, we developed prediction models using the CatBoost algorithm with hyperparameters finely tuned via Bayesian optimization. The SHapley Additive exPlanation was applied to elucidate the influence of different urban configurations. Our findings reveal significant seasonal variations and distinct differences between weekdays and weekends, showing higher occupancy rates in winter than in summer and elevated rates during weekends. Additionally, significant fluctuations were observed among different age groups on weekdays. The prediction models exhibited robust capabilities, achieving R2 values exceeding 0.95 for weekdays and 0.91 for weekends. Key findings highlight that the density of points of interest related to shopping and food, along with variability in building heights, significantly impact occupancy rates. The influence of diverse urban configurations varies across different hours of the day. By the comprehensive analysis of regional variations and the application of advanced machine learning techniques to predict and explain residential occupancy rates, this research offers tools to optimize the management and sustainability of built environments, enhances the understanding of urban operation patterns and contributes to advancing smart city technologies, providing valuable insights for urban and energy planning. © 2024 Elsevier B.V., All rights reserved.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201483446&doi=10.1016%2Fj.jobe.2024.110463&partnerID=40&md5=4c3d1267f645db0152948e40743a8821,CE,Environmental,Residential community; Occupancy rate; Urban environment; Ensemble model; SHapley additive exPlanation,Journal of Building Engineering
