# XAI-Construction
Artificial intelligence is increasingly applied in construction engineering and management (CEM), yet the limited transparency of many learning models constrains trust, validation, and practical adoption in high-stakes decision contexts. Explainable AI (XAI) has emerged as a promising solution to improve transparency in construction AI. However, the diversity of methods and levels of interpretability highlights the need for a review that maps current use cases, trends, and research gaps. This study addresses this need by applying a PRISMA-guided search and analyzing 55 journal articles published in past five years. A taxonomy-driven framework, together with bibliometric and text-analytic techniques were used to map publication patterns and thematic structures, and to identify application areas, model types, explanation approaches, and reported limitations. Findings indicate rapid growth in XAI research, with SHAP widely used for interpreting tree-structured models, while attention and saliency techniques used primarily in sensor and vision applications. Reported challenges include limited and imbalanced datasets, weak validation across contexts, minimal human-centered evaluation, and incomplete integration with building information modeling, digital twin, and automated construction workflows. Building on gaps in the existing literature, this study highlights four major future research directions in strengthening data foundations for explainability, advancing modeling and explanation methods, integrating XAI into operational workflows, and improving evaluation and human-centered validation.
